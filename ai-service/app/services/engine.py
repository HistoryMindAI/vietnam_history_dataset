from app.services.search_service import semantic_search, scan_by_year, scan_by_year_range, detect_dynasty_from_query, detect_place_from_query
import re

# Pre-compile regex for faster matching
YEAR_PATTERN = re.compile(r"(?<![\d-])([1-9][0-9]{1,3})(?!\d)")

# Year range: "tá»« nÄƒm 1225 Ä‘áº¿n nÄƒm 1400", "tá»« 1225 Ä‘áº¿n 1400", "giai Ä‘oáº¡n 1225-1400"
YEAR_RANGE_PATTERN = re.compile(
    r"(?:tá»«\s*(?:nÄƒm\s*)?|giai\s*Ä‘oáº¡n\s*)"
    r"(\d{3,4})"
    r"\s*(?:Ä‘áº¿n|tá»›i|[-â€“â€”])\s*(?:nÄƒm\s*)?"
    r"(\d{3,4})",
    re.IGNORECASE
)


def extract_single_year(text: str):
    """
    Extracts a single year between 40 and 2025 from text.
    """
    m = YEAR_PATTERN.search(text)
    if m:
        year = int(m.group(1))
        if 40 <= year <= 2025:
            return year
    return None


def extract_year_range(text: str):
    """
    Extracts a year range from text (e.g., 'tá»« nÄƒm 1225 Ä‘áº¿n 1400').
    Returns (start_year, end_year) or None.
    """
    m = YEAR_RANGE_PATTERN.search(text)
    if m:
        start = int(m.group(1))
        end = int(m.group(2))
        if 40 <= start <= 2025 and 40 <= end <= 2025 and start < end:
            return (start, end)
    return None


def extract_multiple_years(text: str):
    """
    Extracts multiple distinct years from text.
    Returns list of years if 2+ found, else None.
    E.g.: 'nÄƒm 938 vÃ  nÄƒm 1288' â†’ [938, 1288]
    """
    # First check if this is a year range query (handled separately)
    if extract_year_range(text):
        return None

    matches = YEAR_PATTERN.findall(text)
    years = []
    for m in matches:
        y = int(m)
        if 40 <= y <= 2025 and y not in years:
            years.append(y)
    return sorted(years) if len(years) >= 2 else None


MAX_EVENTS_PER_YEAR = 1
MAX_TOTAL_EVENTS = 5
MAX_TOTAL_EVENTS_DYNASTY = 10  # More results for dynasty-level queries
MAX_TOTAL_EVENTS_RANGE = 15   # More results for year range queries

# Identity patterns â€” who are you?
IDENTITY_PATTERNS = [
    "who are you", "báº¡n lÃ  ai", "giá»›i thiá»‡u báº£n thÃ¢n",
    "what is your name", "tÃªn báº¡n lÃ  gÃ¬", "tÃªn cá»§a báº¡n",
    "you are who", "giá»›i thiá»‡u vá» báº¡n", "báº¡n tÃªn gÃ¬",
    "hÃ£y giá»›i thiá»‡u", "cho tÃ´i biáº¿t vá» báº¡n",
]

# Creator patterns â€” who made you?
CREATOR_PATTERNS = [
    "ai táº¡o ra", "ai phÃ¡t triá»ƒn", "ai xÃ¢y dá»±ng", "ai lÃ m ra",
    "created by", "made by", "developed by", "built by",
    "táº¡o ra báº¡n", "phÃ¡t triá»ƒn báº¡n", "xÃ¢y dá»±ng báº¡n",
    "ai táº¡o báº¡n", "ai Ä‘Ã£ táº¡o", "do ai", "Ä‘Æ°á»£c táº¡o bá»Ÿi",
    "tÃ¡c giáº£", "nhÃ  phÃ¡t triá»ƒn", "developer",
]

IDENTITY_RESPONSE = (
    "## ðŸ›ï¸ Xin chÃ o! TÃ´i lÃ  **History Mind AI** â€” Trá»£ lÃ½ Lá»‹ch sá»­ Viá»‡t Nam.\n\n"
    "TÃ´i Ä‘Æ°á»£c sinh ra tá»« niá»m Ä‘am mÃª vá»›i **4.000 nÄƒm lá»‹ch sá»­ dÃ¢n tá»™c Viá»‡t Nam**, "
    "vá»›i sá»© má»‡nh giÃºp má»i ngÆ°á»i tiáº¿p cáº­n vÃ  khÃ¡m phÃ¡ di sáº£n lá»‹ch sá»­ má»™t cÃ¡ch dá»… dÃ ng, "
    "chÃ­nh xÃ¡c vÃ  sinh Ä‘á»™ng.\n\n"
    "### ðŸ“š TÃ´i cÃ³ thá»ƒ giÃºp báº¡n:\n"
    "- ðŸ” **Tra cá»©u sá»± kiá»‡n** theo nÄƒm, triá»u Ä‘áº¡i, hoáº·c nhÃ¢n váº­t\n"
    "- ðŸ“– **Ká»ƒ chuyá»‡n lá»‹ch sá»­** tá»« thá»i HÃ¹ng VÆ°Æ¡ng Ä‘áº¿n hiá»‡n Ä‘áº¡i\n"
    "- âš”ï¸ **PhÃ¢n tÃ­ch chiáº¿n cÃ´ng** â€” Báº¡ch Äáº±ng, Chi LÄƒng, Äiá»‡n BiÃªn Phá»§...\n"
    "- ðŸ° **TÃ¬m hiá»ƒu triá»u Ä‘áº¡i** â€” LÃ½, Tráº§n, LÃª, Nguyá»…n...\n"
    "- ðŸ“… **So sÃ¡nh giai Ä‘oáº¡n** â€” tá»« nÄƒm X Ä‘áº¿n nÄƒm Y\n\n"
    "HÃ£y há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» lá»‹ch sá»­ Viá»‡t Nam! ðŸ‡»ðŸ‡³"
)

CREATOR_RESPONSE = (
    "## ðŸ› ï¸ Ai Ä‘Ã£ táº¡o ra tÃ´i?\n\n"
    "TÃ´i â€” **History Mind AI** â€” Ä‘Æ°á»£c xÃ¢y dá»±ng bá»Ÿi **Ä‘á»™i ngÅ© HistoryMindAI**, "
    "má»™t nhÃ³m sinh viÃªn vÃ  ká»¹ sÆ° Ä‘am mÃª cÃ´ng nghá»‡ AI vÃ  lá»‹ch sá»­ Viá»‡t Nam.\n\n"
    "### ðŸ§  CÃ´ng nghá»‡ Ä‘áº±ng sau tÃ´i:\n"
    "- **AI & NLP**: Sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ vÃ  tÃ¬m kiáº¿m ngá»¯ nghÄ©a (Semantic Search) "
    "Ä‘á»ƒ hiá»ƒu cÃ¢u há»i cá»§a báº¡n báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn\n"
    "- **FAISS + Embeddings**: TÃ¬m kiáº¿m vector nhanh chÃ³ng trong hÃ ng nghÃ¬n sá»± kiá»‡n lá»‹ch sá»­\n"
    "- **Dá»¯ liá»‡u**: ÄÆ°á»£c huáº¥n luyá»‡n trÃªn bá»™ dá»¯ liá»‡u lá»‹ch sá»­ Viá»‡t Nam gá»“m hÆ¡n 50.000 máº«u, "
    "bao phá»§ tá»« thá»i ká»³ HÃ¹ng VÆ°Æ¡ng dá»±ng nÆ°á»›c Ä‘áº¿n hiá»‡n Ä‘áº¡i\n\n"
    "### ðŸŽ¯ Sá»© má»‡nh:\n"
    "Mang lá»‹ch sá»­ Viá»‡t Nam Ä‘áº¿n gáº§n hÆ¡n vá»›i má»i ngÆ°á»i thÃ´ng qua cÃ´ng nghá»‡ AI, "
    "giÃºp tháº¿ há»‡ tráº» hiá»ƒu vÃ  trÃ¢n trá»ng di sáº£n vÄƒn hÃ³a dÃ¢n tá»™c.\n\n"
    "ðŸ’¡ *HÃ£y thá»­ há»i tÃ´i: \"Tráº­n Báº¡ch Äáº±ng 938 diá»…n ra nhÆ° tháº¿ nÃ o?\"*"
)


def clean_story_text(text: str, year: int | None = None) -> str:
    """
    Clean up story text by removing redundant prefixes and making it a complete sentence.
    Handles various data patterns from the Vietnam history dataset.
    """
    if not text:
        return ""
    
    result = text.strip()
    
    # Phase 1: Remove structural/query-style prefixes (these are data artifacts, not content)
    structural_patterns = [
        r'^CÃ¢u há»i nháº¯m tá»›i sá»± kiá»‡n\s*',
        r'^TÃ³m táº¯t bá»‘i cáº£nh\s*â€“\s*diá»…n biáº¿n\s*â€“\s*káº¿t quáº£ cá»§a\s*',
        r'^Bá»‘i cáº£nh:\s*',
        r'^Ká»ƒ vá» .+ vÃ  Ä‘Ã³ng gÃ³p cá»§a .+ trong\s*',
    ]
    for pattern in structural_patterns:
        result = re.sub(pattern, '', result, flags=re.IGNORECASE)
    
    # Phase 2: Remove year prefixes to avoid "NÄƒm 1930: NÄƒm 1930, ..." duplication
    year_prefixes = [
        r'^NÄƒm \d+[,:]?\s*',
        r'^VÃ o nÄƒm \d+[,:]?\s*',
        r'^nÄƒm \d+[,:]?\s*',
        r'^\d{3,4}[,:]\s*',
    ]
    for pattern in year_prefixes:
        result = re.sub(pattern, '', result, flags=re.IGNORECASE)
    
    # Phase 3: Remove action-style prefixes
    action_prefixes = [
        r'^gáº¯n má»‘c \d+ vá»›i\s*',
        r'^diá»…n ra\s*',
        r'^xáº£y ra\s*',
    ]
    for pattern in action_prefixes:
        result = re.sub(pattern, '', result, flags=re.IGNORECASE)
    
    # Phase 4: Remove trailing metadata
    result = re.sub(r'\(\d{4}\)[.:,]?\s*$', '', result)  # trailing (1911).
    result = re.sub(r',\s*Ä‘á»‹a Ä‘iá»ƒm\s+.+$', '', result)   # trailing ", Ä‘á»‹a Ä‘iá»ƒm HÃ  Ná»™i"
    result = re.sub(r'\s+thuá»™c\s+.+\d{4}[.,]?\s*$', '', result)  # trailing "thuá»™c X 1945."
    
    return result.strip()


def extract_core_keywords(text: str) -> set:
    """
    Extract core keywords from event text for fuzzy deduplication.
    """
    if not text:
        return set()
    
    # Common words to ignore
    stop_words = {
        "nÄƒm", "cá»§a", "vÃ ", "trong", "lÃ ", "cÃ³", "Ä‘Æ°á»£c", "vá»›i", "cÃ¡c", "nhá»¯ng",
        "diá»…n", "ra", "vÃ o", "xáº£y", "ká»ƒ", "vá»", "tÃ³m", "táº¯t", "gÃ¬", "nÃ o",
        "bá»‘i", "cáº£nh", "biáº¿n", "káº¿t", "quáº£", "gáº¯n", "má»‘c", "thá»i", "ká»³",
        "sá»±", "kiá»‡n", "lá»‹ch", "sá»­", "viá»‡t", "nam", "the", "of", "and", "in",
        "cÃ¢u", "há»i", "nháº¯m", "tá»›i"
    }
    
    normalized = re.sub(r'[^\w\s]', ' ', text.lower())
    words = normalized.split()
    keywords = {w for w in words if len(w) > 2 and w not in stop_words}
    return keywords


from difflib import SequenceMatcher

def compute_text_similarity(text1: str, text2: str) -> float:
    """Compute similarity between two texts using SequenceMatcher."""
    return SequenceMatcher(None, text1, text2).ratio()

def deduplicate_and_enrich(raw_events: list, max_events: int = MAX_TOTAL_EVENTS) -> list:
    """
    Deduplicate events and enrich with complete information.
    Aggressively merges similar events to prevent repetition.
    """
    if not raw_events:
        return []
    
    # Group events by year
    by_year = {}
    for e in raw_events:
        year = e.get("year", 0)
        if year not in by_year:
            by_year[year] = []
        by_year[year].append(e)
    
    result_events = []
    
    for year in sorted(by_year.keys()):
        year_events = by_year[year]
        if not year_events:
            continue

        # Sort by content length (descending) to prefer longer, detailed stories as base 
        year_events.sort(key=lambda x: len(x.get("story", "") or x.get("event", "")), reverse=True)
        
        unique_cluster = []
        
        for event in year_events:
            event_text = clean_story_text(event.get("story", "") or event.get("event", ""))
            event_lower = event_text.lower()
            
            is_duplicate = False
            
            for cluster_item in unique_cluster:
                base_event = cluster_item["event"]
                base_text = clean_story_text(base_event.get("story", "") or base_event.get("event", ""))
                base_lower = base_text.lower()
                
                # Check for containment or high similarity
                if (event_lower in base_lower or base_lower in event_lower):
                    is_duplicate = True
                else:
                    sim = compute_text_similarity(event_lower, base_lower)
                    if sim > 0.5:  # Tuned threshold: 0.3 too aggressive, 0.6 too loose
                        is_duplicate = True
                
                if is_duplicate:
                    # Merge info into base_event (the longer one usually)
                    # Merge persons/places
                    current_persons = set(base_event.get("persons", []))
                    current_persons.update(event.get("persons", []))
                    base_event["persons"] = list(current_persons)
                    
                    current_places = set(base_event.get("places", []))
                    current_places.update(event.get("places", []))
                    base_event["places"] = list(current_places)
                    
                    # Keep the absolute longest story text
                    if len(event_text) > len(base_text):
                         base_event["story"] = event.get("story", "")
                         base_event["event"] = event.get("event", "")
                    
                    break # Found a match, stop checking other clusters
            
            if not is_duplicate:
                unique_cluster.append({"event": event, "text": event_text})
        
        # Add enriched unique events from this year
        for item in unique_cluster:
            result_events.append(item["event"])
            
        if len(result_events) >= max_events:
            break
            
    return result_events[:max_events]


def format_complete_answer(events: list) -> str:
    """
    Format events into a concise answer, grouped by year.
    Avoids duplication and produces natural-sounding Vietnamese text.
    """
    if not events:
        return None
    
    # Group by year for cleaner output
    by_year = {}
    for e in events:
        year = e.get("year")
        if year not in by_year:
            by_year[year] = []
        by_year[year].append(e)
    
    paragraphs = []
    
    # Sort years
    sorted_years = sorted(by_year.keys()) if all(isinstance(y, int) for y in by_year.keys() if y) else by_year.keys()
    
    seen_texts = set()  # Prevent exact duplicate sentences across years
    
    for year in sorted_years:
        year_events = by_year[year]
        event_texts = []
        
        for e in year_events:
            # Prefer story (longer, more detailed), fallback to event
            story = e.get("story", "") or e.get("event", "")
            clean_story = clean_story_text(story, year)
            
            # Skip if empty
            if not clean_story:
                continue
            
            # Extract title for context if available
            title = e.get("title", "")
            clean_title = clean_story_text(title, year) if title else ""
            
            # If story is very short or same as title, try to combine
            if clean_title and clean_story and clean_title.lower() != clean_story.lower():
                # Check if title is already part of the story
                if clean_title.lower() not in clean_story.lower():
                    clean_story = f"{clean_title}: {clean_story}"
            
            # Capitalize first letter
            clean_story = clean_story[0].upper() + clean_story[1:]
            
            # Ensure ends with punctuation
            if not clean_story.endswith(('.', '!', '?')):
                clean_story += "."
            
            # Dedup check AFTER normalization so key is consistent
            dedup_key = clean_story.lower().strip()
            if dedup_key in seen_texts:
                continue
            seen_texts.add(clean_story.lower())
            event_texts.append(clean_story)
        
        if event_texts:
            joined_events = " ".join(event_texts)
            if year:
                paragraphs.append(f"**NÄƒm {year}:** {joined_events}")
            else:
                paragraphs.append(joined_events)
            
    return "\n\n".join(paragraphs) if paragraphs else None


def engine_answer(query: str):
    q = query.lower()

    # Handle creator queries â€” "ai táº¡o ra báº¡n?", "ai phÃ¡t triá»ƒn báº¡n?"
    # Check BEFORE identity to avoid 'báº¡n lÃ  ai' substring matching
    if any(pattern in q for pattern in CREATOR_PATTERNS):
        return {
            "query": query,
            "intent": "creator",
            "answer": CREATOR_RESPONSE,
            "events": [],
            "no_data": False
        }

    # Handle identity queries â€” "báº¡n lÃ  ai?", "giá»›i thiá»‡u báº£n thÃ¢n"
    if any(pattern in q for pattern in IDENTITY_PATTERNS):
        return {
            "query": query,
            "intent": "identity",
            "answer": IDENTITY_RESPONSE,
            "events": [],
            "no_data": False
        }

    intent = "semantic"
    raw_events = []
    is_dynasty_query = False
    is_range_query = False

    # Detect intent â€” priority: year_range > multi_year > dynasty > definition > single_year > semantic
    year_range = extract_year_range(query)
    multi_years = extract_multiple_years(query)
    dynasty = detect_dynasty_from_query(query)
    place = detect_place_from_query(query)

    if year_range:
        # Year range query: "tá»« nÄƒm 1225 Ä‘áº¿n 1400"
        start_yr, end_yr = year_range
        intent = "year_range"
        is_range_query = True
        raw_events = scan_by_year_range(start_yr, end_yr)
        # Supplement with semantic search for richer results
        if len(raw_events) < 3:
            raw_events.extend(semantic_search(query))
    elif multi_years:
        # Multiple years: "nÄƒm 938 vÃ  nÄƒm 1288"
        intent = "multi_year"
        is_range_query = True
        for yr in multi_years:
            raw_events.extend(scan_by_year(yr))
        # Also add semantic results for context
        raw_events.extend(semantic_search(query))
    elif dynasty or place:
        # Dynasty/place query â€” use semantic search with filters
        intent = "dynasty" if dynasty else "place"
        is_dynasty_query = True
        raw_events = semantic_search(query)
    elif "lÃ  gÃ¬" in q or "lÃ  ai" in q:
        intent = "definition"
        raw_events = semantic_search(query)
    else:
        year = extract_single_year(query)
        if year:
            intent = "year"
            raw_events = scan_by_year(year)
        else:
            intent = "semantic"
            raw_events = semantic_search(query)

    no_data = not raw_events

    # Use higher event limit for range/dynasty queries
    if is_range_query:
        max_events = MAX_TOTAL_EVENTS_RANGE
    elif is_dynasty_query:
        max_events = MAX_TOTAL_EVENTS_DYNASTY
    else:
        max_events = MAX_TOTAL_EVENTS

    # Deduplicate and enrich events
    unique_events = deduplicate_and_enrich(raw_events, max_events) if not no_data else []
    
    # Generate complete, comprehensive answer
    answer = format_complete_answer(unique_events)

    return {
        "query": query,
        "intent": intent,
        "answer": answer,
        "events": unique_events,  # Return deduplicated, enriched events
        "no_data": no_data
    }
