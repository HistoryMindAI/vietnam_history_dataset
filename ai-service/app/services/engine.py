from app.services.search_service import (
    semantic_search, scan_by_year, scan_by_year_range,
    detect_dynasty_from_query, detect_place_from_query,
    resolve_query_entities, scan_by_entities,
    scan_by_dynasty_timeline, scan_national_resistance,
    scan_territorial_conflicts, scan_civil_wars, scan_broad_history,
    DYNASTY_ORDER,
)
from app.services.query_understanding import (
    rewrite_query, extract_question_intent,
    generate_search_variations,
)
from app.services.cross_encoder_service import (
    filter_and_rank_events,
    validate_answer_relevance,
)
from app.services.nli_validator_service import validate_events_nli
from app.services.semantic_intent import (
    classify_semantic_intent, SemanticIntent,
)
from app.services.implicit_context import (
    expand_query_with_implicit_context,
    filter_discriminating_keywords,
    is_vietnam_scope_query,
    is_broad_vietnam_query,
    has_resistance_terms,
    NON_DISCRIMINATING_KEYWORDS,
)
import app.core.startup as startup
import re

# Pre-compile regex for faster matching
YEAR_PATTERN = re.compile(r"(?<![\d-])([1-9][0-9]{1,3})(?!\d)")

# Year range patterns - support multiple formats
YEAR_RANGE_PATTERNS = [
    # "t·ª´ nƒÉm 40 ƒë·∫øn nƒÉm 2025"
    re.compile(
        r"(?:t·ª´\s*(?:nƒÉm\s*)?|giai\s*ƒëo·∫°n\s*)"
        r"(\d{1,4})"
        r"\s*(?:ƒë·∫øn|t·ªõi|[-‚Äì‚Äî])\s*(?:nƒÉm\s*)?"
        r"(\d{1,4})",
        re.IGNORECASE
    ),
    # "nƒÉm 40 ƒë·∫øn 2025"
    re.compile(
        r"nƒÉm\s+(\d{1,4})\s+(?:ƒë·∫øn|t·ªõi|[-‚Äì‚Äî])\s+(?:nƒÉm\s*)?(\d{1,4})",
        re.IGNORECASE
    ),
    # "40-2025", "40 ƒë·∫øn 2025"
    re.compile(
        r"\b(\d{1,4})\s*(?:ƒë·∫øn|t·ªõi|[-‚Äì‚Äî])\s*(\d{1,4})\b",
        re.IGNORECASE
    ),
    # "from 40 to 2025"
    re.compile(
        r"from\s+(\d{1,4})\s+to\s+(\d{1,4})",
        re.IGNORECASE
    ),
    # "between 40 and 2025"
    re.compile(
        r"between\s+(\d{1,4})\s+and\s+(\d{1,4})",
        re.IGNORECASE
    ),
]


def extract_single_year(text: str):
    """
    Extracts a single year between 40 and 2025 from text.
    """
    m = YEAR_PATTERN.search(text)
    if m:
        year = int(m.group(1))
        if 40 <= year <= 2025:
            return year
    return None


def extract_year_range(text: str):
    """
    Extracts a year range from text with multiple format support.
    
    Supported formats:
    - "t·ª´ nƒÉm 40 ƒë·∫øn nƒÉm 2025"
    - "nƒÉm 40 ƒë·∫øn 2025"
    - "40-2025", "40 ƒë·∫øn 2025"
    - "from 40 to 2025"
    - "between 40 and 2025"
    - "giai ƒëo·∫°n 40-2025"
    
    Returns (start_year, end_year) or None.
    """
    for pattern in YEAR_RANGE_PATTERNS:
        m = pattern.search(text)
        if m:
            start = int(m.group(1))
            end = int(m.group(2))
            
            # Validate year range - minimum year is 40 (Hai B√† Tr∆∞ng)
            if 40 <= start <= 2025 and 40 <= end <= 2025 and start < end:
                return (start, end)
    
    return None


def extract_multiple_years(text: str):
    """
    Extracts multiple distinct years from text.
    Returns list of years if 2+ found, else None.
    E.g.: 'nƒÉm 938 v√† nƒÉm 1288' ‚Üí [938, 1288]
    """
    # First check if this is a year range query (handled separately)
    if extract_year_range(text):
        return None

    matches = YEAR_PATTERN.findall(text)
    years = []
    for m in matches:
        y = int(m)
        if 40 <= y <= 2025 and y not in years:
            years.append(y)
    return sorted(years) if len(years) >= 2 else None


MAX_EVENTS_PER_YEAR = 1
MAX_TOTAL_EVENTS = 5
MAX_TOTAL_EVENTS_DYNASTY = 10  # More results for dynasty-level queries
MAX_TOTAL_EVENTS_RANGE = 15   # More results for year range queries
MAX_TOTAL_EVENTS_ENTITY = 10  # Results for multi-entity queries (person + topic)
MIN_CLEAN_TEXT_LENGTH = 15    # Minimum text length after cleaning (filter metadata noise)

# Relationship patterns ‚Äî is X related to Y?
RELATIONSHIP_PATTERNS = [
    "l√† g√¨ c·ªßa nhau", "c√≥ quan h·ªá g√¨", "li√™n quan g√¨",
    "l√† ai c·ªßa", "v√† .+ l√†",
    # Unaccented fallbacks for queries without diacritics
    "la gi cua nhau", "co quan he gi", "lien quan gi",
    "la ai cua",
]

# Greeting patterns ‚Äî casual conversation
GREETING_PATTERNS = [
    # English greetings - EXACT MATCH to avoid false positives
    r'\bhello\b', r'\bhi\b(?!\s+\w)', r'\bhey\b', 
    r'\bgood morning\b', r'\bgood afternoon\b', r'\bgood evening\b',
    r'\bhow are you\b', r'\bwhat\'s up\b', r'\bhow do you do\b', r'\bnice to meet you\b',
    # Vietnamese greetings - EXACT MATCH
    r'\bxin ch√†o\b', r'\bch√†o b·∫°n\b', r'\bch√†o\b(?!\s+\w)', 
    r'\bch√†o bu·ªïi s√°ng\b', r'\bch√†o bu·ªïi chi·ªÅu\b', 
    r'\bch√†o bu·ªïi t·ªëi\b', r'\bb·∫°n kh·ªèe kh√¥ng\b', r'\bb·∫°n c√≥ kh·ªèe kh√¥ng\b', r'\bkh·ªèe kh√¥ng\b',
    r'\bd·∫°o n√†y th·∫ø n√†o\b', r'\bh√¥m nay th·∫ø n√†o\b', r'\bb·∫°n th·∫ø n√†o\b', r'\bm·ªçi vi·ªác th·∫ø n√†o\b',
    r'\br·∫•t vui ƒë∆∞·ª£c g·∫∑p\b', r'\bh√¢n h·∫°nh\b', r'\bch√†o m·ª´ng\b(?!\s+\w)',
    # Casual Vietnamese
    r'\balo\b', r'\bal√¥\b', r'\bh·∫ø l√¥\b', r'\bh√™ l√¥\b', r'\bh√™ nh√¥\b', r'\bh√™ l√¥ b·∫°n\b',
    r'\bch√†o c·∫≠u\b', r'\bch√†o m·ª´ng b·∫°n\b', r'\bch√†o m·ª´ng ƒë·∫øn v·ªõi\b',
]

# Thank you patterns
THANK_PATTERNS = [
    r'\bthank you\b', r'\bthanks\b', r'\bthank\b', r'\bthx\b', r'\bty\b',
    r'\bc·∫£m ∆°n\b', r'\bc√°m ∆°n\b', r'\bthanks b·∫°n\b', r'\bc·∫£m ∆°n b·∫°n\b', r'\bc·∫£m ∆°n nhi·ªÅu\b',
    r'\bc·∫£m ∆°n b·∫°n nhi·ªÅu\b', r'\bthanks nhi·ªÅu\b', r'\bc·∫£m ∆°n nh√©\b', r'\bc·∫£m ∆°n nha\b',
    r'\bc·∫£m ∆°n r·∫•t nhi·ªÅu\b', r'\bxin c·∫£m ∆°n\b',
]

# Goodbye patterns
GOODBYE_PATTERNS = [
    r'\bbye\b', r'\bgoodbye\b', r'\bsee you\b', r'\bsee ya\b', r'\bfarewell\b', r'\btake care\b',
    r'\bt·∫°m bi·ªát\b', r'\bch√†o t·∫°m bi·ªát\b', r'\bh·∫πn g·∫∑p l·∫°i\b', r'\bg·∫∑p l·∫°i\b', r'\bbye bye\b',
    r'\bb√°i bai\b', r'\bt·∫°m bi·ªát nh√©\b', r'\bch√†o nh√©\b', r'\bƒëi ƒë√¢y\b', r'\bƒëi nh√©\b',
]

# Identity patterns ‚Äî who are you?
IDENTITY_PATTERNS = [
    "who are you", "b·∫°n l√† ai", "gi·ªõi thi·ªáu b·∫£n th√¢n",
    "what is your name", "t√™n b·∫°n l√† g√¨", "t√™n c·ªßa b·∫°n",
    "you are who", "gi·ªõi thi·ªáu v·ªÅ b·∫°n", "b·∫°n t√™n g√¨",
    "h√£y gi·ªõi thi·ªáu", "cho t√¥i bi·∫øt v·ªÅ b·∫°n",
]

# Creator patterns ‚Äî who made you?
CREATOR_PATTERNS = [
    "ai t·∫°o ra", "ai ph√°t tri·ªÉn", "ai x√¢y d·ª±ng", "ai l√†m ra",
    "created by", "made by", "developed by", "built by",
    "t·∫°o ra b·∫°n", "ph√°t tri·ªÉn b·∫°n", "x√¢y d·ª±ng b·∫°n",
    "ai t·∫°o b·∫°n", "ai ƒë√£ t·∫°o", "do ai", "ƒë∆∞·ª£c t·∫°o b·ªüi",
    "t√°c gi·∫£", "nh√† ph√°t tri·ªÉn", "developer",
    "ƒë∆∞·ª£c t·∫°o ra th·∫ø n√†o", "t·∫°o ra th·∫ø n√†o", "ƒë∆∞·ª£c t·∫°o th·∫ø n√†o",
]

IDENTITY_RESPONSE = (
    "Xin ch√†o! T√¥i l√† **History Mind AI** ‚Äî tr·ª£ l√Ω l·ªãch s·ª≠ Vi·ªát Nam.\n\n"
    "T√¥i ƒë∆∞·ª£c t·∫°o ra v·ªõi mong mu·ªën gi√∫p b·∫°n kh√°m ph√° "
    "4.000 nƒÉm l·ªãch s·ª≠ d√¢n t·ªôc m·ªôt c√°ch d·ªÖ d√†ng v√† sinh ƒë·ªông.\n\n"
    "B·∫°n c√≥ th·ªÉ h·ªèi t√¥i v·ªÅ:\n\n"
    "- Tra c·ª©u s·ª± ki·ªán theo nƒÉm, tri·ªÅu ƒë·∫°i ho·∫∑c nh√¢n v·∫≠t\n"
    "- Nh·ªØng tr·∫≠n chi·∫øn n·ªïi ti·∫øng ‚Äî B·∫°ch ƒê·∫±ng, Chi LƒÉng, ƒêi·ªán Bi√™n Ph·ªß\n"
    "- C√°c tri·ªÅu ƒë·∫°i ‚Äî L√Ω, Tr·∫ßn, L√™, Nguy·ªÖn\n"
    "- So s√°nh c√°c giai ƒëo·∫°n l·ªãch s·ª≠\n\n"
    "H√£y th·ª≠ ƒë·∫∑t c√¢u h·ªèi, t√¥i s·∫µn s√†ng gi√∫p b·∫°n!"
)

GREETING_RESPONSE = (
    "Xin ch√†o! üëã\n\n"
    "T√¥i l√† **History Mind AI** ‚Äî tr·ª£ l√Ω l·ªãch s·ª≠ Vi·ªát Nam c·ªßa b·∫°n.\n\n"
    "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n kh√°m ph√° 4.000 nƒÉm l·ªãch s·ª≠ d√¢n t·ªôc. "
    "H√£y th·ª≠ h·ªèi t√¥i v·ªÅ:\n\n"
    "- C√°c s·ª± ki·ªán l·ªãch s·ª≠: *\"Tr·∫≠n B·∫°ch ƒê·∫±ng nƒÉm 1288\"*\n"
    "- Nh√¢n v·∫≠t anh h√πng: *\"Ai l√† Tr·∫ßn H∆∞ng ƒê·∫°o?\"*\n"
    "- Tri·ªÅu ƒë·∫°i: *\"K·ªÉ v·ªÅ nh√† Tr·∫ßn\"*\n"
    "- So s√°nh: *\"So s√°nh nh√† L√Ω v√† nh√† Tr·∫ßn\"*\n\n"
    "B·∫°n mu·ªën t√¨m hi·ªÉu v·ªÅ ƒëi·ªÅu g√¨?"
)

THANK_RESPONSE = (
    "R·∫•t vui ƒë∆∞·ª£c gi√∫p b·∫°n! üòä\n\n"
    "N·∫øu b·∫°n c√≥ th√™m c√¢u h·ªèi v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam, "
    "ƒë·ª´ng ng·∫°i h·ªèi t√¥i nh√©!"
)

GOODBYE_RESPONSE = (
    "T·∫°m bi·ªát! üëã\n\n"
    "H·∫πn g·∫∑p l·∫°i b·∫°n. Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh!\n\n"
    "N·∫øu c·∫ßn t√¨m hi·ªÉu th√™m v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam, "
    "t√¥i lu√¥n s·∫µn s√†ng gi√∫p ƒë·ª°."
)

CREATOR_RESPONSE = (
    "T√¥i ƒë∆∞·ª£c x√¢y d·ª±ng b·ªüi **V√µ ƒê·ª©c Hi·∫øu** (h1eudayne), "
    "m·ªôt sinh vi√™n ƒëam m√™ c√¥ng ngh·ªá AI v√† l·ªãch s·ª≠ Vi·ªát Nam.\n\n"
    "**V·ªÅ t√°c gi·∫£**\n\n"
    "- Thi·∫øt k·∫ø v√† ph√°t tri·ªÉn to√†n b·ªô h·ªá th·ªëng t·ª´ √Ω t∆∞·ªüng ƒë·∫øn s·∫£n ph·∫©m\n"
    "- X√¢y d·ª±ng b·ªô d·ªØ li·ªáu h∆°n 1.000.000 m·∫´u l·ªãch s·ª≠ Vi·ªát Nam\n"
    "- Hu·∫•n luy·ªán m√¥ h√¨nh AI hi·ªÉu ti·∫øng Vi·ªát t·ª± nhi√™n\n"
    "- Ph√°t tri·ªÉn giao di·ªán web\n\n"
    "**C√¥ng ngh·ªá s·ª≠ d·ª•ng**\n\n"
    "- T√¨m ki·∫øm ng·ªØ nghƒ©a (Semantic Search)\n"
    "- FAISS + Embeddings cho truy v·∫•n vector nhanh\n"
    "- D·ªØ li·ªáu t·ª´ th·ªùi H√πng V∆∞∆°ng ƒë·∫øn hi·ªán ƒë·∫°i\n\n"
    "**Li√™n h·ªá**\n\n"
    "- GitHub: [h1eudayne](https://github.com/h1eudayne?tab=repositories)\n"
    "- Facebook: [V√µ ƒê·ª©c Hi·∫øu](https://www.facebook.com/vo.duc.hieu2005/)\n"
    "- Email: voduchieu42@gmail.com\n"
    "- Phone: 0915106276"
)


def clean_story_text(text: str, year: int | None = None) -> str:
    """
    Clean up story text by removing redundant prefixes and making it a complete sentence.
    Handles various data patterns from the Vietnam history dataset.
    """
    if not text:
        return ""
    
    result = text.strip()
    
    # Phase 1: Remove structural/query-style prefixes (these are data artifacts, not content)
    structural_patterns = [
        r'^C√¢u h·ªèi nh·∫Øm t·ªõi s·ª± ki·ªán\s*',
        r'^T√≥m t·∫Øt b·ªëi c·∫£nh\s*‚Äì\s*di·ªÖn bi·∫øn\s*‚Äì\s*k·∫øt qu·∫£ c·ªßa\s*',
        r'^B·ªëi c·∫£nh:\s*',
        r'^K·ªÉ v·ªÅ .+ v√† ƒë√≥ng g√≥p c·ªßa .+ trong\s*',
    ]
    for pattern in structural_patterns:
        result = re.sub(pattern, '', result, flags=re.IGNORECASE)
    
    # Phase 1b: Remove semicolon-style summary prefixes
    # Pattern: "Event di·ªÖn ra nƒÉm 1960; Description..." ‚Üí keep only Description
    # Pattern: "Event x·∫£y ra nƒÉm 1284; Description..." ‚Üí keep only Description
    result = re.sub(r'^.+\s+di·ªÖn ra nƒÉm\s+\d{3,4};\s*', '', result, flags=re.IGNORECASE)
    result = re.sub(r'^.+\s+x·∫£y ra nƒÉm\s+\d{3,4};\s*', '', result, flags=re.IGNORECASE)
    
    # Phase 1c: Remove event-title prefix patterns
    # Pattern: "Event (1284): Description" ‚Üí keep only Description
    result = re.sub(r'^.+\(\d{4}\):\s*', '', result, flags=re.IGNORECASE)
    # Pattern: "H·ªãch t∆∞·ªõng sƒ© (1284)." ‚Üí remove if it's just a bare title+year
    # Only match short text (< 80 chars) to avoid stripping full sentences
    if len(result) < 80:
        result = re.sub(r'^[^.;!?]+\(\d{4}\)\.?\s*$', '', result, flags=re.IGNORECASE)
    
    # Phase 2: Remove year prefixes to avoid "NƒÉm 1930: NƒÉm 1930, ..." duplication
    year_prefixes = [
        r'^NƒÉm \d+[,:]?\s*',
        r'^V√†o nƒÉm \d+[,:]?\s*',
        r'^nƒÉm \d+[,:]?\s*',
        r'^\d{3,4}[,:]\s*',
    ]
    for pattern in year_prefixes:
        result = re.sub(pattern, '', result, flags=re.IGNORECASE)
    
    # Phase 3: Remove action-style prefixes
    action_prefixes = [
        r'^g·∫Øn m·ªëc \d+ v·ªõi\s*',
        r'^di·ªÖn ra\s*',
        r'^x·∫£y ra\s*',
    ]
    for pattern in action_prefixes:
        result = re.sub(pattern, '', result, flags=re.IGNORECASE)
    
    # Phase 4: Remove trailing metadata
    result = re.sub(r'\(\d{4}\)[.:,]?\s*$', '', result)  # trailing (1911).
    result = re.sub(r',\s*ƒë·ªãa ƒëi·ªÉm\s+.+$', '', result)   # trailing ", ƒë·ªãa ƒëi·ªÉm H√† N·ªôi"
    result = re.sub(r'\s+thu·ªôc\s+.+\d{4}[.,]?\s*$', '', result)  # trailing "thu·ªôc X 1945."
    
    return result.strip()


def extract_core_keywords(text: str) -> set:
    """
    Extract core keywords from event text for fuzzy deduplication.
    """
    if not text:
        return set()
    
    # Common words to ignore
    stop_words = {
        "nƒÉm", "c·ªßa", "v√†", "trong", "l√†", "c√≥", "ƒë∆∞·ª£c", "v·ªõi", "c√°c", "nh·ªØng",
        "di·ªÖn", "ra", "v√†o", "x·∫£y", "k·ªÉ", "v·ªÅ", "t√≥m", "t·∫Øt", "g√¨", "n√†o",
        "b·ªëi", "c·∫£nh", "bi·∫øn", "k·∫øt", "qu·∫£", "g·∫Øn", "m·ªëc", "th·ªùi", "k·ª≥",
        "s·ª±", "ki·ªán", "l·ªãch", "s·ª≠", "vi·ªát", "nam", "the", "of", "and", "in",
        "c√¢u", "h·ªèi", "nh·∫Øm", "t·ªõi"
    }
    
    normalized = re.sub(r'[^\w\s]', ' ', text.lower())
    words = normalized.split()
    keywords = {w for w in words if len(w) > 2 and w not in stop_words}
    return keywords


from difflib import SequenceMatcher

def compute_text_similarity(text1: str, text2: str) -> float:
    """Compute similarity between two texts using SequenceMatcher."""
    return SequenceMatcher(None, text1, text2).ratio()

def _is_similar_event(text1_lower: str, text2_lower: str, kw1: set | None = None, kw2: set | None = None) -> bool:
    """
    Check if two cleaned event texts are similar enough to be considered duplicates.
    Uses multiple strategies: containment, SequenceMatcher, and keyword overlap.
    """
    # Strategy 1: Direct containment
    if text1_lower in text2_lower or text2_lower in text1_lower:
        return True
    
    # Strategy 2: SequenceMatcher similarity
    sim = compute_text_similarity(text1_lower, text2_lower)
    if sim > 0.6:
        return True
    
    # Strategy 3: Keyword-based Jaccard overlap (catches reformulated sentences)
    if kw1 is not None and kw2 is not None and kw1 and kw2:
        intersection = kw1 & kw2
        union = kw1 | kw2
        jaccard = len(intersection) / len(union) if union else 0
        if jaccard > 0.7:
            return True
    
    return False


def deduplicate_and_enrich(raw_events: list, max_events: int = MAX_TOTAL_EVENTS) -> list:
    """
    Deduplicate events and enrich with complete information.
    Aggressively merges similar events to prevent repetition.
    Uses GLOBAL cross-year dedup to catch same-event across different year groups.
    """
    if not raw_events:
        return []
    
    # Group events by year
    by_year = {}
    for e in raw_events:
        year = e.get("year", 0)
        if year not in by_year:
            by_year[year] = []
        by_year[year].append(e)
    
    # Global cluster for cross-year dedup
    global_cluster = []  # [{"event": doc, "text": cleaned, "text_lower": lower, "keywords": set}]
    
    for year in sorted(by_year.keys()):
        year_events = by_year[year]
        if not year_events:
            continue

        # Sort by content length (descending) to prefer longer, detailed stories as base 
        year_events.sort(key=lambda x: len(x.get("story", "") or x.get("event", "")), reverse=True)
        
        for event in year_events:
            event_text = clean_story_text(event.get("story", "") or event.get("event", ""))
            
            # Filter out texts that are too short after cleaning (metadata noise)
            if len(event_text.strip()) < MIN_CLEAN_TEXT_LENGTH:
                continue
            
            event_lower = event_text.lower()
            event_keywords = extract_core_keywords(event_text)
            
            is_duplicate = False
            
            # Compare against ALL previously accepted events (global cross-year dedup)
            for cluster_item in global_cluster:
                base_event = cluster_item["event"]
                base_lower = cluster_item["text_lower"]
                base_keywords = cluster_item["keywords"]
                
                if _is_similar_event(event_lower, base_lower, event_keywords, base_keywords):
                    is_duplicate = True
                    
                    # Merge info into base_event (the longer one usually)
                    current_persons = set(base_event.get("persons", []))
                    current_persons.update(event.get("persons", []))
                    base_event["persons"] = list(current_persons)
                    
                    current_places = set(base_event.get("places", []))
                    current_places.update(event.get("places", []))
                    base_event["places"] = list(current_places)
                    
                    # Keep the absolute longest story text
                    base_text = cluster_item["text"]
                    if len(event_text) > len(base_text):
                        base_event["story"] = event.get("story", "")
                        base_event["event"] = event.get("event", "")
                        cluster_item["text"] = event_text
                        cluster_item["text_lower"] = event_lower
                        cluster_item["keywords"] = event_keywords
                    
                    break  # Found a match, stop checking other clusters
            
            if not is_duplicate:
                global_cluster.append({
                    "event": event,
                    "text": event_text,
                    "text_lower": event_lower,
                    "keywords": event_keywords,
                })
            
            if len(global_cluster) >= max_events:
                break
        
        if len(global_cluster) >= max_events:
            break
    
    return [item["event"] for item in global_cluster[:max_events]]


# Pattern to detect question/prompt titles dynamically
_QUESTION_TITLE_RE = re.compile(
    r'(?:'
    r'k·ªÉ t√™n|t√≥m t·∫Øt|v√¨ sao|t·∫°i sao|v√¨ l√Ω do g√¨|'
    r'ai l√†|ƒëi·ªÅu g√¨|h√£y cho bi·∫øt|n√™u|gi·∫£i th√≠ch|'
    r'b·ªëi c·∫£nh n√†o|h·∫≠u qu·∫£|t√°c ƒë·ªông|vai tr√≤|'
    r'quan tr·ªçng ƒë·ªëi v·ªõi|√Ω nghƒ©a|k·∫øt qu·∫£ ra sao|'
    r'x·∫£y ra khi n√†o|di·ªÖn bi·∫øn|li·ªát k√™|m√¥ t·∫£|'
    r'so s√°nh|ph√¢n t√≠ch|nh√¢n v·∫≠t trung t√¢m|'
    r's·ª± ki·ªán n·ªïi b·∫≠t|c√≥ √Ω nghƒ©a l·ªãch s·ª≠|'
    r'trong nƒÉm \d{3,4}|·ªü vi·ªát nam'
    r')',
    re.IGNORECASE,
)


def _is_question_title(title: str) -> bool:
    """Dynamically detect if a title is actually a question/prompt."""
    if not title:
        return False
    t = title.lower().strip()
    if t.endswith('?'):
        return True
    return bool(_QUESTION_TITLE_RE.search(t))


def format_complete_answer(events: list, group_by: str = "year") -> str:
    """
    Format events into a concise answer.
    Supports two grouping modes:
      - "year" (default): group by year for chronological output
      - "dynasty": group by dynasty for dynasty-timeline output
    Avoids duplication and produces natural-sounding Vietnamese text.
    Dynamically detects and skips question-style titles.
    """
    if not events:
        return None

    if group_by == "dynasty":
        return _format_by_dynasty(events)

    return _format_by_year(events)


def _format_event_text(e: dict, year=None, seen_texts: set = None) -> str | None:
    """Format a single event into clean text. Returns None if duplicate."""
    story = e.get("story", "") or e.get("event", "")
    clean_story = clean_story_text(story, year)

    if not clean_story:
        return None

    title = e.get("title", "")
    clean_title = clean_story_text(title, year) if title else ""

    if clean_title and clean_story and clean_title.lower() != clean_story.lower():
        if not _is_question_title(clean_title):
            if clean_title.lower() not in clean_story.lower():
                clean_story = f"{clean_title}: {clean_story}"

    clean_story = clean_story[0].upper() + clean_story[1:]
    if not clean_story.endswith(('.', '!', '?')):
        clean_story += "."

    dedup_key = re.sub(r'[^\w\s]', '', clean_story.lower()).strip()
    dedup_key = re.sub(r'\s+', ' ', dedup_key)

    if seen_texts is not None:
        if dedup_key in seen_texts:
            return None
        seen_texts.add(dedup_key)

    return clean_story


def _format_by_year(events: list) -> str | None:
    """Group events by year (original behavior)."""
    by_year = {}
    for e in events:
        year = e.get("year")
        if year not in by_year:
            by_year[year] = []
        by_year[year].append(e)

    paragraphs = []
    sorted_years = sorted(by_year.keys()) if all(isinstance(y, int) for y in by_year.keys() if y) else by_year.keys()
    seen_texts = set()

    for year in sorted_years:
        event_texts = []
        for e in by_year[year]:
            text = _format_event_text(e, year, seen_texts)
            if text:
                event_texts.append(text)
        if event_texts:
            joined = " ".join(event_texts)
            if year:
                paragraphs.append(f"**NƒÉm {year}:** {joined}")
            else:
                paragraphs.append(joined)

    return "\n\n".join(paragraphs) if paragraphs else None


def _format_by_dynasty(events: list) -> str | None:
    """
    Group events by dynasty in canonical order.
    Produces output like:
      **Nh√† Ng√¥ (939):** ...
      **Nh√† ƒêinh (968):** ...
      **Nh√† L√Ω (1009‚Äì1225):** ...
    """
    # Build dynasty ‚Üí events mapping
    by_dynasty: dict[str, list] = {}
    for e in events:
        dynasty = e.get("dynasty", "Kh√°c")
        if dynasty not in by_dynasty:
            by_dynasty[dynasty] = []
        by_dynasty[dynasty].append(e)

    paragraphs = []
    seen_texts = set()

    for dynasty in DYNASTY_ORDER:
        dynasty_events = by_dynasty.get(dynasty, [])
        if not dynasty_events:
            continue

        # Sort events within dynasty by year
        dynasty_events.sort(key=lambda d: d.get("year", 9999))

        event_texts = []
        for e in dynasty_events:
            text = _format_event_text(e, e.get("year"), seen_texts)
            if text:
                event_texts.append(text)

        if event_texts:
            # Create dynasty header with year range
            years = [e.get("year") for e in dynasty_events if e.get("year")]
            if years:
                year_range = f"{min(years)}‚Äì{max(years)}" if min(years) != max(years) else str(min(years))
                header = f"**{dynasty} ({year_range}):**"
            else:
                header = f"**{dynasty}:**"
            paragraphs.append(f"{header} {' '.join(event_texts)}")

    return "\n\n".join(paragraphs) if paragraphs else None


def _filter_by_query_keywords(query: str, events: list) -> list:
    """
    Dynamic keyword relevance filter.
    Scores events by query-word overlap and removes low-scoring outliers.
    Uses relative scoring (remove bottom quartile) instead of absolute threshold
    to handle diverse query types gracefully.
    """
    # Stopwords ‚Äî carry no semantic meaning for filtering
    STOPWORDS = {
        "l√†", "g√¨", "c·ªßa", "v√†", "hay", "ho·∫∑c", "c√≥", "kh√¥ng", "ƒë∆∞·ª£c", "b·ªã",
        "cho", "v·ªõi", "t·ª´", "ƒë·∫øn", "trong", "ngo√†i", "v·ªÅ", "theo", "nh∆∞",
        "h√£y", "k·ªÉ", "n√™u", "li·ªát", "t√≥m", "t·∫Øt", "m√¥", "t·∫£", "gi·∫£i",
        "th√≠ch", "t√¥i", "b·∫°n", "ai", "n√†o", "ƒë√¢u", "sao", "th·∫ø", "nh√©",
        "·∫°", "v·∫≠y", "r·ªìi", "nha", "nh·ªâ", "n√†y", "ƒë√≥", "kia", "·∫•y",
        "nh·ªØng", "c√°c", "m·ªôt", "m·ªçi", "m·ªói", "nhi·ªÅu", "√≠t", "ra",
        "l√™n", "xu·ªëng", "v√†o", "ƒëi", "l·∫°i", "ƒë√£", "ƒëang", "s·∫Ω", "c≈©ng",
        "r·∫•t", "qu√°", "l·∫Øm", "nh·∫•t", "h∆°n",
    }

    q_low = query.lower()
    # Extract meaningful keywords from query (2+ chars, not stopwords)
    query_words = set()
    for word in q_low.split():
        word_clean = word.strip(".,!?;:\"'()[]{}‚Äî‚Äì-")
        if len(word_clean) >= 2 and word_clean not in STOPWORDS:
            query_words.add(word_clean)

    if len(query_words) < 2:
        return events  # Not enough keywords to filter

    # Remove non-discriminating keywords (e.g., "vi·ªát nam" in a VN-history dataset)
    query_words = filter_discriminating_keywords(query_words)

    # Score each event by word overlap with query
    scored = []
    for doc in events:
        doc_text = (
            (doc.get("story", "") or "") + " " +
            (doc.get("event", "") or "") + " " +
            " ".join(doc.get("keywords", []) or [])
        ).lower()

        score = sum(1 for w in query_words if w in doc_text)
        scored.append((doc, score))

    if not scored:
        return events

    # Find the maximum score achieved
    max_score = max(s for _, s in scored)
    if max_score <= 1:
        return events  # All events have low overlap, don't filter

    # Relative threshold: keep events with score >= 50% of max score
    # This removes clear outliers while keeping contextually relevant events
    threshold = max(2, max_score // 2)
    relevant = [doc for doc, score in scored if score >= threshold]

    # Fallback: if too aggressive, keep all events with score > 0
    if not relevant:
        relevant = [doc for doc, score in scored if score > 0]

    return relevant if relevant else events


def _strip_accents(text: str) -> str:
    """Strip Vietnamese diacritical marks for fuzzy matching."""
    import unicodedata
    nfkd = unicodedata.normalize('NFKD', text)
    return ''.join(c for c in nfkd if not unicodedata.combining(c))


def _detect_same_entity(query: str, resolved: dict) -> dict | None:
    """
    Detect if query mentions multiple names that are actually the same entity.
    Dynamically checks ALL alias sources: person_aliases, topic_synonyms, dynasty_aliases.
    Handles both accented and unaccented queries.
    Returns {"entity_type": str, "canonical": str, "names_mentioned": list, "all_aliases": list} or None.
    """
    q_low = query.lower()
    q_stripped = _strip_accents(q_low)

    # Define all alias sources to check dynamically
    # Each entry: (alias_dict, entity_type_label, entity_type_vi)
    alias_sources = [
        (startup.PERSON_ALIASES, "person", "ng∆∞·ªùi"),
        (startup.TOPIC_SYNONYMS, "topic", "ch·ªß ƒë·ªÅ"),
        (startup.DYNASTY_ALIASES, "dynasty", "tri·ªÅu ƒë·∫°i"),
    ]

    for alias_dict, entity_type, entity_type_vi in alias_sources:
        if not alias_dict:
            continue

        # Build complete name ‚Üí canonical mapping for this source
        name_to_canonical = dict(alias_dict)  # alias ‚Üí canonical
        # Add canonical ‚Üí canonical for self-references
        for canonical in set(alias_dict.values()):
            name_to_canonical[canonical] = canonical

        # For persons, also include person index keys
        if entity_type == "person":
            for person_key in startup.PERSONS_INDEX:
                if person_key not in name_to_canonical:
                    name_to_canonical[person_key] = person_key

        # Find all names mentioned in query (longest-first to avoid partial matches)
        mentioned = []
        for name in sorted(name_to_canonical.keys(), key=len, reverse=True):
            if name in q_low or _strip_accents(name) in q_stripped:
                mentioned.append((name, name_to_canonical[name]))

        # Remove substrings ‚Äî "tr·∫ßn" is substring of "tr·∫ßn h∆∞ng ƒë·∫°o"
        filtered = []
        for name, canonical in mentioned:
            is_substring = any(
                name != other_name and name in other_name
                for other_name, _ in mentioned
            )
            if not is_substring:
                filtered.append((name, canonical))

        if len(filtered) >= 2:
            # Check if 2+ distinct names resolve to same canonical entity
            canonical_set = set(m[1] for m in filtered)
            if len(canonical_set) == 1:
                canonical = list(canonical_set)[0]
                all_aliases = [alias for alias, can in alias_dict.items()
                              if can == canonical and alias != canonical]
                return {
                    "entity_type": entity_type,
                    "entity_type_vi": entity_type_vi,
                    "canonical": canonical,
                    "names_mentioned": [m[0] for m in filtered],
                    "all_aliases": all_aliases,
                }

    return None


def _generate_same_entity_response(info: dict) -> str:
    """
    Generate a response explaining multiple names refer to the same entity.
    Works dynamically for any entity type: person, topic, dynasty.
    """
    canonical = info["canonical"]
    names = info["names_mentioned"]
    all_aliases = info.get("all_aliases", [])
    entity_type = info.get("entity_type", "person")

    # Format the names mentioned
    name_parts = [f"**{n.title()}**" for n in names]
    names_str = " v√† ".join(name_parts)

    # Dynamic label based on entity type
    type_labels = {
        "person": ("c√πng m·ªôt ng∆∞·ªùi", "T√™n ch√≠nh", "C√°c t√™n g·ªçi kh√°c"),
        "topic": ("c√πng m·ªôt ch·ªß ƒë·ªÅ / s·ª± ki·ªán", "T√™n ch√≠nh", "C√°c t√™n g·ªçi kh√°c"),
        "dynasty": ("c√πng m·ªôt tri·ªÅu ƒë·∫°i / th·ªùi k·ª≥", "T√™n ch√≠nh", "C√°c t√™n g·ªçi kh√°c"),
    }
    same_label, main_label, alias_label = type_labels.get(
        entity_type, ("c√πng m·ªôt th·ª±c th·ªÉ", "T√™n ch√≠nh", "C√°c t√™n g·ªçi kh√°c")
    )

    response = f"{names_str} l√† **{same_label}**.\n\n"
    response += f"{main_label}: **{canonical.title()}**\n\n"

    if all_aliases:
        alias_str = ", ".join(a.title() for a in all_aliases)
        response += f"{alias_label}: {alias_str}\n\n"

    response += "---\n\nD∆∞·ªõi ƒë√¢y l√† c√°c s·ª± ki·ªán li√™n quan:"
    return response


def engine_answer(query: str):
    # --- STEP 0: Query Understanding (NLU) ---
    # Rewrite query: fix typos, expand abbreviations, restore accents
    rewritten = rewrite_query(query)
    # Use rewritten for all downstream processing
    q = rewritten.lower()
    q_display = query  # Keep original for display

    # Detect high-level question intent for context
    question_intent = extract_question_intent(rewritten)

    # Handle greeting queries ‚Äî "hello", "hi", "xin ch√†o"
    # Use regex for exact matching to avoid false positives
    if any(re.search(pattern, q) for pattern in GREETING_PATTERNS):
        return {
            "query": q_display,
            "intent": "greeting",
            "answer": GREETING_RESPONSE,
            "events": [],
            "no_data": False
        }

    # Handle thank you queries
    if any(re.search(pattern, q) for pattern in THANK_PATTERNS):
        return {
            "query": q_display,
            "intent": "thank",
            "answer": THANK_RESPONSE,
            "events": [],
            "no_data": False
        }

    # Handle goodbye queries
    if any(re.search(pattern, q) for pattern in GOODBYE_PATTERNS):
        return {
            "query": q_display,
            "intent": "goodbye",
            "answer": GOODBYE_RESPONSE,
            "events": [],
            "no_data": False
        }

    # Handle creator queries ‚Äî "ai t·∫°o ra b·∫°n?", "ai ph√°t tri·ªÉn b·∫°n?"
    # Check BEFORE identity to avoid 'b·∫°n l√† ai' substring matching
    if any(pattern in q for pattern in CREATOR_PATTERNS):
        return {
            "query": q_display,
            "intent": "creator",
            "answer": CREATOR_RESPONSE,
            "events": [],
            "no_data": False
        }

    # Handle identity queries ‚Äî "b·∫°n l√† ai?", "gi·ªõi thi·ªáu b·∫£n th√¢n"
    if any(pattern in q for pattern in IDENTITY_PATTERNS):
        return {
            "query": q_display,
            "intent": "identity",
            "answer": IDENTITY_RESPONSE,
            "events": [],
            "no_data": False
        }

    intent = "semantic"
    raw_events = []
    is_dynasty_query = False
    is_range_query = False
    is_entity_query = False
    same_person_info = None
    semantic_group_by = "year"  # Default grouping; "dynasty" for timeline intent

    # Dynamic entity resolution (data-driven, no hardcoded patterns)
    # Uses rewritten query for better entity matching
    resolved = resolve_query_entities(rewritten)
    has_persons = bool(resolved.get("persons"))
    has_topics = bool(resolved.get("topics"))
    has_dynasties = bool(resolved.get("dynasties"))
    has_places = bool(resolved.get("places"))
    has_entities = has_persons or has_topics or has_dynasties or has_places

    # --- STEP 1.5: SEMANTIC INTENT CLASSIFICATION ---
    # Classify query BEFORE keyword-based intent chain.
    # High-confidence semantic intents shortcircuit directly to structured retrieval.
    semantic_intent = classify_semantic_intent(rewritten, resolved)

    if semantic_intent.confidence >= 0.8:
        if semantic_intent.intent == "dynasty_timeline":
            intent = "dynasty_timeline"
            is_dynasty_query = True
            raw_events = scan_by_dynasty_timeline()
            semantic_group_by = "dynasty"
        elif semantic_intent.intent == "resistance_national":
            intent = "resistance_national"
            is_entity_query = True
            raw_events = scan_national_resistance()
        elif semantic_intent.intent == "territorial_event":
            intent = "territorial_event"
            is_entity_query = True
            raw_events = scan_territorial_conflicts()
        elif semantic_intent.intent == "civil_war":
            intent = "civil_war"
            is_entity_query = True
            raw_events = scan_civil_wars()
        elif semantic_intent.intent == "broad_history":
            intent = "broad_history"
            is_dynasty_query = True
            raw_events = scan_broad_history()

    # If semantic intent resolved with results, skip legacy intent chain
    # Otherwise, fall through to existing keyword/entity-based logic
    year_range = None
    multi_years = None

    if not raw_events:
        # Detect intent ‚Äî priority: year_range > multi_year > relationship > definition > entity > single_year > semantic
        year_range = extract_year_range(rewritten)
        multi_years = extract_multiple_years(rewritten)

        # --- SAME-ENTITY DETECTION (Dynamic) ---
        # Detects if 2+ names in query refer to same entity (person, topic, or dynasty)
        # E.g.: "Quang Trung v√† Nguy·ªÖn Hu·ªá" ‚Üí same person
        # E.g.: "M√¥ng C·ªï v√† Nguy√™n M√¥ng" ‚Üí same topic
        if has_persons or has_topics or has_dynasties:
            same_person_info = _detect_same_entity(rewritten, resolved)

        # Detect relationship/definition patterns
        # Check both rewritten (accented) and original (may be unaccented) queries
        q_rewritten = rewritten.lower()
        is_relationship = (any(p in q_rewritten for p in RELATIONSHIP_PATTERNS) or
                           any(p in q for p in RELATIONSHIP_PATTERNS))
        is_definition = ("l√† g√¨" in q_rewritten or "l√† ai" in q_rewritten or
                         "la gi" in q or "la ai" in q)

        if year_range:
            # Year range query: "t·ª´ nƒÉm 1225 ƒë·∫øn 1400"
            start_yr, end_yr = year_range
            intent = "year_range"
            is_range_query = True
            raw_events = scan_by_year_range(start_yr, end_yr)
            # Supplement with semantic search for richer results
            if len(raw_events) < 3:
                raw_events.extend(semantic_search(rewritten))
        elif multi_years:
            # Multiple years: "nƒÉm 938 v√† nƒÉm 1288"
            intent = "multi_year"
            is_range_query = True
            for yr in multi_years:
                raw_events.extend(scan_by_year(yr))
            # Also add semantic results for context
            raw_events.extend(semantic_search(rewritten))
        elif same_person_info and (is_relationship or is_definition):
            # Both "l√† g√¨ c·ªßa nhau" and "l√† ai" with both names ‚Üí same person response
            intent = "relationship"
            is_entity_query = True
            raw_events = scan_by_entities(resolved)

            # --- PERSON-RELEVANCE FILTER ---
            # Keep only docs where the target person appears in doc's persons metadata
            # This prevents docs that merely mention the person in story text (e.g.,
            # "ƒë√°nh b·∫°i T√¢y S∆°n" in Nguy·ªÖn dynasty docs) from polluting results
            if has_persons and raw_events:
                target_persons = set(p.lower() for p in resolved["persons"])
                # Also include all aliases for each target person
                target_with_aliases = set(target_persons)
                for alias, canonical in startup.PERSON_ALIASES.items():
                    if canonical in target_persons:
                        target_with_aliases.add(alias)
                filtered = []
                for doc in raw_events:
                    doc_persons = set(p.lower() for p in doc.get("persons", []))
                    if doc_persons & target_with_aliases:
                        filtered.append(doc)
                if filtered:
                    raw_events = filtered

            if 0 < len(raw_events) < 3:
                raw_events.extend(semantic_search(rewritten))
        elif is_definition and has_persons:
            # "X l√† ai?" ‚Äî use semantic search as primary, entity scan as supplement
            intent = "definition"
            is_entity_query = True
            raw_events = scan_by_entities(resolved)
            if 0 < len(raw_events) < 3:
                raw_events.extend(semantic_search(rewritten))
        elif has_entities:
            # Multi-entity query (data-driven): person, dynasty, topic, place
            # Determines sub-intent for more specific labeling
            if has_persons and has_dynasties:
                intent = "multi_entity"
            elif has_persons:
                intent = "person"
            elif has_dynasties:
                intent = "dynasty"
                is_dynasty_query = True
            elif has_places:
                intent = "place"
            elif has_topics:
                intent = "topic"
            else:
                intent = "multi_entity"
            
            is_entity_query = True
            # Use inverted index scan for fast O(1) lookup
            raw_events = scan_by_entities(resolved)

            # --- PERSON-RELEVANCE FILTER ---
            # When query specifies persons BUT NOT dynasties, keep only docs where
            # person appears in doc's persons metadata. Skip when dynasties are present
            # because dynasty matching is more reliable (person may be misresolved).
            # E.g., "nh√† Tr·∫ßn + chi·∫øn c√¥ng" might misresolve "b√† tri·ªáu" as person,
            # but dynasty "tr·∫ßn" correctly finds nh√† Tr·∫ßn docs.
            if has_persons and not has_dynasties and not has_topics and raw_events:
                target_persons = set(p.lower() for p in resolved["persons"])
                target_with_aliases = set(target_persons)
                for alias, canonical in startup.PERSON_ALIASES.items():
                    if canonical in target_persons:
                        target_with_aliases.add(alias)
                person_filtered = [
                    doc for doc in raw_events
                    if set(p.lower() for p in doc.get("persons", [])) & target_with_aliases
                ]
                # If person filter removed everything, the entity resolution may be wrong
                # ‚Üí clear results so no_data=true and UI auto-response kicks in
                raw_events = person_filtered

            # --- DYNASTY-AWARE FILTERING ---
            # When query specifies a dynasty, filter out docs from unrelated dynasties
            # Prevents "nh√† Nguy·ªÖn" docs from leaking into "nh√† Tr·∫ßn" queries
            # EXCEPTION: Skip when query contains qu·ªëc hi·ªáu (country names like
            # "ƒê·∫°i Vi·ªát", "ƒê·∫°i C·ªì Vi·ªát", "ƒê·∫°i Nam") because these span multiple
            # dynasties and shouldn't be filtered to just one
            QUOC_HIEU = {"ƒë·∫°i vi·ªát", "ƒë·∫°i c·ªì vi·ªát", "ƒë·∫°i nam", "vi·ªát nam"}
            has_quoc_hieu = bool(
                set(p.lower() for p in resolved.get("places", [])) & QUOC_HIEU
            )
            if has_dynasties and raw_events and not has_quoc_hieu:
                target_dynasties = set(d.lower() for d in resolved["dynasties"])
                filtered = []
                for doc in raw_events:
                    doc_dynasty = doc.get("dynasty", "").strip().lower()
                    # Keep if: no dynasty metadata, OR dynasty matches target
                    if not doc_dynasty or any(td in doc_dynasty or doc_dynasty in td for td in target_dynasties):
                        filtered.append(doc)
                # Only apply filter if it doesn't remove everything
                if filtered:
                    raw_events = filtered

            # --- DYNAMIC KEYWORD RELEVANCE FILTER ---
            # When query has specific action/context keywords, filter events to match
            # E.g.: "chi·∫øn c√¥ng ch·ªëng Nguy√™n M√¥ng" ‚Üí keep only combat-related events
            if raw_events:
                raw_events = _filter_by_query_keywords(rewritten, raw_events)

            # Only supplement with semantic search when entity scan found SOME results
            # but fewer than 3. When entity scan found ZERO results for specific
            # person/entity queries, DON'T fallback ‚Äî this is a DATA GAP and semantic
            # search will only return noise. Let no_data=true so the UI can respond.
            entity_scan_count = len(raw_events)
            if 0 < entity_scan_count < 3:
                raw_events.extend(semantic_search(rewritten))
        elif is_definition:
            intent = "definition"
            raw_events = semantic_search(rewritten)
        else:
            year = extract_single_year(rewritten)
            if year:
                intent = "year"
                raw_events = scan_by_year(year)
            else:
                intent = "semantic"
                raw_events = semantic_search(rewritten)

    # --- IMPLICIT CONTEXT EXPANSION (Semantic-Intent-Aware) ---
    # Only triggers when semantic intent didn't already resolve the query.
    # If semantic_intent was high-confidence, we already have structured results.
    if semantic_intent.confidence < 0.8:
        implicit_ctx = expand_query_with_implicit_context(rewritten, resolved)
        if len(raw_events) < 3 and (implicit_ctx["is_vietnam_scope"] or implicit_ctx["has_resistance"]):
            if implicit_ctx["is_broad"] or implicit_ctx["has_resistance"]:
                if not raw_events:
                    intent = "implicit_context"

                # Strategy 1: Search using expanded resistance/event terms
                for extra_query in implicit_ctx["extra_search_queries"]:
                    extra_results = semantic_search(extra_query)
                    raw_events.extend(extra_results)

                # Strategy 2: For very broad queries, scan all documents by dynasty
                if implicit_ctx["is_broad"] and len(raw_events) < 5:
                    for dynasty_key in list(startup.DYNASTY_INDEX.keys()):
                        for idx in startup.DYNASTY_INDEX[dynasty_key][:3]:
                            if idx < len(startup.DOCUMENTS):
                                doc = startup.DOCUMENTS[idx]
                                if doc not in raw_events:
                                    raw_events.append(doc)

                # Strategy 3: Scan by expanded terms in inverted keyword index
                for term in implicit_ctx["expanded_terms"]:
                    term_normalized = term.replace(" ", "_")
                    for idx in startup.KEYWORD_INDEX.get(term, []) + startup.KEYWORD_INDEX.get(term_normalized, []):
                        if idx < len(startup.DOCUMENTS):
                            doc = startup.DOCUMENTS[idx]
                            if doc not in raw_events:
                                raw_events.append(doc)

    # --- FALLBACK CHAIN ---
    # When primary search finds nothing, try harder
    # BUT: if entities were resolved and entity scan found nothing, it's a DATA GAP
    # ‚Üí don't waste time on semantic search which will return irrelevant results
    if not raw_events and not (is_entity_query and has_entities):
        # Fallback 1: Semantic search with rewritten query
        # (may help if rewrite changed the query significantly)
        if rewritten.lower() != query.lower():
            raw_events = semantic_search(rewritten)
        
        # Fallback 2: Try search variations (entity-focused queries)
        if not raw_events and has_entities:
            variations = generate_search_variations(rewritten, resolved)
            for var_query in variations:
                var_results = semantic_search(var_query)
                if var_results:
                    raw_events.extend(var_results)
                    break  # Use first successful variation
        
        # Fallback 3: Pure semantic search with original query
        if not raw_events and query.lower() != rewritten.lower():
            raw_events = semantic_search(query)

    # --- CONTEXT7 FILTERING & RANKING ---
    # Apply Context7 to filter and rank events based on query relevance
    # This ensures the answer stays focused on the question
    if raw_events:
        raw_events = filter_and_rank_events(raw_events, query, max_results=50)

    # --- NLI ANSWER VALIDATION ---
    # Use NLI model to verify events actually address the question
    # SKIP for entity/dynasty queries ‚Äî events already passed 4 filter layers:
    #   entity-scan ‚Üí dynasty filter ‚Üí keyword filter ‚Üí cross-encoder
    # NLI is too aggressive for broad queries like "k·ªÉ v·ªÅ X" and causes
    # false negatives (e.g., removes "Tr·∫≠n B·∫°ch ƒê·∫±ng 1288" from nh√† Tr·∫ßn query)
    # Only apply NLI for pure semantic searches where there's no structural match
    if raw_events and not (is_entity_query or is_dynasty_query or is_range_query):
        raw_events = validate_events_nli(query, raw_events)

    # --- FINAL RELEVANCE GUARD ---
    # When query mentions specific persons, verify at least one result actually
    # discusses that person. Only check persons that appear in the ORIGINAL query
    # text ‚Äî entity resolution may produce false matches (e.g., "h·ªç" ‚Üí "h·ªì").
    if raw_events and has_persons and resolved.get("persons"):
        query_lower = query.lower()
        # Only validate persons that actually appear in the original query
        query_persons = [p.lower() for p in resolved["persons"] if p.lower() in query_lower]
        
        if query_persons:
            # Check if at least one result mentions the queried person
            has_relevant = False
            for doc in raw_events:
                doc_text = (
                    (doc.get("story", "") or "") + " " +
                    (doc.get("event", "") or "") + " " +
                    (doc.get("title", "") or "") + " " +
                    " ".join(p for p in doc.get("persons", []) or [])
                ).lower()
                for person in query_persons:
                    person_words = person.split()
                    if len(person_words) >= 2 and all(w in doc_text for w in person_words):
                        has_relevant = True
                        break
                    elif len(person_words) == 1 and person in doc_text:
                        has_relevant = True
                        break
                if has_relevant:
                    break
            if not has_relevant:
                raw_events = []  # No doc mentions the queried person ‚Üí noise

    no_data = not raw_events

    # Use higher event limit for range/dynasty/entity queries
    if is_range_query:
        max_events = MAX_TOTAL_EVENTS_RANGE
    elif is_dynasty_query:
        max_events = MAX_TOTAL_EVENTS_DYNASTY
    elif is_entity_query:
        max_events = MAX_TOTAL_EVENTS_ENTITY
    else:
        max_events = MAX_TOTAL_EVENTS

    # Deduplicate and enrich events
    unique_events = deduplicate_and_enrich(raw_events, max_events) if not no_data else []
    
    # Generate complete, comprehensive answer
    answer = format_complete_answer(unique_events, group_by=semantic_group_by)

    # --- SPECIAL & QU·ªêC HI·ªÜU INTRO SENTENCES ---
    # Prepend a poetic, engaging intro based on query keywords or qu·ªëc hi·ªáu
    if answer and not no_data:
        intro_added = False

        # 1) Keyword-based special intros (checked against original query text)
        _KEYWORD_INTROS = {
            "chi·∫øn tranh vi·ªát nam": (
                'B·∫°n ƒëang mu·ªën t√¨m hi·ªÉu v·ªÅ: "Kh√°ng chi·∫øn ch·ªëng gi·∫∑c ngo·∫°i x√¢m'
                " ‚Äì b·∫£n h√πng ca gi·ªØ n∆∞·ªõc vang v·ªçng su·ªët chi·ªÅu d√†i"
                ' l·ªãch s·ª≠ d√¢n t·ªôc Vi·ªát Nam ta."'
            ),
        }
        query_lower = query.lower()
        for keyword, intro in _KEYWORD_INTROS.items():
            if keyword in query_lower:
                answer = intro + "\n\n" + answer
                intro_added = True
                break

        # 2) Qu·ªëc hi·ªáu intros (checked against resolved places)
        if not intro_added:
            _QUOC_HIEU_INTROS = {
                "ƒë·∫°i vi·ªát": (
                    "**ƒê·∫°i Vi·ªát** ‚Äì bi·ªÉu t∆∞·ª£ng c·ªßa √Ω ch√≠ qu·∫≠t c∆∞·ªùng v√† tinh th·∫ßn b·∫•t khu·∫•t"
                    " ‚Äì ƒë√£ ghi v√†o l·ªãch s·ª≠ nh·ªØng chi·∫øn t√≠ch l·∫´y l·ª´ng;"
                    " ch√∫ng ta c√πng nhau xem l·∫°i v√†i n√©t trong b·∫£n h√πng ca r·∫°ng r·ª° ·∫•y nh√© :"
                ),
                "ƒë·∫°i c·ªì vi·ªát": (
                    "**ƒê·∫°i C·ªì Vi·ªát** ‚Äì qu·ªëc hi·ªáu ƒë·∫ßu ti√™n kh·∫≥ng ƒë·ªãnh n·ªÅn ƒë·ªôc l·∫≠p"
                    " ‚Äì ƒë√°nh d·∫•u b∆∞·ªõc ngo·∫∑t vƒ© ƒë·∫°i trong h√†nh tr√¨nh d·ª±ng n∆∞·ªõc;"
                    " h√£y c√πng nh√¨n l·∫°i nh·ªØng d·∫•u m·ªëc quan tr·ªçng ·∫•y :"
                ),
                "ƒë·∫°i nam": (
                    "**ƒê·∫°i Nam** ‚Äì qu·ªëc hi·ªáu th·ªùi Nguy·ªÖn, bi·ªÉu tr∆∞ng cho s·ª± th·ªëng nh·∫•t"
                    " ‚Äì ch·ª©a ƒë·ª±ng bao thƒÉng tr·∫ßm c·ªßa l·ªãch s·ª≠ c·∫≠n ƒë·∫°i;"
                    " h√£y c√πng kh√°m ph√° nh·ªØng s·ª± ki·ªán n·ªïi b·∫≠t :"
                ),
            }
            resolved_places = set(p.lower() for p in resolved.get("places", []))
            for quoc_hieu, intro in _QUOC_HIEU_INTROS.items():
                if quoc_hieu in resolved_places:
                    answer = intro + "\n\n" + answer
                    break

    # Prepend same-entity explanation ONLY when user explicitly asks about relationship
    # "Quang Trung v√† Nguy·ªÖn Hu·ªá l√† g√¨?" ‚Üí show same-entity
    # "K·ªÉ t√™n chi·∫øn c√¥ng ch·ªëng qu√¢n Nguy√™n M√¥ng" ‚Üí DON'T show same-entity
    if same_person_info and (is_relationship or is_definition) and answer:
        same_entity_response = _generate_same_entity_response(same_person_info)
        answer = same_entity_response + "\n\n" + answer
    elif same_person_info and (is_relationship or is_definition) and not answer:
        answer = _generate_same_entity_response(same_person_info)

    # Smart no_data response ‚Äî suggest alternative phrasing
    if no_data:
        answer = _generate_no_data_suggestion(q_display, rewritten, resolved, question_intent)
    
    # --- LEGACY VALIDATION (Cross-Encoder based) ---
    # Kept as secondary check; NLI validation above is the primary filter
    if answer and not no_data:
        validation = validate_answer_relevance(answer, query)
        if not validation["is_relevant"]:
            pass  # Logged but not acted upon (NLI handles filtering)

    return {
        "query": q_display,
        "intent": intent,
        "answer": answer,
        "events": unique_events,  # Return deduplicated, enriched events
        "no_data": no_data
    }


def _generate_no_data_suggestion(original_query: str, rewritten: str, resolved: dict, question_intent: str | None) -> str:
    """
    Generate a helpful suggestion when no data is found.
    Instead of just saying "kh√¥ng t√¨m th·∫•y", guide the user to rephrase.
    """
    suggestions = []
    
    # Check if query was rewritten (means user may have typos)
    if rewritten.lower() != original_query.lower():
        suggestions.append(f"T√¥i ƒë√£ hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n l√†: *\"{rewritten}\"*")
    
    suggestions.append("T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. B·∫°n c√≥ th·ªÉ th·ª≠:")
    suggestions.append("")
    suggestions.append("- **H·ªèi c·ª• th·ªÉ h∆°n** ‚Äî v√≠ d·ª•: *\"Tr·∫≠n B·∫°ch ƒê·∫±ng nƒÉm 1288\"*")
    suggestions.append("- **D√πng t√™n nh√¢n v·∫≠t** ‚Äî v√≠ d·ª•: *\"Tr·∫ßn H∆∞ng ƒê·∫°o ƒë√°nh qu√¢n Nguy√™n\"*")
    suggestions.append("- **N√™u tri·ªÅu ƒë·∫°i** ‚Äî v√≠ d·ª•: *\"Nh√† Tr·∫ßn c√≥ s·ª± ki·ªán g√¨ n·ªïi b·∫≠t?\"*")
    suggestions.append("- **Tra theo nƒÉm** ‚Äî v√≠ d·ª•: *\"NƒÉm 1945 c√≥ s·ª± ki·ªán g√¨?\"*")
    
    return "\n".join(suggestions)
