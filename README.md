# Vietnam History AI â€” Há»‡ thá»‘ng Chatbot Lá»‹ch sá»­ Viá»‡t Nam

Dá»± Ã¡n nÃ y lÃ  há»‡ thá»‘ng Chatbot thÃ´ng minh há»— trá»£ tra cá»©u vÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i vá» lá»‹ch sá»­ Viá»‡t Nam, sá»­ dá»¥ng ká»¹ thuáº­t **RAG (Retrieval-Augmented Generation)** káº¿t há»£p **NLU (Natural Language Understanding)**.

## ğŸ¯ Status

```
âœ… Version: 3.0.0
âœ… Tests: 78 engine tests passing (100%)
âœ… AI Models: 3 ONNX models (Embedding + Cross-Encoder + NLI)
âœ… Status: PRODUCTION READY
```

---

## ğŸš€ Quick Start

### 1. CÃ i Ä‘áº·t

```bash
cd ai-service
pip install -r requirements.txt
```

### 2. Build FAISS Index (tá»« HuggingFace)

```bash
python scripts/build_from_huggingface.py
# TÃ¹y chá»‰nh: MAX_SAMPLES=100000 python scripts/build_from_huggingface.py
```

### 3. Cháº¡y API

```bash
uvicorn app.main:app --reload
# â†’ http://localhost:8000
```

### 4. Deploy

#### Docker (Khuyáº¿n nghá»‹)
```bash
docker build -t historymindai:latest ./ai-service
docker run -d -p 8000:8000 historymindai:latest

# Hoáº·c dÃ¹ng Docker Compose
docker-compose up -d
```

#### Deploy tá»± Ä‘á»™ng lÃªn Railway
```bash
# Windows
.\deploy.ps1

# Linux/Mac
chmod +x deploy.sh && ./deploy.sh
```

#### Push lÃªn GitHub
```bash
# Windows
.\push-to-github.ps1

# Linux/Mac
chmod +x push-to-github.sh && ./push-to-github.sh
```

**ğŸ“– Lá»™ trÃ¬nh phÃ¡t triá»ƒn AI**: [AI_DEVELOPMENT_ROADMAP.md](AI_DEVELOPMENT_ROADMAP.md)

---

## ğŸ— Kiáº¿n trÃºc há»‡ thá»‘ng

```mermaid
graph TD
    subgraph "ğŸ–¥ Frontend â€” React"
        A["Giao diá»‡n Chat"]
    end

    subgraph "âš™ï¸ Backend â€” Spring Boot"
        B["API Gateway / Orchestrator"]
    end

    subgraph "ğŸ¤– AI Service â€” FastAPI"
        NLU["NLU Layer<br/>Sá»­a lá»—i, phá»¥c há»“i dáº¥u, entity detection"]
        ENGINE["Query Engine"]
        CE["Cross-Encoder Rerank<br/>mmarco multilingual ONNX"]
        NLI["NLI Validator<br/>Entailment checking"]
    end

    subgraph "ğŸ’¾ Data Layer"
        D1["FAISS Index â€” Semantic vectors"]
        D2["meta.json â€” Metadata"]
        D3["knowledge_base.json â€” Aliases, Synonyms"]
    end

    A -- "HTTP" --> B
    B -- "REST" --> NLU
    NLU --> ENGINE
    ENGINE --> D1 & D2 & D3
    ENGINE --> CE
    CE --> NLI
    NLI --> FORMAT["ğŸ“¤ Response"]
```

---

## ğŸ§  AI Pipeline

```mermaid
flowchart LR
    Q["ğŸ“ CÃ¢u há»i"] --> NLU["ğŸ”¤ NLU<br/>Rewrite + Fix"]
    NLU --> Search["ğŸ” Semantic Search<br/>vietnamese-sbert<br/>130 MB ONNX"]
    Search -->|"Top-50"| Rerank["ğŸ“Š Cross-Encoder<br/>mmarco multilingual<br/>113 MB ONNX"]
    Rerank -->|"Top-10"| NLI["âœ… NLI Validator<br/>MiniLMv2 multilingual<br/>102 MB ONNX"]
    NLI --> Answer["ğŸ’¬ CÃ¢u tráº£ lá»i"]

    style Search fill:#FF9800,color:#fff
    style Rerank fill:#3F51B5,color:#fff
    style NLI fill:#7B1FA2,color:#fff
```

### 3 AI Models (táº¥t cáº£ cháº¡y local, ONNX, miá»…n phÃ­)

| Model | Chá»©c nÄƒng | KÃ­ch thÆ°á»›c |
|---|---|---|
| `keepitreal/vietnamese-sbert` | Encode cÃ¢u há»i â†’ vector | 130 MB |
| `mmarco-mMiniLMv2-L12-H384-v1` | Re-rank káº¿t quáº£ (14 ngÃ´n ngá»¯) | 113 MB |
| `multilingual-MiniLMv2-L6-mnli-xnli` | Kiá»ƒm tra entailment | 102 MB |
| **Tá»•ng** | | **~345 MB** |

---

## ğŸ”¤ NLU â€” Hiá»ƒu NgÃ´n Ngá»¯ Tá»± NhiÃªn

| TÃ­nh nÄƒng | VÃ­ dá»¥ | Káº¿t quáº£ |
|-----------|-------|---------|
| **Sá»­a lá»—i chÃ­nh táº£** | `nguyen huye` | â†’ `nguyá»…n huá»‡` |
| **Má»Ÿ rá»™ng viáº¿t táº¯t** | `VN Ä‘á»™c láº­p` | â†’ `Viá»‡t Nam Ä‘á»™c láº­p` |
| **Phá»¥c há»“i dáº¥u** | `tran hung dao` | â†’ `tráº§n hÆ°ng Ä‘áº¡o` |
| **Fuzzy Matching** | `tráº§n hÆ°ng Ä‘ao` | â†’ `tráº§n hÆ°ng Ä‘áº¡o` |
| **Phonetic Normalization** | `cháº§n hÆ°ng Ä‘áº¡o` | â†’ `tráº§n hÆ°ng Ä‘áº¡o` |
| **Synonym Expansion** | `quÃ¢n mÃ´ng cá»•` | â†’ `nguyÃªn mÃ´ng` |
| **Fallback Chain** | KhÃ´ng tÃ¬m Ä‘Æ°á»£c â†’ thá»­ 3 cÃ¡ch | â†’ gá»£i Ã½ alternatives |

---

## ğŸ¤– Query Engine â€” Luá»“ng xá»­ lÃ½

```mermaid
flowchart TD
    INPUT["ğŸ“ CÃ¢u há»i"] --> NLU["ğŸ§  NLU: Rewrite + Fix"]
    NLU --> CREATOR{"TÃ¡c giáº£?"}
    CREATOR -- CÃ³ --> CR["ğŸ¤– Creator Response"]
    CREATOR -- KhÃ´ng --> IDENTITY{"Báº¡n lÃ  ai?"}
    IDENTITY -- CÃ³ --> ID["ğŸ¤– Identity"]
    IDENTITY -- KhÃ´ng --> YEAR_RANGE{"Khoáº£ng nÄƒm?"}
    YEAR_RANGE -- CÃ³ --> YR["ğŸ“… scan_by_year_range"]
    YEAR_RANGE -- KhÃ´ng --> MULTI_YEAR{"Nhiá»u nÄƒm?"}
    MULTI_YEAR -- CÃ³ --> MY["ğŸ“… scan_by_year Ã— N"]
    MULTI_YEAR -- KhÃ´ng --> ENTITY{"CÃ³ entity?"}
    ENTITY -- CÃ³ --> SAME_CHECK{"Relationship?"}
    SAME_CHECK -- CÃ³ --> SAME["ğŸ”— Same-Entity Detection"]
    SAME_CHECK -- KhÃ´ng --> ME["ğŸ” scan_by_entities"]
    ENTITY -- KhÃ´ng --> DEF{"'lÃ  gÃ¬/lÃ  ai'?"}
    DEF -- CÃ³ --> SEM1["ğŸ§  semantic_search"]
    DEF -- KhÃ´ng --> SINGLE{"NÄƒm Ä‘Æ¡n?"}
    SINGLE -- CÃ³ --> SY["ğŸ“… scan_by_year"]
    SINGLE -- KhÃ´ng --> SEM2["ğŸ§  semantic_search"]

    SAME & ME & YR & MY & SEM1 & SY & SEM2 --> RERANK["ğŸ“Š Cross-Encoder Rerank"]
    RERANK --> NLICHECK["âœ… NLI Validation"]
    NLICHECK --> RESULT{"Káº¿t quáº£?"}
    RESULT -- CÃ³ --> FORMAT["Format Answer"]
    RESULT -- KhÃ´ng --> FALLBACK["ğŸ”„ Fallback Chain"]
    FORMAT --> OUTPUT["ğŸ“¤ JSON Response"]
    FALLBACK --> OUTPUT

    style NLU fill:#1b4332,color:#fff
    style RERANK fill:#3F51B5,color:#fff
    style NLICHECK fill:#7B1FA2,color:#fff
```

---

## ğŸ”§ Data-Driven Architecture

> **Muá»‘n thÃªm alias/synonym?** Sá»­a `knowledge_base.json` â€” KHÃ”NG cáº§n sá»­a code.
> **ThÃªm documents?** Rebuild FAISS index â€” inverted indexes tá»± build táº¡i startup.

| Thao tÃ¡c | File cáº§n sá»­a | Code cáº§n sá»­a |
|----------|-------------|-------------|
| ThÃªm alias nhÃ¢n váº­t | `knowledge_base.json` | âŒ KhÃ´ng |
| ThÃªm synonym chá»§ Ä‘á» | `knowledge_base.json` | âŒ KhÃ´ng |
| ThÃªm alias triá»u Ä‘áº¡i | `knowledge_base.json` | âŒ KhÃ´ng |
| ThÃªm viáº¿t táº¯t | `knowledge_base.json` | âŒ KhÃ´ng |
| ThÃªm sá»­a lá»—i chÃ­nh táº£ | `knowledge_base.json` | âŒ KhÃ´ng |
| ThÃªm documents má»›i | Rebuild FAISS | âŒ KhÃ´ng |

---

## ğŸ§ª Testing

```bash
python -m pytest tests/test_engine.py -v     # 78 tests
python -m pytest tests/ -v                   # Full suite
```

| File | Tests | Ná»™i dung |
|------|-------|---------|
| `test_engine.py` | 78 | Engine: intent, entity, year, multi-entity |
| `test_nlu.py` | 55 | NLU: rewriting, fuzzy, accents, phonetic |
| `test_comprehensive.py` | 74 | Integration tests |
| `test_search_utils.py` | 53 | Search, indexing, relevance |
| `test_pipeline.py` | 30 | Data pipeline |
| `test_year_extraction.py` | 30 | Year extraction |
| `test_text_cleaning.py` | 20 | Text normalization |
| *+ 8 more files* | 68 | API, schema, performance, dedup |

---

## ğŸ“‚ Cáº¥u trÃºc

```
vietnam_history_dataset/
â”œâ”€â”€ ai-service/                            # ğŸ¤– FastAPI AI Service
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py                  # Config paths & constants
â”‚   â”‚   â”‚   â””â”€â”€ startup.py                 # Load models + build indexes
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.py                  # Query Engine chÃ­nh
â”‚   â”‚   â”‚   â”œâ”€â”€ query_understanding.py     # ğŸ§  NLU Layer
â”‚   â”‚   â”‚   â”œâ”€â”€ search_service.py          # Entity resolution + FAISS
â”‚   â”‚   â”‚   â”œâ”€â”€ cross_encoder_service.py   # ğŸ“Š Cross-Encoder Re-ranking
â”‚   â”‚   â”‚   â””â”€â”€ nli_validator_service.py   # âœ… NLI Answer Validation
â”‚   â”‚   â””â”€â”€ main.py                        # FastAPI entry point
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ build_from_huggingface.py      # ğŸš€ Pipeline: HuggingFace â†’ FAISS
â”‚   â”œâ”€â”€ onnx_model/                        # Embedding model (130 MB)
â”‚   â”œâ”€â”€ onnx_cross_encoder/                # Cross-Encoder model (113 MB)
â”‚   â”œâ”€â”€ onnx_nli/                          # NLI model (102 MB)
â”‚   â”œâ”€â”€ faiss_index/                       # FAISS index + metadata
â”‚   â””â”€â”€ knowledge_base.json                # ğŸ”‘ Aliases, Synonyms, Typos
â”œâ”€â”€ scripts/                               # Export scripts (ONNX models)
â”œâ”€â”€ tests/                                 # Unit tests (20 files)
â”œâ”€â”€ pipeline/                              # Data processing pipeline
â”œâ”€â”€ AI_DEVELOPMENT_ROADMAP.md              # ğŸ“– Lá»™ trÃ¬nh phÃ¡t triá»ƒn AI
â”œâ”€â”€ deploy.ps1 / deploy.sh                 # ğŸš€ Auto deploy scripts
â””â”€â”€ push-to-github.ps1 / push-to-github.sh # ğŸ“¤ Auto push scripts
```

## ğŸ“š Tech Stack

| ThÃ nh pháº§n | CÃ´ng nghá»‡ |
|-----------|-----------|
| Framework | FastAPI + Uvicorn |
| Vector DB | FAISS (Facebook AI) |
| Embedding | `keepitreal/vietnamese-sbert` (ONNX) |
| Reranker | `mmarco-mMiniLMv2-L12-H384-v1` (ONNX) |
| NLI | `multilingual-MiniLMv2-L6-mnli-xnli` (ONNX) |
| NLU | Fuzzy matching, accent restoration, phonetic normalization |
| Data | HuggingFace Datasets, Dynamic Entity Registry |

---

_Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn nháº±m gÃ¬n giá»¯ vÃ  truyá»n bÃ¡ kiáº¿n thá»©c lá»‹ch sá»­ Viá»‡t Nam thÃ´ng qua cÃ´ng nghá»‡ AI hiá»‡n Ä‘áº¡i._
