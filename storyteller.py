import sys
sys.stdout.reconfigure(encoding="utf-8")

import json
import re
from pathlib import Path
from datasets import load_dataset
import random
from functools import lru_cache
from collections import defaultdict

DATASET_DIR = Path(
    "vietnam_history_dataset/default/0.0.0/3fdbbebc92e755190b4eaeb1522d97a753f0f18a"
)

ARROW_FILES = [
    str(DATASET_DIR / "vietnam-history-1_m-vi-train-00000-of-00002.arrow"),
    str(DATASET_DIR / "vietnam-history-1_m-vi-train-00001-of-00002.arrow"),
]

UNKNOWN_ENTITIES = set()

OUT_PATH = "data/history_timeline.json"

YEAR_ANY = re.compile(
    r"(?:^|\D)(?:nƒÉm|NƒÉm)\s*"
    r"([1-9][0-9]{2,3})"
    r"(?![0-9])",
    re.UNICODE
)

DATE_WITH_YEAR = re.compile(
    r"\b([0-3]?\d/[01]?\d/([1-9][0-9]{2,3}))\b"
)

YEAR_INLINE = re.compile(
    r"(?:ƒë·∫ßu|gi·ªØa|cu·ªëi|m√πa\s+\w+)?\s*nƒÉm\s+([1-9][0-9]{2,3})",
    re.I
)

PERSON_PATTERN = re.compile(
    r"\b(?:Vua\s+)?"
    r"("
        # Nguy·ªÖn Hu·ªá, Tr·∫ßn H∆∞ng ƒê·∫°o...
        r"(?:Nguy·ªÖn|L√™|L√Ω|Tr·∫ßn|ƒêinh|H·ªì|Ng√¥|Ph·∫°m|Phan|B√πi|ƒê·ªó|V≈©|V√µ|Ho√†ng|Hu·ª≥nh|ƒê·∫∑ng|D∆∞∆°ng|Kh√∫c|M·∫°c)"
        r"(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+){1,3}"
    r"|"
        # L√Ω Th√°i T·ªï, Tr·∫ßn Th√°nh T√¥ng
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+"
        r"\s+(?:Th√°i|Th√°nh|Nh√¢n)\s+(?:T·ªï|T√¥ng)"
    r"|"
        # Mi·∫øu hi·ªáu ƒë∆°n
        r"(?:Quang\s+Trung|Gia\s+Long|Minh\s+M·∫°ng|T·ª±\s+ƒê·ª©c|H√†m\s+Nghi)"
    r")\b"
)

ROYAL_TITLES = {
    "quang trung",
    "b·∫Øc b√¨nh v∆∞∆°ng",
    "gia long",
    "minh m·∫°ng",
    "t·ª± ƒë·ª©c",
    "thi·ªáu tr·ªã",
    "h√†m nghi",
}

CANONICAL_PERSON = {
    "quang trung": "Nguy·ªÖn Hu·ªá",
    "b·∫Øc b√¨nh v∆∞∆°ng": "Nguy·ªÖn Hu·ªá",
    "gia long": "Nguy·ªÖn √Ånh",
    "minh m·∫°ng": "Minh M·∫°ng",
    "t·ª± ƒë·ª©c": "T·ª± ƒê·ª©c",
    "thi·ªáu tr·ªã": "Thi·ªáu Tr·ªã",
    "h√†m nghi": "H√†m Nghi",
    "l√Ω c√¥ng u·∫©n": "L√Ω Th√°i T·ªï",
    "vua l√Ω th√°i t·ªï": "L√Ω Th√°i T·ªï",
    "nguy·ªÖn t·∫•t th√†nh": "Nguy·ªÖn T·∫•t Th√†nh",
    "h·ªì ch√≠ minh": "H·ªì Ch√≠ Minh",
    # Th√™m c√°c t√™n chu·∫©n ƒë·ªÉ is_valid_person nh·∫≠n di·ªán
    "nguy·ªÖn hu·ªá": "Nguy·ªÖn Hu·ªá",
    "nguy·ªÖn √°nh": "Nguy·ªÖn √Ånh",
    "l√Ω th√°i t·ªï": "L√Ω Th√°i T·ªï",
}

def normalize_person(name: str) -> str:
    if not name:
        return name
    key = name.strip().lower()
    return CANONICAL_PERSON.get(key, name.strip())

JUNK_PATTERNS = [
    r"ƒê·ªÉ tr·∫£ l·ªùi\.?",
    r"N·ªôi dung\.?",
    r"√ù nghƒ©a l·ªãch s·ª≠\.?",
    r"√ù nghƒ©a\.?",
    r"V·ªÅ l√¢u d√†i,?",
    r"ƒê√¢y l√† c·ªôt m·ªëc quan tr·ªçng v√¨",
    r"Tr√¨nh b√†y ng·∫Øn g·ªçn, m·∫°ch l·∫°c\.?",
    r"V√†o\s*,?",
    r"\*\*",
]

BAD_PERSON_KEYWORDS = {
    "Vi·ªát Nam", "ƒê·∫°i Vi·ªát", "ƒê·∫°i La", "ThƒÉng Long", "Hoa L∆∞",
    "ƒêi·ªán Bi√™n Ph·ªß", "Th√°ng T√°m", "B√¨nh Ng√¥", "Chi·∫øu", "Hi·ªáp"
}

def is_real_person(name: str) -> bool:
    if len(name.split()) < 2:
        return False
    for bad in BAD_PERSON_KEYWORDS:
        if bad in name:
            return False
    return True


ACTION_GROUPS = [
    ("ƒë√°nh tan", "ƒë·∫©y lui", "ƒë√°nh b·∫°i", "ti√™u di·ªát"),
    ("d·ª±ng ch√≠nh quy·ªÅn", "kh√¥i ph·ª•c quy·ªÅn t·ª± ch·ªß", "gi√†nh quy·ªÅn t·ª± ch·ªß"),
    ("l√™n ng√¥i", "x∆∞ng v∆∞∆°ng"),
    ("th·ªëng nh·∫•t", "d·∫πp lo·∫°n"),
]


INFORMATIVE_VERBS = [
    "ƒë√°nh", "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ti√™u di·ªát",
    "d·ª±ng", "l·∫≠p", "x∆∞ng", "l√™n ng√¥i", "ƒë·ªïi",
    "k√Ω", "ban", "c√¥ng b·ªë",
    "t·∫•n c√¥ng", "ph√≤ng th·ªß", "ch·∫∑n",
    "nh∆∞·ª£ng", "m·∫•t", "r∆°i v√†o",
    "th·ªëng nh·∫•t", "chia c·∫Øt",
    "kh√¥i ph·ª•c", "so√°n ng√¥i", "l·∫≠t ƒë·ªï", "th√†nh l·∫≠p",
    "ph√°t ƒë·ªông", "h·∫°", "chi·∫øm", "gi·ªØ", "gi·∫£i ph√≥ng",
    "ban b·ªë", "h·∫° chi·∫øu", "k√Ω k·∫øt"
]

STATE_VERBS = [
    "suy y·∫øu", "tan r√£", "th·∫•t b·∫°i",
    "kh·ªßng ho·∫£ng", "·ªïn ƒë·ªãnh",
    "ph√°t tri·ªÉn", "r∆°i v√†o", "b∆∞·ªõc v√†o",
    "m·ªü ra", "ch·∫•m d·ª©t", "kh·∫≥ng ƒë·ªãnh"
]

STOPWORDS = {
    "di·ªÖn", "ra", "x·∫£y", "x·∫£y ra", "nƒÉm",
    "ƒë∆∞·ª£c", "b·ªã", "l√†", "v√†", "·ªü",
    "sau", "khi", "tr∆∞·ªõc", "trong",
    "t·∫°i", "v·ªõi", "do", "ƒë·ªÉ"
}

STOPWORDS |= {
    "ƒë√≥", "n√†y", "kia",
    "ƒë√£", "s·∫Ω", "c≈©ng",
    "nh·ªØng", "c√°c"
}

VIETNAMESE_SURNAMES = {
    "nguy·ªÖn", "l√™", "l√Ω", "tr·∫ßn", "ƒëinh", "ph·∫°m",
    "v≈©", "v√µ", "ho√†ng", "hu·ª≥nh", "ƒë·∫∑ng",
    "b√πi", "ƒë·ªó", "h·ªì", "ng√¥", "d∆∞∆°ng", "phan"
}

VIETNAMESE_SURNAMES |= {
    "kh√∫c",
    "m·∫°c",
    "ƒëinh",
    "h·ªì",
}


NON_PERSON_PHRASES = {
    "lam s∆°n", "t√¢y s∆°n",
    "ƒë·∫°i vi·ªát", "ƒë·∫°i nam", "ƒë·∫°i c·ªì vi·ªát",
    "nh√† l√Ω", "nh√† tr·∫ßn", "nh√† l√™", "nh√† nguy·ªÖn",
    "qu√¢n thanh", "qu√¢n minh",
    "b·∫°ch ƒë·∫±ng", "ng·ªçc h·ªìi", "ƒë·ªëng ƒëa",
    "kh·ªüi nghƒ©a", "kh√°ng chi·∫øn", "chi·∫øn d·ªãch",
}

NON_PERSON_TOKENS = {
    "s∆°n", "giang", "ƒë√¥", "th√†nh",
    "kh√°ng", "tr·∫≠n", "qu√¢n",
    "tri·ªÅu", "n∆∞·ªõc",
    "t·ªëng", "minh", "thanh",
    "m√¥ng", "c·ªï"
}

ACTION_HINTS = {
    "ƒë√°nh", "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ch·ªâ huy",
    "l√£nh ƒë·∫°o", "ti·∫øn c√¥ng", "ph·∫£n c√¥ng",
    "d·ª±ng", "x∆∞ng", "l√™n ng√¥i",
    "ban", "k√Ω", "t√¥n", "ch·ªß ƒë·ªông"
}

ACTION_HINTS |= {
    "nh∆∞·ªùng ng√¥i",
    "so·∫°n",
    "vi·∫øt",
    "ban",
    "d·ª±ng",
    "l·∫≠p",
    "ch·ªß tr√¨",
    "kh·ªüi x∆∞·ªõng",
    "l√£nh ƒë·∫°o",
    "ra ƒëi",
}

HEROIC_YEARS = {
    938, 981, 1009, 1010,
    1075, 1077,
    1258, 1285, 1288,
    1418, 1427,
    1471,
    1789,
    1930, 1945,
    1960, 1968, 1972, 1975
}

TRAGIC_YEARS = {1858, 1884, 1955}


HEROIC_YEARS |= {1954}

ENTITY_REGISTRY = {
    "person": set([
        "Nguy·ªÖn T·∫•t Th√†nh",
        "H·ªì Ch√≠ Minh",
        "H·ªì Qu√Ω Ly",
        "Nguy·ªÖn Hu·ªá",
        "Quang Trung",
        "Gia Long",
        "Nguy·ªÖn √Ånh",
        "L√™ L·ª£i",
        "Tr·∫ßn H∆∞ng ƒê·∫°o",
        "Ng√¥ Quy·ªÅn",
    ]),
    "place": set([
        "ThƒÉng Long",
        "B·∫°ch ƒê·∫±ng",
        "Ng·ªçc H·ªìi",
        "ƒê·ªëng ƒêa",
        "ƒêi·ªán Bi√™n Ph·ªß",
        "Lam S∆°n",
        # ...
    ]),
    "other": set([
        "ƒê·∫°i Vi·ªát",
        "Nh√† Tr·∫ßn",
        "Qu√¢n Thanh",
        "Kh·ªüi nghƒ©a Lam S∆°n",
        "T√¢y S∆°n",
        "ƒê·∫°i Ngu"
    ])
}

ENTITY_LOOKUP: dict[str, str] = {}

for kind, names in ENTITY_REGISTRY.items():
    for n in names:
        ENTITY_LOOKUP[n] = kind


def classify_entity(name: str) -> str | None:
    return ENTITY_LOOKUP.get(name)



INVALID_PERSON_HINTS = {
    "qu√¢n", "nh√† n∆∞·ªõc", "m·∫∑t tr·∫≠n", "ƒë·∫£ng",
    "chi·∫øn d·ªãch", "t·∫øt", "c√°ch m·∫°ng",
    "hi·ªáp ƒë·ªãnh", "qu·ªëc hi·ªáu",
    "thƒÉng long", "ƒë√† n·∫µng", "s√†i g√≤n",
    "ƒë·∫°i vi·ªát", "ƒë·∫°i nam", "ƒë·∫°i c·ªì vi·ªát"
}

INVALID_PERSON_PREFIX = {
    "th·ªùi", "tri·ªÅu", "nh√†", "th·ªùi k·ª≥", "th·ªùi k√¨"
}


def is_valid_person(name: str) -> bool:
    if not name:
        return False

    name = name.strip()
    name_l = name.lower()
    parts = name_l.split()

    # üëë mi·∫øu hi·ªáu ‚Üí lu√¥n h·ª£p l·ªá
    if re.search(r"(th√°i\s+(t·ªï|t√¥ng)|th√°nh\s+t√¥ng|nh√¢n\s+t√¥ng)$", name_l):
        return True

    # alias vua ho·∫∑c mi·∫øu hi·ªáu chu·∫©n
    if name_l in CANONICAL_PERSON or name_l in ROYAL_TITLES:
        return True

    # t·ªëi thi·ªÉu 2 token
    if len(parts) < 2:
        return False

    # ‚ùå prefix kh√¥ng ph·∫£i ng∆∞·ªùi
    if parts[0] in INVALID_PERSON_PREFIX:
        return False

    # ‚ùå phrase phi nh√¢n
    if name_l in NON_PERSON_PHRASES:
        return False

    # ‚ùå ch·ª©a token phi nh√¢n
    for p in parts:
        if (
            p in NON_PERSON_TOKENS
            or p in INVALID_PERSON_HINTS
        ):
            return False

    # ‚ùå kh√¥ng c√≥ h·ªç Vi·ªát ‚Üí lo·∫°i
    if parts[0] not in VIETNAMESE_SURNAMES:
        return False

    return True

def normalize_persons(persons: list[str]) -> list[str]:
    result = set()

    for p in persons:
        p2 = canonical_person(p)
        if not is_valid_person(p2):
            continue

        kind = classify_entity(p2)
        if kind and kind != "person":
            continue

        result.add(p2)

    return sorted(result)


def strip_evaluation(text: str) -> str:
    return re.sub(
        r",?\s*(m·ªü ra|kh·∫≥ng ƒë·ªãnh|ƒë√°nh d·∫•u|th·ªÉ hi·ªán)[^,.]*",
        "",
        text,
        flags=re.I
    ).strip(" ,.")

def canonical_person(name: str) -> str:
    if not name:
        return name

    key = name.strip().lower()
    return CANONICAL_PERSON.get(key, name.strip())

def extract_parenthetical_persons(text: str):
    persons = []

    def repl(m):
        content = m.group(1).strip()
        if is_valid_person(content):
            persons.append(canonical_person(content))
            return content  # ‚¨ÖÔ∏è GI·ªÆ L·∫†I
        return ""

    clean_text = re.sub(r"\(([^()]{2,50})\)", repl, text)
    return clean_text.strip(), persons

def is_person_actor(text: str, person: str) -> bool:
    """
    PERSON l√† actor n·∫øu:
    - PERSON ƒë·ª©ng g·∫ßn ƒë·ªông t·ª´ h√†nh ƒë·ªông (tr∆∞·ªõc ho·∫∑c sau)
    - ho·∫∑c PERSON l√† alias c·ªßa nh√¢n v·∫≠t th·ª±c hi·ªán h√†nh ƒë·ªông
    """
    t = text.lower()
    p = person.lower()

    # t·∫≠p alias: Quang Trung ‚Üî Nguy·ªÖn Hu·ªá
    aliases = {p}
    for k, v in CANONICAL_PERSON.items():
        if v.lower() == p.lower():
            aliases.add(k)

    ACTIONS = [
        "ƒë√°nh", "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ti·∫øn c√¥ng",
        "ch·ªß ƒë·ªông", "d√πng", "nh·ª≠",
        "l√™n ng√¥i", "x∆∞ng v∆∞∆°ng",
        "d·ª±ng", "l·∫≠p", "ban",
        "so·∫°n", "vi·∫øt",
        "ra ƒëi", "kh·ªüi x∆∞·ªõng",
        "l√£nh ƒë·∫°o", "ch·ªâ huy",
        "so√°n ng√¥i", "l·∫≠t ƒë·ªï", "th√†nh l·∫≠p",
        "ph√°t ƒë·ªông", "h·∫°", "chi·∫øm", "gi·ªØ"
    ]

    for name in aliases:
        for act in ACTIONS:
            # PERSON tr∆∞·ªõc ho·∫∑c sau verb (¬±40 k√Ω t·ª±)
            if re.search(rf"{name}.{{0,40}}{act}", t):
                return True
            if re.search(rf"{act}.{{0,40}}{name}", t):
                return True

    return False

def is_political_actor(text: str, person: str) -> bool:
    t = text.lower()
    p = person.lower()

    for k in [
        "l√™n ng√¥i",
        "nh∆∞·ªùng ng√¥i",
        "ban chi·∫øu",
        "x∆∞ng v∆∞∆°ng",
        "tr·ªã v√¨",
        "ƒë·ªïi qu·ªëc hi·ªáu",
        "l·∫≠p nh√†",
        "d·ª±ng ch√≠nh quy·ªÅn",
        "ra ƒëi",
        "so√°n ng√¥i",
        "l·∫≠t ƒë·ªï",
        "th√†nh l·∫≠p",
        "h·∫° chi·∫øu",
        "ban b·ªë",
    ]:
        if re.search(rf"{p}.{{0,40}}{k}|{k}.{{0,40}}{p}", t):
            return True

    return False

def extract_persons_from_body(text: str) -> set[str]:
    all_persons = set(cached_extract_all_persons(text))
    subjects = set()

    # üëë vua ‚Üí lu√¥n l√† subject
    kings = {
        p for p in all_persons
        if re.search(r"(th√°i\s+(t·ªï|t√¥ng)|th√°nh\s+t√¥ng|nh√¢n\s+t√¥ng)$", p.lower())
    }
    if kings:
        return {canonical_person(k) for k in kings}

    for p in all_persons:
        if is_person_actor(text, p) or is_political_actor(text, p):
            subjects.add(canonical_person(p))

    return subjects

def clean_text(text):
    if not text:
        return None

    for p in JUNK_PATTERNS:
        text = re.sub(p, "", text, flags=re.I)

    text = re.sub(
        r"^(NƒÉm\s+[0-9]{3,4}[,:]?\s*)([^.]{0,80}?)(?:di·ªÖn ra|x·∫£y ra)\s+",
        r"\1\2",
        text,
        flags=re.I
    )

    text = re.sub(r"2/9/?\s*1945", "ng√†y 2 th√°ng 9 nƒÉm 1945", text)
    text = re.sub(r"[;:]", ".", text)
    text = re.sub(r"\s+", " ", text)

    sentences = re.split(r"(?<=\.)\s+", text)

    if len(sentences) >= 2 and len(sentences[0]) < 40:
        sentences[0] = sentences[0].rstrip(". ,") + ", " + sentences[1]
        sentences = [sentences[0]] + sentences[2:]

    text = " ".join(sentences[:2])
    text = re.sub(r"ƒë·ªïi\s*m·ªõi", "ƒê·ªïi m·ªõi", text, flags=re.I)

    return text.strip(" ,.-") if len(text) >= 50 else None

CORE_ACTIONS = [
    "l√™n ng√¥i", "t√¥n l·∫≠p", "x∆∞ng v∆∞∆°ng",
    "ban chi·∫øu", "k√Ω hi·ªáp ƒë·ªãnh",
    "ƒë√°nh b·∫°i", "ƒë√°nh tan", "gi√†nh th·∫Øng l·ª£i",
    "kh·ªüi nghƒ©a", "kh√°ng chi·∫øn",
    "th√†nh l·∫≠p", "ƒë·ªïi qu·ªëc hi·ªáu",
    "gi·∫£i ph√≥ng", "th·ªëng nh·∫•t"
]

def choose_representative_event(events: list[str]) -> str:
    def score(e: str):
        s = 0
        if any(k in e.lower() for k in ["m·ªü ra", "ch·∫•m d·ª©t", "kh·∫≥ng ƒë·ªãnh"]):
            s += 2
        if any(k in e.lower() for k in CORE_ACTIONS):
            s += 2
        s += len(e) / 100
        return s

    return max(events, key=score)

def extract_all_persons(text: str) -> set[str]:
    persons: set[str] = set()

    if not text:
        return persons

    for m in PERSON_PATTERN.finditer(text):
        raw = m.group(1).strip()
        p = canonical_person(raw)

        # validate h√¨nh th·ª©c ng∆∞·ªùi
        if not is_valid_person(p):
            continue

        # lo·∫°i n·∫øu entity registry n√≥i KH√îNG ph·∫£i ng∆∞·ªùi
        kind = classify_entity(p)
        if kind and kind != "person":
            continue

        persons.add(p)

    return persons

def resolve_entity(name: str):
    kind = classify_entity(name)
    if kind:
        return kind

    UNKNOWN_ENTITIES.add(name)
    return None


def pick_tone(tones):
    """
    Ch·ªçn tone ƒë·∫°i di·ªán ƒë·ªÉ k·ªÉ chuy·ªán.
    ∆Øu ti√™n: heroic > tragic > neutral
    """
    if not tones:
        return "neutral"

    if isinstance(tones, (list, set)):
        if "heroic" in tones:
            return "heroic"
        if "tragic" in tones:
            return "tragic"
        return next(iter(tones))

    return tones


def ask_by_person(timeline, name: str):
    name = canonical_person(name)
    results = []

    for year, block in timeline.items():
        for e in block["events"]:
            if any(name.lower() == p.lower() for p in e.get("persons_all", [])):
                subject = infer_subject(
                    e["event"],
                    set(e.get("persons", [])),
                    e["nature"]
                )
                results.append(
                    storyteller(
                        int(year),
                        pick_tone(e["tone"]),
                        e["event"],
                        subject
                    )
                )

    return results or None

def remove_non_informative_clauses(text):
    clauses = [c.strip() for c in text.split(",") if c.strip()]
    kept = []

    for c in clauses:
        # ‚úÖ c√≥ PERSON ‚Üí gi·ªØ
        if cached_extract_all_persons(c):
            kept.append(c)
            continue


        lc = c.lower()

        if any(v in lc for v in INFORMATIVE_VERBS + STATE_VERBS):
            kept.append(c)
            continue

        if re.search(
            r"(qu√¢n|ƒë√¥|kinh|s√¥ng|th√†nh|hi·ªáp ƒë·ªãnh|qu·ªëc hi·ªáu|b√†i|t√°c ph·∫©m|c·∫£i c√°ch)",
            lc
        ):
            kept.append(c)

    return ", ".join(kept)

def remove_year_phrases(text, year):
    text = re.sub(
        rf"(di·ªÖn ra|x·∫£y ra)(?:\s+(?:v√†o|trong))?\s+nƒÉm\s+{year}",
        "",
        text,
        flags=re.I
    )
    return re.sub(r"\s+,", ",", text).strip(" ,.")

def force_person_from_text(event_text: str) -> list[str]:
    forced = []

    FORCE_MAP = {
        "Nguy·ªÖn T·∫•t Th√†nh": "Nguy·ªÖn T·∫•t Th√†nh",
        "H·ªì Ch√≠ Minh": "H·ªì Ch√≠ Minh",
        "Quang Trung": "Nguy·ªÖn Hu·ªá",
        "Nguy·ªÖn Hu·ªá": "Nguy·ªÖn Hu·ªá",
        "Gia Long": "Nguy·ªÖn √Ånh",
    }

    for k, v in FORCE_MAP.items():
        if k in event_text:
            forced.append(v)

    return forced


def merge_events_by_year(events: list[dict]) -> list[dict]:
    """
    Merge c√°c s·ª± ki·ªán TR√ôNG N·ªòI DUNG trong c√πng m·ªôt nƒÉm
    """
    if not events:
        return []

    buckets: dict[str, list[dict]] = defaultdict(list)

    # 1Ô∏è‚É£ Bucket theo ch·ªØ k√Ω n·ªôi dung
    for e in events:
        sig = event_signature(e["event"])
        buckets[sig].append(e)

    merged_events: list[dict] = []

    # 2Ô∏è‚É£ Merge t·ª´ng bucket
    for bucket in buckets.values():
        base = {
            "year": bucket[0]["year"],
            "event": choose_representative_event(
                [b["event"] for b in bucket]
            ),
            "persons": sorted(
                set(p for b in bucket for p in b.get("persons", []))
            ),
            "persons_all": sorted(
                set(p for b in bucket for p in b.get("persons_all", []))
            ),
            "nature": sorted(
                set(n for b in bucket for n in b.get("nature", []))
            ),
            "tone": sorted(
                set(t for b in bucket for t in b.get("tone", []))
            ),
            "keywords": sorted(
                set(k for b in bucket for k in b.get("keywords", []))
            ),
        }

        merged_events.append(base)

    return merged_events

def build_year_summary(events):
    tones = set(t for e in events for t in e["tone"])
    natures = set(n for e in events for n in e["nature"])

    if "heroic" in tones and "tragic" in tones:
        return "M·ªôt nƒÉm mang t√≠nh b∆∞·ªõc ngo·∫∑t, v·ª´a ghi d·∫•u th·∫Øng l·ª£i l·ªõn v·ª´a ƒë·ªÉ l·∫°i h·ªá qu·∫£ l·ªãch s·ª≠ s√¢u s·∫Øc."

    if "heroic" in tones:
        if "military" in natures:
            return "M·ªôt nƒÉm ghi d·∫•u th·∫Øng l·ª£i qu√¢n s·ª± quan tr·ªçng c·ªßa d√¢n t·ªôc."
        return "M·ªôt nƒÉm ƒë√°nh d·∫•u b∆∞·ªõc ti·∫øn l·ªõn trong ti·∫øn tr√¨nh l·ªãch s·ª≠ d√¢n t·ªôc."


    if "tragic" in tones:
        return "M·ªôt nƒÉm ƒë·∫ßy bi·∫øn c·ªë, ƒë·ªÉ l·∫°i nh·ªØng t·ªïn th·∫•t v√† chia c·∫Øt l·ªãch s·ª≠."

    return "M·ªôt nƒÉm c√≥ nh·ªØng chuy·ªÉn bi·∫øn quan tr·ªçng trong ti·∫øn tr√¨nh l·ªãch s·ª≠."

def extract_year(text: str):
    if m := DATE_WITH_YEAR.search(text):
        return m.group(2)

    if m := YEAR_ANY.search(text):
        return m.group(1)

    return None

def purge(text):
    text = re.sub(r"\s+\.", ".", text)
    text = re.sub(r"\.+", ".", text)
    return text.strip(" .,-")

def classify_tone(text: str, year: str | None = None) -> set[str]:
    t = text.lower()
    tones = set()

    heroic = [
        "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ƒë·∫©y lui",
        "to√†n th·∫Øng", "gi·∫£i ph√≥ng",
        "th·ªëng nh·∫•t", "gi√†nh ƒë·ªôc l·∫≠p",
        "t·ª± ch·ªß", "ch·∫•m d·ª©t √°ch",
        "bu·ªôc qu√¢n", "bu·ªôc ph·∫£i"
    ]
    if any(k in t for k in heroic):
        tones.add("heroic")

    tragic = [
        "b·ªã x√¢m l∆∞·ª£c", "m·∫•t n∆∞·ªõc",
        "b·∫Øc thu·ªôc", "minh thu·ªôc",
        "chia c·∫Øt", "√°p ƒë·∫∑t",
        "m·ªü ƒë·∫ßu cu·ªôc chi·∫øn"
    ]

    if any(k in t for k in tragic):
        tones.add("tragic")

    if year:
        y = int(year)
        if y in HEROIC_YEARS:
            tones.add("heroic")
        if y in TRAGIC_YEARS:
            tones.add("tragic")

    return tones or {"neutral"}

def classify_nature(text: str) -> list[str]:
    t = text.lower()
    labels = []

    if any(k in t for k in [
        "kh·ªüi nghƒ©a", "kh√°ng chi·∫øn",
        "chi·∫øn d·ªãch", "ti·∫øn c√¥ng",
        "ph·∫£n c√¥ng", "t·∫•n c√¥ng",
        "x√¢m l∆∞·ª£c", "ƒë√°nh b·∫°i",
        "ƒë√°nh tan", "ƒë√°nh lui",
        "chi·∫øn th·∫Øng", "bu·ªôc qu√¢n r√∫t"
    ]):
        labels.append("military")

    if any(k in t for k in [
        "l√™n ng√¥i", "t√¥n l·∫≠p",
        "x∆∞ng v∆∞∆°ng", "d·ª±ng ch√≠nh quy·ªÅn",
        "th√†nh l·∫≠p"
    ]):
        labels.append("political")

    if any(k in t for k in [
        "hi·ªáp ƒë·ªãnh", "k√Ω k·∫øt",
        "ƒë√†m ph√°n"
    ]):
        labels.append("diplomacy")

    if any(k in t for k in [
        "ƒë·ªïi qu·ªëc hi·ªáu", "ban chi·∫øu",
        "c·∫£i c√°ch"
    ]):
        labels.append("institutional")

    return sorted(set(labels)) if labels else ["general"]

HEROIC_ENDINGS = [
    "S·ª± ki·ªán n√†y m·ªü ra m·ªôt ch∆∞∆°ng s·ª≠ h√†o h√πng c·ªßa d√¢n t·ªôc.",
    "Chi·∫øn c√¥ng ·∫•y kh·∫≥ng ƒë·ªãnh √Ω ch√≠ t·ª± ch·ªß v√† s·ª©c s·ªëng b·ªÅn b·ªâ c·ªßa ng∆∞·ªùi Vi·ªát.",
    "ƒê√¢y l√† d·∫•u m·ªëc th·ªÉ hi·ªán b·∫£n lƒ©nh v√† kh√°t v·ªçng l√†m ch·ªß v·∫≠n m·ªánh d√¢n t·ªôc.",
]

TRAGIC_ENDINGS = [
    "ƒê√≥ l√† giai ƒëo·∫°n bi th∆∞∆°ng, khi ƒë·∫•t n∆∞·ªõc r∆°i v√†o th·ª≠ th√°ch kh·∫Øc nghi·ªát.",
    "Bi·∫øn c·ªë n√†y ƒë·ªÉ l·∫°i nh·ªØng m·∫•t m√°t s√¢u s·∫Øc cho v·∫≠n m·ªánh d√¢n t·ªôc.",
    "Th·ªùi k·ª≥ ·∫•y ghi d·∫•u n·ªói ƒëau v√† nh·ªØng t·ªïn th·∫•t n·∫∑ng n·ªÅ c·ªßa ƒë·∫•t n∆∞·ªõc.",
]





def storyteller(year, kind, content, subject=None):
    content = content.rstrip(".")

    if subject:
        content = subject + " " + content[0].lower() + content[1:]

    if kind == "heroic":
        return (
            f"NƒÉm {year}, {content}. "
            f"{random.choice(HEROIC_ENDINGS)}"
        )

    if kind == "tragic":
        return (
            f"NƒÉm {year}, {content}. "
            f"{random.choice(TRAGIC_ENDINGS)}"
        )

    return f"NƒÉm {year}, {content}."


def collapse_year_events(events: list[dict]) -> list[dict]:
    groups = {}

    for e in events:
        sig = event_signature(e["event"])
        groups.setdefault(sig, []).append(e)

    collapsed = []

    for sig, group in groups.items():
        best = max(
            group,
            key=lambda x: (
                len(x["event"]),
                "r·ªùi b·∫øn" in x["event"],
                "ban" in x["event"],
                "k√Ω" in x["event"],
                "tuy√™n" in x["event"]
            )
        )
        all_nature = sorted(set(n for g in group for n in g["nature"]))
        all_tone = sorted(set(t for g in group for t in g["tone"]))

        collapsed.append({
            "year": best["year"],
            "event": best["event"],
            "nature": all_nature,
            "tone": all_tone
        })


    return collapsed


def deduplicate_phrases(text):
    parts = re.split(r"[.]", text)
    seen = set()
    result = []

    for p in parts:
        key = re.sub(r"\W+", "", p.lower())
        if key and key not in seen:
            seen.add(key)
            result.append(p.strip())

    return ". ".join(result).strip()

def extract_core_tokens(text: str) -> set:
    cores = set()
    for a in CORE_ACTIONS:
        if a in text.lower():
            cores.add(a)

    names = re.findall(
        r"[a-z√†-·ªπ]+(?:\s+[a-z√†-·ªπ]+){1,3}", text.lower()
    )
    cores.update(names[:2])
    return cores

def collapse_fragments(text):
    return re.sub(r"\.\s+([a-z√†-·ªπ])", r", \1", text, flags=re.I)

def remove_repeated_subject(text):
    parts = re.split(r"\.\s+", text)
    if len(parts) < 2:
        return text

    first = parts[0]
    names = re.findall(
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+)+", first
    )

    for i in range(1, len(parts)):
        for name in names:
            parts[i] = re.sub(rf"^{name}\s*", "", parts[i])

    return ". ".join(parts).strip()

def remove_redundant_actions(text):
    clauses = [c.strip() for c in re.split(r",", text)]
    kept = []
    used_groups = set()

    for clause in clauses:
        lowered = clause.lower()
        matched_group = None

        for i, group in enumerate(ACTION_GROUPS):
            if any(k in lowered for k in group):
                matched_group = i
                break

        if matched_group is not None:
            if matched_group in used_groups:
                continue
            used_groups.add(matched_group)

        kept.append(clause)

    return ", ".join(kept)

def normalize_event_text(text: str) -> set:
    text = text.lower()

    text = re.sub(r"\b(1[0-9]{3})\b", "", text)
    text = re.sub(
        r"(di·ªÖn ra|x·∫£y ra|nƒÉm|sau khi|ƒë∆∞·ª£c|v√†o|ƒë√£|c√°c)",
        "",
        text
    )
    text = re.sub(r"[^\w\s]", " ", text)

    words = [
        w for w in text.split()
        if w not in STOPWORDS and len(w) > 3
    ]

    return set(words)

def normalize_temporal_clause(text: str) -> str:
    return re.sub(
        r",?\s*Sau khi [^,]+?(?=,|$)",
        "",
        text,
        flags=re.I
    )




def is_same_event(e1: str, e2: str, threshold=0.3) -> bool:
    # 1. So core (ch·ªß th·ªÉ + h√†nh ƒë·ªông)
    c1 = extract_core_tokens(e1)
    c2 = extract_core_tokens(e2)

    if c1 and c2 and len(c1 & c2) >= 2:
        return True


    # 2. Fallback bag-of-words
    s1 = normalize_event_text(e1)
    s2 = normalize_event_text(e2)

    if not s1 or not s2:
        return False

    overlap = len(s1 & s2)
    score = overlap / min(len(s1), len(s2))

    return score >= threshold


def lowercase_after_comma(text):
    return re.sub(
        r",\s+(?![A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+[A-Zƒê√Ç√ä√î∆Ø])([A-Zƒê√Ç√ä√î∆Ø])",
        lambda m: ", " + m.group(1).lower(),
        text
    )


def remove_repeated_subject_inline(text):
    names = re.findall(
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+)+",
        text
    )
    if len(names) < 2:
        return text

    main = names[0]

    text = re.sub(
        rf",\s*{re.escape(main)}\b",
        ",",
        text
    )

    text = re.sub(
        rf",\s*{re.escape(main)}\s+",
        ", ",
        text
    )

    return re.sub(r"\s+,", ",", text)

def normalize_titles(text):
    return re.sub(
        r",\s*(VƒÉn ki·ªán|T√°c ph·∫©m|S·ª± ki·ªán)\s+",
        ", ",
        text,
        flags=re.I
    )

def event_signature(text: str) -> str:
    text = text.lower()
    text = re.sub(r"\b1[0-9]{3}\b", "", text)
    text = re.sub(
        r"(di·ªÖn ra|x·∫£y ra|nƒÉm|sau khi|ƒë∆∞·ª£c|vua|tri·ªÅu)",
        "",
        text
    )
    text = re.sub(r"[^\w\s]", " ", text)

    tokens = [
        w for w in text.split()
        if w not in STOPWORDS and len(w) > 3
    ]

    return " ".join(tokens[:8])   # ‚¨Ö tƒÉng t·ª´ 6 ‚Üí 8

def extract_keywords(text: str) -> list[str]:
    keywords = set()

    persons = re.findall(
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+){1,3}",
        text
    )

    for p in persons:
        if is_valid_person(p):
            keywords.add(p)
            break

    actions = re.findall(
        r"(ƒë√°nh b·∫°i|ƒë√°nh tan|l√™n ng√¥i|x∆∞ng v∆∞∆°ng|"
        r"kh·ªüi nghƒ©a|kh√°ng chi·∫øn|"
        r"ban chi·∫øu|k√Ω hi·ªáp ƒë·ªãnh|"
        r"th√†nh l·∫≠p|gi·∫£i ph√≥ng|th·ªëng nh·∫•t)",
        text.lower()
    )

    keywords.update(actions)
    return sorted(keywords)

def merge_events(events: list[str]) -> str:
    clauses = []

    for e in events:
        e = re.sub(r"(di·ªÖn ra|x·∫£y ra)", "", e)
        clauses.append(e.strip(" ,."))

    base = max(clauses, key=len)

    for c in clauses:
        if "m·ªü ra" in c or "ch·∫•m d·ª©t" in c or "kh·∫≥ng ƒë·ªãnh" in c:
            if c not in base:
                base += ", " + c

    return purge(base)

def prune_event_sentence(text: str) -> str:
    clauses = [c.strip() for c in text.split(",") if c.strip()]
    if len(clauses) <= 1:
        return text

    kept = []

    # 1Ô∏è‚É£ clause c√≥ PERSON
    for c in clauses:
        if cached_extract_all_persons(c):
            kept.append(c)


    # 2Ô∏è‚É£ n·∫øu ch∆∞a c√≥ ‚Üí clause c√≥ action
    if not kept:
        for c in clauses:
            if any(v in c.lower() for v in INFORMATIVE_VERBS + STATE_VERBS):
                kept.append(c)

    # 3Ô∏è‚É£ gi·ªØ h·ªá qu·∫£ l·ªãch s·ª≠
    for c in clauses:
        if any(k in c.lower() for k in ["m·ªü ra", "ch·∫•m d·ª©t", "kh·∫≥ng ƒë·ªãnh"]):
            if c not in kept:
                kept.append(c)

    return ", ".join(kept or [clauses[0]])


def ask_by_event(timeline, query: str):
    query = query.lower()
    matches = []

    for year, block in timeline.items():
        for e in block["events"]:
            if query in e["event"].lower():
                matches.append((year, e))

    if not matches:
        return None

    year, event = max(matches, key=lambda x: len(x[1]["event"]))
    tone = pick_tone(event.get("tone", []))

    return storyteller(int(year), tone, event["event"])

def scan_by_entity(timeline, entity: str):
    entity = entity.lower()
    results = []

    for year, block in timeline.items():
        for e in block["events"]:
            persons = [p.lower() for p in e.get("persons", [])]
            if any(entity in p for p in persons):
                results.append({
                    "year": int(year),
                    "event": e["event"],
                    "tone": e["tone"],
                    "nature": e["nature"]
                })

    return sorted(results, key=lambda x: x["year"])


def ask_by_year(timeline, year: int):
    block = timeline.get(str(year))
    if not block:
        return f"Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán n√†o trong nƒÉm {year}."

    results = []
    for e in block["events"]:
        subject = infer_subject(
            e["event"],
            set(e.get("persons", [])),
            e["nature"]
        )
        results.append(
            storyteller(
                year,
                pick_tone(e["tone"]),
                e["event"],
                subject
            )
        )

    return results

def ask(timeline, question: str):
    q = question.strip().lower()

    # 1Ô∏è‚É£ h·ªèi theo nƒÉm
    m = re.search(r"nƒÉm\s+([1-9][0-9]{2,3})", q)
    if m:
        return ask_by_year(timeline, int(m.group(1)))

    # 2Ô∏è‚É£ h·ªèi theo nh√¢n v·∫≠t
    person = extract_person_query(q)
    person_answer = ask_by_person(timeline, person)
    if person_answer:
        return person_answer

    # 3Ô∏è‚É£ fallback h·ªèi theo s·ª± ki·ªán
    return ask_by_event(timeline, q)


def load_timeline(path=OUT_PATH):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def narrate_year(timeline, year):
    block = timeline.get(str(year))
    if not block:
        return None

    events = block["events"]

    merged = merge_events([e["event"] for e in events])
    tones = sorted(set(t for e in events for t in e["tone"]))

    return {
        "year": int(year),
        "event": merged,
        "tone": tones
    }

def remove_duplicate_subjects_global(text):
    clauses = [c.strip() for c in text.split(",")]
    if len(clauses) < 2:
        return text

    m = re.match(
        r"^([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+)+)\b",
        clauses[0]
    )
    if not m:
        return text

    subject = m.group(1)

    cleaned = [clauses[0]]
    for c in clauses[1:]:
        c = re.sub(rf"^{subject}\s*", "", c)
        cleaned.append(c)

    return ", ".join(cleaned)

def infer_subject(body: str, persons: set, nature: list[str]) -> str | None:
    if persons:
        return next(iter(persons))

    t = body.lower()

    if "military" in nature:
        if "qu√¢n d√¢n" in t:
            return "Qu√¢n d√¢n Vi·ªát Nam"
        if "nghƒ©a qu√¢n" in t:
            return "Nghƒ©a qu√¢n"
        return "L·ª±c l∆∞·ª£ng ƒë∆∞∆°ng th·ªùi"

    if any(n in nature for n in ["political", "institutional", "diplomacy"]):
        return "Ch√≠nh quy·ªÅn ƒë∆∞∆°ng th·ªùi"

    if re.search(r"(chi·∫øu|hi·ªáp ƒë·ªãnh|tuy√™n ng√¥n|s·∫Øc l·ªánh)", t):
        return "VƒÉn ki·ªán l·ªãch s·ª≠"

    return None

def render_event_with_subject(year, body, subject=None):
    if subject:
        return f"NƒÉm {year}, {subject} {body[0].lower() + body[1:]}."
    return f"NƒÉm {year}, {body}."


def extract_person_query(q: str) -> str | None:
    q = q.lower().strip()

    matches = re.findall(
        r"[a-z√†-·ªπ]+(?:\s+[a-z√†-·ªπ]+){1,3}",
        q
    )

    for m in sorted(matches, key=len, reverse=True):
        if m in NON_PERSON_PHRASES:
            continue

        p = canonical_person(m)
        if is_valid_person(p):
            return p

    return None

def fix_common_noun_phrases(text):
    return re.sub(
        r",\s*(Qu√¢n|Nghƒ©a qu√¢n|Tri·ªÅu|Ch√≠nh quy·ªÅn)\b",
        lambda m: ", " + m.group(1).lower(),
        text
    )

def smooth_actions(text):
    text = re.sub(
        r"(ƒë√°nh (?:tan|b·∫°i|lui)[^,]+),\s*(l√£nh ƒë·∫°o[^,]+)",
        r"\2 \1",
        text,
        flags=re.I
    )

    text = re.sub(
        r"(d·ª±ng[^,]+),\s*(n·∫Øm quy·ªÅn[^,]+)",
        r"\1, sau ƒë√≥ \2",
        text,
        flags=re.I
    )

    text = re.sub(
        r"(d√πng[^,]+),\s*(ƒë√°nh (?:b·∫°i|tan|lui)[^,]+)",
        r"\1 v√† \2",
        text,
        flags=re.I
    )

    return text

def is_collective_event(nature: list[str], body: str) -> bool:
    # ‚õî n·∫øu text c√≥ PERSON ‚Üí KH√îNG BAO GI·ªú collective
    if extract_all_persons(body):
        return False

    t = body.lower()

    if "military" in nature and any(k in t for k in [
        "c√°ch m·∫°ng",
        "t·ªïng ti·∫øn c√¥ng",
        "kh√°ng chi·∫øn",
        "chi·∫øn d·ªãch",
        "to√†n th·∫Øng",
        "qu√¢n d√¢n"
    ]):
        return True

    return False

def extract_implicit_ruler(text: str) -> set[str]:
    persons = set()

    patterns = [
        r"(?:Vua\s+)?([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+(?:Th√°i|Th√°nh|Nh√¢n)\s+(?:T·ªï|T√¥ng))",
        r"(?:th·ªùi|d∆∞·ªõi th·ªùi|tri·ªÅu)\s+([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+(?:Th√°i|Th√°nh|Nh√¢n)\s+(?:T·ªï|T√¥ng))",
    ]

    for p in patterns:
        for m in re.findall(p, text):
            persons.add(canonical_person(m))

    return persons


@lru_cache(maxsize=100_000)
def cached_extract_all_persons(text: str) -> tuple[str, ...]:
    """
    Cache k·∫øt qu·∫£ extract person theo text
    """
    return tuple(extract_all_persons(text))

def normalize(text: str):

    if not text:
        return None

    if len(text) < 50:
        return None

    # b·ªè c√¢u h·ªèi / chat
    if "?" in text:
        return None

    raw = clean_text(text)
    if not raw:
        return None

    year = extract_year(raw)
    if not year:
        return None

    body_raw = re.sub(
        rf"^NƒÉm\s+{year}[,:.\s-]*",
        "",
        raw,
        flags=re.I
    )

    # ===== PERSON TR∆Ø·ªöC KHI C·∫ÆT =====
    persons_subject = extract_persons_from_body(body_raw)
    persons_all = extract_all_persons(body_raw)


    # sau khi c√≥ persons_subject, persons_all
    forced = force_person_from_text(body_raw)
    persons_subject |= set(forced)
    persons_all |= set(forced)


    # üëë vua ng·∫ßm
    implicit = extract_implicit_ruler(body_raw)
    persons_subject |= implicit
    persons_all |= implicit

    # PERSON trong ngo·∫∑c
    body, parenthetical_persons = extract_parenthetical_persons(body_raw)
    persons_all |= set(parenthetical_persons)

    # ===== TEXT PIPELINE =====
    body = purge(body)
    body = remove_year_phrases(body, year)
    body = normalize_titles(body)
    body = prune_event_sentence(body)
    body = deduplicate_phrases(body)
    body = normalize_temporal_clause(body)
    body = remove_duplicate_subjects_global(body)
    body = lowercase_after_comma(body)
    body = fix_common_noun_phrases(body)
    body = remove_redundant_actions(body)
    body = remove_non_informative_clauses(body)
    body = collapse_fragments(body)
    body = remove_repeated_subject_inline(body)
    body = remove_repeated_subject(body)

    if len(body) < 30:
        return None

    nature = classify_nature(body)
    tone = classify_tone(body, year)

    body = body[0].upper() + body[1:]

    return year, body, nature, tone, persons_subject, persons_all


def iter_raw(ds):
    for row in ds:
        for m in row.get("messages", []):
            if m.get("role") == "assistant":
                yield m.get("content", "")


def main():
    print("[INFO] Loading dataset...")
    ds = load_dataset("arrow", data_files=ARROW_FILES, split="train")

    timeline: dict[str, list[dict]] = {}

    total_raw = 0
    total_kept = 0

    for line in iter_raw(ds):
        total_raw += 1

        res = normalize(line)
        if not res:
            continue

        year, body, nature, tone, persons_subject, persons_all = res

        timeline.setdefault(year, []).append({
            "year": int(year),
            "event": body,
            "persons": sorted(persons_subject),
            "persons_all": sorted(persons_all),
            "nature": set(nature),
            "tone": tone,
            "keywords": set(extract_keywords(body))
        })

        total_kept += 1

    final_timeline = {}

    for year, events in timeline.items():
        final_timeline[year] = {
            "summary": build_year_summary(events),
            "events": merge_events_by_year(events)
        }

    timeline = final_timeline

    Path(OUT_PATH).parent.mkdir(parents=True, exist_ok=True)
    with open(OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(timeline, f, ensure_ascii=False, indent=2)

    print(
        f"[DONE] Raw: {total_raw} | "
        f"Gi·ªØ l·∫°i: {total_kept} | "
        f"NƒÉm c√≥ s·ª± ki·ªán: {len(timeline)}"
    )

if __name__ == "__main__":
    main()

