import os
import sys
import json
import re
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# ===================== FIX WINDOWS ENCODING =====================
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

# ===================== CONFIG =====================
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

FAISS_INDEX_PATH = "./faiss_index/history.index"
META_PATH = "./faiss_index/meta.json"

MODEL_PATH = "./models/qwen2.5-7b-instruct-q4_k_m.gguf"

TOP_K = 8

SYSTEM_RULES = """B·∫°n l√† tr·ª£ l√Ω AI l·ªãch s·ª≠ Vi·ªát Nam.
CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong t√†i li·ªáu.
KH√îNG suy ƒëo√°n.
KH√îNG d√πng ki·∫øn th·ª©c b√™n ngo√†i.
N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin, ch·ªâ tr·∫£ l·ªùi ƒë√∫ng 1 c√¢u:
Kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu.
Ch·ªâ tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
"""

YEAR_PATTERN = re.compile(r"\b(1[0-9]{3})\b")

# ===================== LOAD FAISS =====================
def load_faiss():
    index = faiss.read_index(FAISS_INDEX_PATH)
    with open(META_PATH, encoding="utf-8") as f:
        meta = json.load(f)
    docs = [m["text"] for m in meta]
    return index, docs

# ===================== YEAR EXTRACTION =====================
def extract_year(query: str):
    m = YEAR_PATTERN.search(query)
    return m.group(1) if m else None

# ===================== QUERY EXPANSION =====================
def expand_query(query: str):
    """
    Gi·ªØ m·ªü r·ªông NH·∫∏ ƒë·ªÉ tƒÉng recall,
    KH√îNG quy·∫øt ƒë·ªãnh logic ·ªü ƒë√¢y
    """
    queries = [query]
    year = extract_year(query)
    if year:
        queries.append(f"NƒÉm {year}")
    return queries

# ===================== FAISS RETRIEVAL =====================
def retrieve_context(query, embedder, index, docs):
    queries = expand_query(query)
    results = []

    for q in queries:
        emb = embedder.encode([q], convert_to_numpy=True).astype("float32")
        faiss.normalize_L2(emb)
        _, ids = index.search(emb, TOP_K)

        for i in ids[0]:
            if 0 <= i < len(docs):
                results.append(docs[i])

    # unique, gi·ªØ th·ª© t·ª±
    seen = set()
    uniq = []
    for r in results:
        if r not in seen:
            uniq.append(r)
            seen.add(r)

    return uniq

# ===================== HARD FILTER BY YEAR =====================
def filter_by_year(docs, year):
    """
    QUY·∫æT ƒê·ªäNH B·∫∞NG CODE ‚Äì KH√îNG GIAO CHO LLM
    """
    if not year:
        return docs

    filtered = []
    for d in docs:
        if d.startswith(f"NƒÉm {year},"):
            filtered.append(d)

    return filtered

# ===================== PROMPT =====================
def build_prompt(context_docs, question):
    context = "\n".join(context_docs)
    return f"""{SYSTEM_RULES}

T√ÄI LI·ªÜU:
{context}

C√ÇU H·ªéI:
{question}

TR·∫¢ L·ªúI:
"""

# ===================== MAIN =====================
def main():
    embedder = SentenceTransformer(EMBED_MODEL)
    index, docs = load_faiss()

    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=4096,
        temperature=0.0,
        n_threads=8,
        n_gpu_layers=0,
        verbose=False
    )

    print("\nüëâ G√µ c√¢u h·ªèi (exit ƒë·ªÉ tho√°t)\n")

    while True:
        query = input("üßë B·∫°n: ").strip()
        if query.lower() == "exit":
            break

        year = extract_year(query)

        # 1Ô∏è‚É£ Retrieve
        ctx_raw = retrieve_context(query, embedder, index, docs)

        # 2Ô∏è‚É£ HARD FILTER (QUAN TR·ªåNG NH·∫§T)
        ctx = filter_by_year(ctx_raw, year)

        # 3Ô∏è‚É£ Kh√¥ng c√≥ ‚Üí tr·∫£ l·ªùi c·ª©ng
        if not ctx:
            print("\nü§ñ AI: Kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu.\n")
            continue

        # 4Ô∏è‚É£ Build prompt & generate
        prompt = build_prompt(ctx, query)

        output = llm(
            prompt,
            max_tokens=120,
            stop=["\n", "Human:", "Assistant:", "ËØ∑", "Premier", "„ÄÇ"]
        )

        raw = output["choices"][0]["text"].strip()
        answer = raw.split("\n")[0].strip()

        if "." in answer:
            answer = answer.split(".")[0] + "."

        if not answer:
            answer = "Kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu."

        print(f"\nü§ñ AI: {answer}\n")

# ===================== RUN =====================
if __name__ == "__main__":
    main()
