import sys, os, types, io

# ===================== SI√äU V√Å L·ªñI TRANSFORMERS (WINDOWS) =====================
import transformers

# 1. ƒê√°nh l·ª´a transformers r·∫±ng flash_attn KH√îNG t·ªìn t·∫°i (ƒë·ªÉ tr√°nh l·ªói __spec__)
original_is_package_available = transformers.utils.import_utils._is_package_available
def patched_is_package_available(pkg_name):
    if pkg_name in ["flash_attn", "triton"]:
        return False
    return original_is_package_available(pkg_name)

transformers.utils.import_utils._is_package_available = patched_is_package_available

# 2. V√¥ hi·ªáu h√≥a Triton ·ªü m·ª©c h·ªá th·ªëng
os.environ["TORCH_COMPILE_DISABLE"] = "1"
os.environ["DISABLE_TRITON"] = "1"

# ===================== IMPORT CH√çNH =====================
import json
import faiss
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BloomTokenizerFast

# Fix Unicode hi·ªÉn th·ªã tr√™n Console Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

# ===================== CONFIG =====================
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
CHAT_MODEL  = "vinai/PhoGPT-4B-Chat"
INDEX_PATH = "./faiss_index/history.index"
META_PATH  = "./faiss_index/meta.json"

def main():
    print("[INFO] N·∫°p embedding model (CPU)...")
    embedder = SentenceTransformer(EMBED_MODEL, device="cpu")

    if not os.path.exists(INDEX_PATH):
        print("[L·ªñI] Kh√¥ng t√¨m th·∫•y FAISS index. H√£y ch·∫°y index_docs.py!")
        return

    with open(META_PATH, encoding="utf-8") as f:
        docs = json.load(f)
    index = faiss.read_index(INDEX_PATH)

    print("[INFO] N·∫°p PhoGPT-4B-Chat (CPU ‚Äì Y√™u c·∫ßu ~12GB RAM)...")

    # N·∫°p c·∫•u h√¨nh
    config = AutoConfig.from_pretrained(CHAT_MODEL, trust_remote_code=True)
    
    # √âp s·ª≠ d·ª•ng ki·∫øn tr√∫c attention c∆° b·∫£n (eager) ƒë·ªÉ kh√¥ng g·ªçi Triton/Flash
    config.attn_config = {"attn_impl": "torch"} 
    
    # N·∫°p Tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL, trust_remote_code=True, use_fast=False)
    except:
        tokenizer = BloomTokenizerFast.from_pretrained(CHAT_MODEL, trust_remote_code=True)

    # N·∫°p Model
    model = AutoModelForCausalLM.from_pretrained(
        CHAT_MODEL,
        config=config,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        device_map={"": "cpu"},
        attn_implementation="eager",
        low_cpu_mem_usage=True
    )
    model.eval()

    print("\n" + "="*40)
    print("üáªüá≥ HistoryMindAI ‚Äì ƒê√£ s·∫µn s√†ng tr·∫£ l·ªùi!")
    print("="*40 + "\n")

    while True:
        try:
            query = input("B·∫°n h·ªèi: ").strip()
        except EOFError: break
        if not query or query.lower() in ["exit", "tho√°t"]: break

        # RAG: T√¨m ng·ªØ c·∫£nh
        q_emb = embedder.encode([query])
        _, I = index.search(q_emb, 2)
        context = "\n".join(docs[i] for i in I[0])

        # Prompt format chu·∫©n PhoGPT
        prompt = f"### C√¢u h·ªèi: {query} D·ª±a tr√™n th√¥ng tin: {context} ### Tr·∫£ l·ªùi:"

        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                # max_new_tokens=400,
                # temperature=0.1,
                # top_p=0.9,
                # do_sample=True,
                # eos_token_id=tokenizer.eos_token_id

                max_new_tokens=100,  # Gi·ªõi h·∫°n tr·∫£ l·ªùi ng·∫Øn g·ªçn
                do_sample=False,     # Quan tr·ªçng: T·∫Øt c√°i n√†y gi√∫p CPU ch·∫°y nhanh h∆°n
                num_beams=1,         # Kh√¥ng d√πng t√¨m ki·∫øm ch√πm
                use_cache=True
            )

        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = text.split("### Tr·∫£ l·ªùi:")[-1].strip()
        print(f"\nBot: {answer}\n")

if __name__ == "__main__":
    main()