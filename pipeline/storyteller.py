import sys
sys.stdout.reconfigure(encoding="utf-8")

import json
import re
from pathlib import Path
# from datasets import load_dataset
import random
from functools import lru_cache
from collections import defaultdict

DATASET_DIR = Path(
    "vietnam_history_dataset/default/0.0.0/3fdbbebc92e755190b4eaeb1522d97a753f0f18a"
)

ARROW_FILES = [
    str(DATASET_DIR / "vietnam-history-1_m-vi-train-00000-of-00002.arrow"),
    str(DATASET_DIR / "vietnam-history-1_m-vi-train-00001-of-00002.arrow"),
]

UNKNOWN_ENTITIES = set()

OUT_PATH = "data/history_timeline.json"

YEAR_ANY = re.compile(r"(?<!\d)([1-9][0-9]{2,3})(?!\d)", re.UNICODE)

DATE_WITH_YEAR = re.compile(
    r"\b([0-3]?\d/[01]?\d/([1-9][0-9]{2,3}))\b"
)

YEAR_INLINE = re.compile(
    r"(?:ƒë·∫ßu|gi·ªØa|cu·ªëi|m√πa\s+\w+)?\s*nƒÉm\s+([1-9][0-9]{2,3})",
    re.I
)

PERSON_PATTERN = re.compile(
    r"\b(?:Vua\s+|Ch√∫a\s+|T∆∞·ªõng\s+|Trung t∆∞·ªõng\s+)?"
    r"((?:Nguy·ªÖn|L√™|L√Ω|Tr·∫ßn|ƒêinh|H·ªì|Ng√¥|Ph·∫°m|Phan|B√πi|ƒê·ªó|V≈©|V√µ|Ho√†ng|Hu·ª≥nh|ƒê·∫∑ng|D∆∞∆°ng|Kh√∫c|M·∫°c)(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+){1,3}"
    r"|[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+(?:Th√°i|Th√°nh|Nh√¢n)\s+(?:T·ªï|T√¥ng)"
    r"|(?:Quang\s+Trung|Gia\s+Long|Minh\s+M·∫°ng|T·ª±\s+ƒê·ª©c|H√†m\s+Nghi|B√°c\s+H·ªì|H·ªì\s+Ch√≠\s+Minh))\b"
)

ROYAL_TITLES = {
    "quang trung",
    "b·∫Øc b√¨nh v∆∞∆°ng",
    "gia long",
    "minh m·∫°ng",
    "t·ª± ƒë·ª©c",
    "thi·ªáu tr·ªã",
    "h√†m nghi",
}

PERSON_ALIASES = {
    "quang trung": "Nguy·ªÖn Hu·ªá",
    "b·∫Øc b√¨nh v∆∞∆°ng": "Nguy·ªÖn Hu·ªá",
    "gia long": "Nguy·ªÖn √Ånh",
}

# C√°c ti·ªÅn t·ªë/t·ª´ kh√≥a ch·ªâ t·∫≠p th·ªÉ, ƒë·ªãa danh ho·∫∑c t·ªï ch·ª©c
COLLECTIVE_PREFIXES = {
    "nh√†", "tri·ªÅu", "qu√¢n", "nghƒ©a qu√¢n", "ƒë·∫ø qu·ªëc", 
    "th·ª±c d√¢n", "ph√°t x√≠t", "nh√¢n d√¢n", "qu√¢n ƒë·ªôi"
}

# C√°c t·ª´ kh√≥a ch·ªâ s·ª± ki·ªán/t√°c ph·∫©m/ƒë·ªãa danh c·ª• th·ªÉ
HISTORY_EXCLUSIONS = {
    "b√¨nh ng√¥ ƒë·∫°i c√°o", "h·ªãch t∆∞·ªõng sƒ©", "nh·∫≠t k√Ω trong t√π", "tuy√™n ng√¥n ƒë·ªôc l·∫≠p",
    "ng√¥ ƒë·∫°i c√°o", "h√¨nh th∆∞", "hi·∫øn ph√°p", "lu·∫≠t h·ªìng ƒë·ª©c", "qu·ªëc h·ªôi",
    "b·∫°ch ƒë·∫±ng", "chi lƒÉng", "ƒë·ªëng ƒëa", "ƒëi·ªán bi√™n ph·ªß", "thƒÉng long", "h√† n·ªôi"
}

PERSON_ALIAS = {
    "Quang Trung": "Nguy·ªÖn Hu·ªá",
    "B·∫Øc B√¨nh V∆∞∆°ng": "Nguy·ªÖn Hu·ªá",
    "Nguy·ªÖn Hu·ªá": "Nguy·ªÖn Hu·ªá",
    "Gia Long": "Nguy·ªÖn √Ånh",
    "Nguy·ªÖn √Ånh": "Nguy·ªÖn √Ånh",
    "L√Ω C√¥ng U·∫©n": "L√Ω Th√°i T·ªï",
    "L√Ω Th√°i T·ªï": "L√Ω Th√°i T·ªï",
    "B√°c H·ªì": "H·ªì Ch√≠ Minh",
    "Nguy·ªÖn T·∫•t Th√†nh": "H·ªì Ch√≠ Minh",
    "Nguy·ªÖn √Åi Qu·ªëc": "H·ªì Ch√≠ Minh",
    "Tr·∫ßn Qu·ªëc Tu·∫•n": "Tr·∫ßn H∆∞ng ƒê·∫°o",
    "H∆∞ng ƒê·∫°o V∆∞∆°ng": "Tr·∫ßn H∆∞ng ƒê·∫°o",
    "H∆∞ng ƒê·∫°o ƒê·∫°i V∆∞∆°ng": "Tr·∫ßn H∆∞ng ƒê·∫°o",
    "L√™ T∆∞ Th√†nh": "L√™ Th√°nh T√¥ng",
    "L√™ Th√°nh T√¥ng": "L√™ Th√°nh T√¥ng"
}

def normalize_person(name: str) -> str:
    return PERSON_ALIAS.get(name.strip(), name.strip())

JUNK_PATTERNS = [
    r"ƒê·ªÉ tr·∫£ l·ªùi\.?",
    r"N·ªôi dung\.?",
    r"√ù nghƒ©a l·ªãch s·ª≠\.?",
    r"√ù nghƒ©a\.?",
    r"V·ªÅ l√¢u d√†i,?",
    r"ƒê√¢y l√† c·ªôt m·ªëc quan tr·ªçng v√¨",
    r"Tr√¨nh b√†y ng·∫Øn g·ªçn, m·∫°ch l·∫°c\.?",
    r"V√†o\s*,?",
    r"\*\*",
]

BAD_PERSON_KEYWORDS = {
    "Vi·ªát Nam", "ƒê·∫°i Vi·ªát", "ƒê·∫°i La", "ThƒÉng Long", "Hoa L∆∞",
    "ƒêi·ªán Bi√™n Ph·ªß", "Th√°ng T√°m", "B√¨nh Ng√¥", "Chi·∫øu", "Hi·ªáp"
}

def is_real_person(name: str) -> bool:
    if len(name.split()) < 2:
        return False
    for bad in BAD_PERSON_KEYWORDS:
        if bad in name:
            return False
    return True


ACTION_GROUPS = [
    ("ƒë√°nh tan", "ƒë·∫©y lui", "ƒë√°nh b·∫°i", "ti√™u di·ªát"),
    ("d·ª±ng ch√≠nh quy·ªÅn", "kh√¥i ph·ª•c quy·ªÅn t·ª± ch·ªß", "gi√†nh quy·ªÅn t·ª± ch·ªß"),
    ("l√™n ng√¥i", "x∆∞ng v∆∞∆°ng"),
    ("th·ªëng nh·∫•t", "d·∫πp lo·∫°n"),
]


INFORMATIVE_VERBS = [
    "ƒë√°nh", "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ti√™u di·ªát",
    "d·ª±ng", "l·∫≠p", "x∆∞ng", "l√™n ng√¥i", "ƒë·ªïi",
    "k√Ω", "ban", "c√¥ng b·ªë",
    "t·∫•n c√¥ng", "ph√≤ng th·ªß", "ch·∫∑n",
    "nh∆∞·ª£ng", "m·∫•t", "r∆°i v√†o",
    "th·ªëng nh·∫•t", "chia c·∫Øt",
]

STATE_VERBS = [
    "suy y·∫øu", "tan r√£", "th·∫•t b·∫°i",
    "kh·ªßng ho·∫£ng", "·ªïn ƒë·ªãnh",
    "ph√°t tri·ªÉn", "r∆°i v√†o", "b∆∞·ªõc v√†o"
]

STOPWORDS = {
    "di·ªÖn", "ra", "x·∫£y", "x·∫£y ra", "nƒÉm",
    "ƒë∆∞·ª£c", "b·ªã", "l√†", "v√†", "·ªü",
    "sau", "khi", "tr∆∞·ªõc", "trong",
    "t·∫°i", "v·ªõi", "do", "ƒë·ªÉ"
}

STOPWORDS |= {
    "ƒë√≥", "n√†y", "kia",
    "ƒë√£", "s·∫Ω", "c≈©ng",
    "nh·ªØng", "c√°c"
}

VIETNAMESE_SURNAMES = {
    "nguy·ªÖn", "l√™", "l√Ω", "tr·∫ßn", "ƒëinh", "ph·∫°m",
    "v≈©", "v√µ", "ho√†ng", "hu·ª≥nh", "ƒë·∫∑ng",
    "b√πi", "ƒë·ªó", "h·ªì", "ng√¥", "d∆∞∆°ng", "phan"
}

VIETNAMESE_SURNAMES |= {
    "kh√∫c",
    "m·∫°c",
    "ƒëinh",
    "h·ªì",
}


NON_PERSON_PHRASES = {
    "lam s∆°n", "t√¢y s∆°n",
    "ƒë·∫°i vi·ªát", "ƒë·∫°i nam", "ƒë·∫°i c·ªì vi·ªát",
    "nh√† l√Ω", "nh√† tr·∫ßn", "nh√† l√™", "nh√† nguy·ªÖn",
    "qu√¢n thanh", "qu√¢n minh",
    "b·∫°ch ƒë·∫±ng", "ng·ªçc h·ªìi", "ƒë·ªëng ƒëa",
    "kh·ªüi nghƒ©a", "kh√°ng chi·∫øn", "chi·∫øn d·ªãch",
}

NON_PERSON_TOKENS = {
    "s∆°n", "giang", "ƒë√¥", "th√†nh",
    "kh√°ng", "tr·∫≠n", "qu√¢n",
    "tri·ªÅu", "n∆∞·ªõc",
    "t·ªëng", "minh", "thanh",
    "m√¥ng", "c·ªï"
}

ACTION_HINTS = {
    "ƒë√°nh", "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ch·ªâ huy",
    "l√£nh ƒë·∫°o", "ti·∫øn c√¥ng", "ph·∫£n c√¥ng",
    "d·ª±ng", "x∆∞ng", "l√™n ng√¥i",
    "ban", "k√Ω", "t√¥n", "ch·ªß ƒë·ªông"
}

ACTION_HINTS |= {
    "nh∆∞·ªùng ng√¥i",
    "so·∫°n",
    "vi·∫øt",
    "ban",
    "d·ª±ng",
    "l·∫≠p",
    "ch·ªß tr√¨",
    "kh·ªüi x∆∞·ªõng",
    "l√£nh ƒë·∫°o",
    "ra ƒëi",
}

HEROIC_YEARS = {
    938, 981, 1009, 1010,
    1075, 1077,
    1258, 1285, 1288,
    1418, 1427,
    1471,
    1789,
    1930, 1945,
    1960, 1968, 1972, 1975
}

TRAGIC_YEARS = {1858, 1884, 1955}


HEROIC_YEARS |= {1954}

ENTITY_REGISTRY = {
    "person": set([
        "Nguy·ªÖn T·∫•t Th√†nh",
        "H·ªì Ch√≠ Minh",
        "H·ªì Qu√Ω Ly",
        "Nguy·ªÖn Hu·ªá",
        "Quang Trung",
        "Gia Long",
        "Nguy·ªÖn √Ånh",
        "L√™ L·ª£i",
        "Tr·∫ßn H∆∞ng ƒê·∫°o",
        "Ng√¥ Quy·ªÅn",
    ]),
    "place": {
        "B·∫°ch ƒê·∫±ng", "Chi LƒÉng", "ƒê·ªëng ƒêa", "ƒêi·ªán Bi√™n Ph·ªß", "H√† N·ªôi", 
        "ThƒÉng Long", "Ng·ªçc H·ªìi", "Lam S∆°n", "Ba ƒê√¨nh", "Hu·∫ø", "S√†i G√≤n",
        "Ngh·ªá An", "R·∫°ch G·∫ßm", "Xo√†i M√∫t", "V·∫°n Ki·∫øp", "H√†m T·ª≠", "Ch∆∞∆°ng D∆∞∆°ng",
        "Thanh H√≥a", "Ph√∫ Xu√¢n", "Gia ƒê·ªãnh", "ƒê·ªãnh T∆∞·ªùng", "Bi√™n H√≤a", "Vƒ©nh Long",
        "H√† Ti√™n", "Qu·∫£ng Tr·ªã", "Qu·∫£ng Nam", "ƒê√† N·∫µng", "L·∫°ng S∆°n", "Cao B·∫±ng",
        "T√¢y B·∫Øc"
    },
    "other": set([
        "ƒê·∫°i Vi·ªát",
        "Nh√† Tr·∫ßn",
        "Qu√¢n Thanh",
        "Kh·ªüi nghƒ©a Lam S∆°n",
        "T√¢y S∆°n",
        "ƒê·∫°i Ngu"
    ]) ,
    "collective": {
        "Qu√¢n Thanh", "Qu√¢n Minh", "Qu√¢n Nguy√™n", "Qu√¢n T·ªëng", "Qu√¢n Nam H√°n",
        "Nh√† Tr·∫ßn", "Nh√† L√Ω", "Nh√† L√™", "Nh√† Nguy·ªÖn", "Nh√† M·∫°c", "Nh√† H·ªì"
    }
}

ENTITY_LOOKUP: dict[str, str] = {}

for kind, names in ENTITY_REGISTRY.items():
    for n in names:
        ENTITY_LOOKUP[n] = kind


def classify_entity(name: str) -> str:
    if name in ENTITY_REGISTRY["place"]:
        return "place"
    if name in ENTITY_REGISTRY["collective"]:
        return "collective"
    if is_valid_person(name):
        return "person"
    return None

CANONICAL_PERSON = {
    "quang trung": "Nguy·ªÖn Hu·ªá",
    "b·∫Øc b√¨nh v∆∞∆°ng": "Nguy·ªÖn Hu·ªá",
    "gia long": "Nguy·ªÖn √Ånh",
    "l√Ω c√¥ng u·∫©n": "L√Ω Th√°i T·ªï",
    "vua l√Ω th√°i t·ªï": "L√Ω Th√°i T·ªï",
    "nguy·ªÖn t·∫•t th√†nh": "Nguy·ªÖn T·∫•t Th√†nh",
    "h·ªì ch√≠ minh": "H·ªì Ch√≠ Minh",
}

INVALID_PERSON_HINTS = {
    "qu√¢n", "nh√† n∆∞·ªõc", "m·∫∑t tr·∫≠n", "ƒë·∫£ng",
    "chi·∫øn d·ªãch", "t·∫øt", "c√°ch m·∫°ng",
    "hi·ªáp ƒë·ªãnh", "qu·ªëc hi·ªáu",
    "thƒÉng long", "ƒë√† n·∫µng", "s√†i g√≤n",
    "ƒë·∫°i vi·ªát", "ƒë·∫°i nam", "ƒë·∫°i c·ªì vi·ªát"
}

INVALID_PERSON_PREFIX = {
    "th·ªùi", "tri·ªÅu", "nh√†", "th·ªùi k·ª≥", "th·ªùi k√¨"
}

# Th√™m v√†o danh s√°ch ƒëen ho·∫∑c h·∫±ng s·ªë ·ªü ƒë·∫ßu file
COLLECTIVE_NOUNS = {
    "nh√¢n d√¢n", "qu√¢n ƒë·ªôi", "tri·ªÅu ƒë√¨nh", "gi·∫∑c", "ph√°t x√≠t", "th·ª±c d√¢n", "ƒë·∫ø qu·ªëc",
    "nghƒ©a qu√¢n", "qu√¢n d√¢n", "qu√¢n t·ªëng", "qu√¢n nam h√°n", "qu√¢n minh", "qu√¢n nguy√™n",
    "chi·∫øn d·ªãch", "kh·ªüi nghƒ©a", "phong tr√†o", "hi·ªáp ƒë·ªãnh", "lu·∫≠t", "hi·∫øn ph√°p", "b·ªô h√¨nh th∆∞"
}

# Th√™m v√†o ph·∫ßn ƒë·∫ßu file storyteller.py
COLLECTIVE_DENY = {
    "nh√¢n d√¢n", "qu√¢n ƒë·ªôi", "tri·ªÅu ƒë√¨nh", "gi·∫∑c", "ph√°t x√≠t", "th·ª±c d√¢n", "ƒë·∫ø qu·ªëc",
    "nghƒ©a qu√¢n", "qu√¢n d√¢n", "qu√¢n t·ªëng", "qu√¢n nam h√°n", "qu√¢n minh", "qu√¢n nguy√™n",
    "qu√¢n thanh", "nh√† tr·∫ßn", "nh√† l√Ω", "nh√† l√™", "nh√† nguy·ªÖn", "ch√≠nh ph·ªß", "qu·ªëc h·ªôi",
    "tri·ªÅu nguy·ªÖn", "tri·ªÅu l√™", "tri·ªÅu l√Ω", "tri·ªÅu tr·∫ßn", "tri·ªÅu ƒë√¨nh hu·∫ø"
}

EVENT_WORKS_DENY = {
    "chi·∫øn d·ªãch", "kh·ªüi nghƒ©a", "phong tr√†o", "hi·ªáp ƒë·ªãnh", "lu·∫≠t", "hi·∫øn ph√°p",
    "b√¨nh ng√¥ ƒë·∫°i c√°o", "h·ªãch t∆∞·ªõng sƒ©", "tuy√™n ng√¥n ƒë·ªôc l·∫≠p", "nh·∫≠t k√Ω trong t√π", 
    "h√¨nh th∆∞", "ng√¥ ƒë·∫°i c√°o", "ƒë·∫°i c√°o"
}

PLACE_DENY = {
    "b·∫°ch ƒë·∫±ng", "chi lƒÉng", "ƒë·ªëng ƒëa", "ƒëi·ªán bi√™n ph·ªß", "thƒÉng long", "h√† n·ªôi", "ba ƒë√¨nh", "vi·ªát nam",
    "thanh h√≥a", "ph√∫ xu√¢n", "gia ƒë·ªãnh", "ƒë·ªãnh t∆∞·ªùng", "bi√™n h√≤a", "vƒ©nh long", "h√† ti√™n",
    "qu·∫£ng tr·ªã", "qu·∫£ng nam", "ƒë√† n·∫µng", "l·∫°ng s∆°n", "cao b·∫±ng", "ngh·ªá an", "r·∫°ch g·∫ßm", "xo√†i m√∫t"
}

# G·ªôp danh s√°ch ch·∫∑n ƒë·ªÉ tra c·ª©u nhanh trong is_valid_person
GLOBAL_PERSON_DENY = (
    COLLECTIVE_DENY | EVENT_WORKS_DENY | PLACE_DENY |
    {
        "nh√¢n d√¢n vi·ªát nam", "qu√¢n ƒë·ªôi nh√¢n d√¢n", "tri·ªÅu ƒë√¨nh nh√† l√™",
        "chi·∫øn d·ªãch l·ªãch s·ª≠", "kh·ªüi nghƒ©a ba ƒë√¨nh", "ph√°t x√≠t nh·∫≠t",
        "th·ª±c d√¢n ph√°p", "ƒë·∫ø qu·ªëc m·ªπ", "gi·∫∑c t·ªëng", "thƒÉng long", "h√† n·ªôi",
        "vi·ªát nam", "ƒë·∫°i vi·ªát", "ƒë·∫°i nam", "ƒë·∫°i c·ªì vi·ªát", "l·ªãch s·ª≠"
    }
)

def is_valid_person(name: str) -> bool:
    if not name: return False
    name_stripped = name.strip()
    if len(name_stripped) < 4: return False
    
    name_low = name_stripped.lower()
    
    # 1. Ki·ªÉm tra Registry ƒë·ªÉ tr√°nh nh·∫ßm Place/Collective th√†nh Person
    if name_stripped in ENTITY_REGISTRY["place"] or name_stripped in ENTITY_REGISTRY["collective"]:
        return False

    # 2. Ch·∫∑n theo danh s√°ch GLOBAL_PERSON_DENY (Substring check)
    if any(deny in name_low for deny in GLOBAL_PERSON_DENY):
        return False

    # 3. Ch·∫∑n theo ti·ªÅn t·ªë v√† h·∫≠u t·ªë (Suffix check quan tr·ªçng cho "M·∫°c tri·ªÅu", "T√¢y S∆°n qu√¢n")
    collective_prefixes = ("nh√† ", "tri·ªÅu ", "qu√¢n ", "nghƒ©a qu√¢n ", "ƒë·ªôi ", "ƒë·∫£ng ", "m·∫∑t tr·∫≠n ")
    collective_suffixes = (" tri·ªÅu", " qu√¢n", " t·ªôc")
    
    if name_low.startswith(collective_prefixes) or name_low.endswith(collective_suffixes):
        return False

    # 4. Ch·∫∑n t·ª´ kh√≥a s·ª± v·∫≠t/s·ª± ki·ªán
    artifact_keywords = {"tuy√™n ng√¥n", "hi·ªáp ƒë·ªãnh", "chi·∫øn d·ªãch", "tr·∫≠n", "ƒë·∫°i ph√°", "kh·ªüi nghƒ©a", "b·∫£n ƒë·ªì", "t√°c ph·∫©m"}
    if any(k in name_low for k in artifact_keywords):
        return False
    
    # 5. Ki·ªÉm tra s·ªë t·ª´ (T√™n ng∆∞·ªùi Vi·ªát: 2-5 t·ª´)
    words = name_low.split()
    if len(words) < 2 or len(words) > 5:
        return False
        
    return True

def normalize_persons(persons: list[str]) -> list[str]:
    result = set()

    for p in persons:
        p2 = canonical_person(p)
        if not is_valid_person(p2):
            continue

        kind = classify_entity(p2)
        if kind and kind != "person":
            continue

        result.add(p2)

    return sorted(result)


def strip_evaluation(text: str) -> str:
    return re.sub(
        r",?\s*(m·ªü ra|kh·∫≥ng ƒë·ªãnh|ƒë√°nh d·∫•u|th·ªÉ hi·ªán)[^,.]*",
        "",
        text,
        flags=re.I
    ).strip(" ,.")

def canonical_person(name: str) -> str:
    if not name: return ""
    
    name_stripped = name.strip()
    
    # 1. Th·ª≠ t√¨m tr·ª±c ti·∫øp trong PERSON_ALIAS (Quan tr·ªçng cho t∆∞·ªõc hi·ªáu ƒë·ª©ng ƒë·ªôc l·∫≠p ho·∫∑c b√≠ danh ƒë·∫∑c bi·ªát)
    # C·∫ßn ƒë·∫£m b·∫£o PERSON_ALIAS c√≥: "B√°c H·ªì": "H·ªì Ch√≠ Minh", "H∆∞ng ƒê·∫°o V∆∞∆°ng": "Tr·∫ßn H∆∞ng ƒê·∫°o"...
    if name_stripped in PERSON_ALIAS:
        return PERSON_ALIAS[name_stripped]
    
    # 2. Danh s√°ch t∆∞·ªõc hi·ªáu c·∫ßn b√≥c t√°ch (S·∫Øp x·∫øp t·ª´ d√†i ƒë·∫øn ng·∫Øn ƒë·ªÉ tr√°nh kh·ªõp nh·∫ßm)
    titles = [
        "H∆∞ng ƒê·∫°o ƒê·∫°i V∆∞∆°ng", "H∆∞ng ƒê·∫°o V∆∞∆°ng", "B·∫Øc B√¨nh V∆∞∆°ng", 
        "Th√°i th∆∞·ª£ng ho√†ng", "Trung t∆∞·ªõng", "ƒê·∫°i t∆∞·ªõng", "Th√°i s∆∞", 
        "Th√°i t·ªï", "Thanh t√¥ng", "Th√°nh t√¥ng", "Nh√¢n t√¥ng", "Vua", "Ch√∫a"
    ]
    
    clean_name = name_stripped
    for t in titles:
        # N·∫øu t√™n b·∫Øt ƒë·∫ßu b·∫±ng t∆∞·ªõc hi·ªáu v√† c√≤n ph·∫ßn t√™n ph√≠a sau
        if name_stripped.startswith(t) and len(name_stripped) > len(t):
            temp_name = name_stripped[len(t):].strip()
            # N·∫øu ph·∫ßn c√≤n l·∫°i c√≥ trong Alias (v√≠ d·ª•: "Qu·ªëc Tu·∫•n" -> "Tr·∫ßn H∆∞ng ƒê·∫°o")
            clean_name = PERSON_ALIAS.get(temp_name, temp_name)
            break
            
    return PERSON_ALIAS.get(clean_name, clean_name)

def extract_parenthetical_persons(text: str):
    persons = []

    def repl(m):
        content = m.group(1).strip()
        if is_valid_person(content):
            persons.append(canonical_person(content))
            return content  # ‚¨ÖÔ∏è GI·ªÆ L·∫†I
        return ""

    clean_text = re.sub(r"\(([^()]{2,50})\)", repl, text)
    return clean_text.strip(), persons

def is_person_actor(text: str, person: str) -> bool:
    """
    PERSON l√† actor n·∫øu:
    - PERSON ƒë·ª©ng g·∫ßn ƒë·ªông t·ª´ h√†nh ƒë·ªông (tr∆∞·ªõc ho·∫∑c sau)
    - ho·∫∑c PERSON l√† alias c·ªßa nh√¢n v·∫≠t th·ª±c hi·ªán h√†nh ƒë·ªông
    """
    t = text.lower()
    p = person.lower()

    # t·∫≠p alias: Quang Trung ‚Üî Nguy·ªÖn Hu·ªá
    aliases = {p}
    for k, v in PERSON_ALIASES.items():
        if v.lower() == p:
            aliases.add(k)

    ACTIONS = [
        "ƒë√°nh", "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ti·∫øn c√¥ng",
        "ch·ªß ƒë·ªông", "d√πng", "nh·ª≠",
        "l√™n ng√¥i", "x∆∞ng v∆∞∆°ng",
        "d·ª±ng", "l·∫≠p", "ban",
        "so·∫°n", "vi·∫øt",
        "ra ƒëi", "kh·ªüi x∆∞·ªõng",
        "l√£nh ƒë·∫°o", "ch·ªâ huy",
        "ƒë·∫°i ph√°", "ti√™u di·ªát", "gi·∫£i ph√≥ng"
    ]

    for name in aliases:
        for act in ACTIONS:
            # PERSON tr∆∞·ªõc ho·∫∑c sau verb (¬±40 k√Ω t·ª±)
            if re.search(rf"{name}.{{0,40}}{act}", t):
                return True
            if re.search(rf"{act}.{{0,40}}{name}", t):
                return True

    return False

def is_political_actor(text: str, person: str) -> bool:
    t = text.lower()
    p = person.lower()

    for k in [
        "l√™n ng√¥i",
        "nh∆∞·ªùng ng√¥i",
        "ban chi·∫øu",
        "x∆∞ng v∆∞∆°ng",
        "tr·ªã v√¨",
        "ƒë·ªïi qu·ªëc hi·ªáu",
        "l·∫≠p nh√†",
        "d·ª±ng ch√≠nh quy·ªÅn",
        "ra ƒëi",
    ]:
        if re.search(rf"{p}.{{0,40}}{k}|{k}.{{0,40}}{p}", t):
            return True

    return False

def extract_persons_from_body(text: str) -> set[str]:
    all_persons = set(cached_extract_all_persons(text))
    subjects = set()

    # üëë vua ‚Üí lu√¥n l√† subject
    kings = {
        p for p in all_persons
        if re.search(r"(th√°i\s+(t·ªï|t√¥ng)|th√°nh\s+t√¥ng|nh√¢n\s+t√¥ng)$", p.lower())
    }
    if kings:
        return {canonical_person(k) for k in kings}

    for p in all_persons:
        if is_person_actor(text, p) or is_political_actor(text, p):
            subjects.add(canonical_person(p))

    return subjects

def clean_text(text):
    """L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n l·ªãch s·ª≠."""
    if not text: return None
    
    # Lo·∫°i b·ªè junk patterns
    for p in JUNK_PATTERNS:
        text = re.sub(p, "", text, flags=re.I)

    # Chu·∫©n h√≥a ng√†y th√°ng ƒë·∫∑c bi·ªát
    text = re.sub(r"2/9/?\s*1945", "ng√†y 2 th√°ng 9 nƒÉm 1945", text)
    
    # X·ª≠ l√Ω d·∫•u c√¢u
    text = re.sub(r"[;:]", ".", text)
    text = re.sub(r"\s+", " ", text)
    
    # Lo·∫°i b·ªè m·ªëc th·ªùi gian th·ª´a ·ªü ƒë·∫ßu c√¢u
    text = re.sub(r"^(V√†o\s+)?nƒÉm\s+[0-9]{3,4}[,:]?\s*", "", text, flags=re.I)
    
    final = text.strip(" ,.-")
    
    # M·ªü r·ªông danh s√°ch h√†nh ƒë·ªông c·ªët l√µi ƒë·ªÉ gi·ªØ l·∫°i c√°c s·ª± ki·ªán nh∆∞ 'gi·∫£i ph√≥ng'
    core_actions = {
        "l√™n ng√¥i", "x∆∞ng v∆∞∆°ng", "d·ªùi ƒë√¥", "th√†nh l·∫≠p", "ƒë√°nh b·∫°i", 
        "k√Ω", "ban h√†nh", "gi·∫£i ph√≥ng", "kh·ªüi nghƒ©a", "ƒë·∫°i ph√°",
        "chi·∫øn th·∫Øng", "th·∫Øng l·ª£i", "tuy√™n ng√¥n"
    }
    is_important = any(act in final.lower() for act in core_actions)
    
    # N·∫øu c√¢u qu√° ng·∫Øn v√† kh√¥ng ch·ª©a h√†nh ƒë·ªông quan tr·ªçng -> Lo·∫°i
    if len(final) < 15 and not is_important:
        return None
        
    return final

CORE_ACTIONS = [
    "l√™n ng√¥i", "t√¥n l·∫≠p", "x∆∞ng v∆∞∆°ng",
    "ban chi·∫øu", "k√Ω hi·ªáp ƒë·ªãnh",
    "ƒë√°nh b·∫°i", "ƒë√°nh tan", "gi√†nh th·∫Øng l·ª£i",
    "kh·ªüi nghƒ©a", "kh√°ng chi·∫øn",
    "th√†nh l·∫≠p", "ƒë·ªïi qu·ªëc hi·ªáu",
    "gi·∫£i ph√≥ng", "th·ªëng nh·∫•t"
]

def choose_representative_event(events: list[str]) -> str:
    def score(e: str):
        s = 0
        if any(k in e.lower() for k in ["m·ªü ra", "ch·∫•m d·ª©t", "kh·∫≥ng ƒë·ªãnh"]):
            s += 2
        if any(k in e.lower() for k in CORE_ACTIONS):
            s += 2
        s += len(e) / 100
        return s

    return max(events, key=score)

def extract_all_places(text: str) -> set[str]:
    places = set()
    if not text:
        return places

    # 1. Ki·ªÉm tra Registry
    for p in ENTITY_REGISTRY["place"]:
        if p in text:
            places.add(p)

    # 2. Regex cho c√°c th·ª±c th·ªÉ ƒë·ªãa l√Ω ph·ªï bi·∫øn
    geo_pattern = re.compile(
        r"\b(?:t·ªânh|th√†nh ph·ªë|th√†nh|huy·ªán|ƒë·∫£o|qu·∫ßn ƒë·∫£o|s√¥ng|n√∫i|v√πng n√∫i|ƒë√®o|c·ª≠a|v·ªãnh|bi·ªÉn|v√πng|ƒë·∫•t|mi·ªÅn|kinh ƒë√¥|ph·ªß|l√†ng|x√£|qu·∫≠n)\s+"
        r"([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+){0,4})",
        re.I
    )

    for m in geo_pattern.finditer(text):
        p = m.group(1).strip()
        # ƒê·∫£m b·∫£o c√°c t·ª´ trong t√™n ƒë·ªãa danh ƒë·ªÅu vi·∫øt hoa (ch·ªëng b·∫Øt nh·∫ßm "B·∫°ch ƒê·∫±ng nƒÉm")
        words = p.split()
        if words and all(w[0].isupper() for w in words):
            if len(p) > 2 and p.lower() not in STOPWORDS:
                places.add(p)

    return places

def extract_all_persons(text: str) -> set[str]:
    persons: set[str] = set()

    if not text:
        return persons

    for m in PERSON_PATTERN.finditer(text):
        raw = m.group(1).strip()
        p = canonical_person(raw)

        # validate h√¨nh th·ª©c ng∆∞·ªùi
        if not is_valid_person(p):
            continue

        # lo·∫°i n·∫øu entity registry n√≥i KH√îNG ph·∫£i ng∆∞·ªùi
        kind = classify_entity(p)
        if kind and kind != "person":
            continue

        persons.add(p)

    return persons

def resolve_entity(name: str):
    kind = classify_entity(name)
    if kind:
        return kind

    UNKNOWN_ENTITIES.add(name)
    return None


def pick_tone(tones):
    """
    Ch·ªçn tone ƒë·∫°i di·ªán ƒë·ªÉ k·ªÉ chuy·ªán.
    ∆Øu ti√™n: heroic > tragic > neutral
    """
    if not tones:
        return "neutral"

    if isinstance(tones, (list, set)):
        if "heroic" in tones:
            return "heroic"
        if "tragic" in tones:
            return "tragic"
        return next(iter(tones))

    return tones


def ask_by_person(timeline, name: str):
    if not name:
        return None

    name = canonical_person(name)
    results = []

    for year, block in timeline.items():
        for e in block.get("events", []):
            persons_all = e.get("persons_all") or []

            for p in persons_all:
                if isinstance(p, str) and p.lower() == name.lower():
                    subject = infer_subject(
                        e["event"],
                        set(e.get("persons", [])),
                        e.get("nature", [])
                    )
                    results.append(
                        storyteller(
                            int(year),
                            pick_tone(e.get("tone", [])),
                            e["event"],
                            subject
                        )
                    )

    return results or None

def remove_non_informative_clauses(text):
    clauses = [c.strip() for c in text.split(",") if c.strip()]
    kept = []

    for c in clauses:
        # ‚úÖ c√≥ PERSON ‚Üí gi·ªØ
        set(cached_extract_all_persons(c))


        lc = c.lower()

        if any(v in lc for v in INFORMATIVE_VERBS + STATE_VERBS):
            kept.append(c)
            continue

        if re.search(
            r"(qu√¢n|ƒë√¥|kinh|s√¥ng|th√†nh|hi·ªáp ƒë·ªãnh|qu·ªëc hi·ªáu|b√†i|t√°c ph·∫©m|c·∫£i c√°ch)",
            lc
        ):
            kept.append(c)

    return ", ".join(kept)

def remove_year_phrases(text, year):
    text = re.sub(
        rf"(di·ªÖn ra|x·∫£y ra)(?:\s+(?:v√†o|trong))?\s+nƒÉm\s+{year}",
        "",
        text,
        flags=re.I
    )
    return re.sub(r"\s+,", ",", text).strip(" ,.")

def force_person_from_text(event_text: str) -> list[str]:
    forced = []

    FORCE_MAP = {
        "Nguy·ªÖn T·∫•t Th√†nh": "Nguy·ªÖn T·∫•t Th√†nh",
        "H·ªì Ch√≠ Minh": "H·ªì Ch√≠ Minh",
        "Quang Trung": "Nguy·ªÖn Hu·ªá",
        "Nguy·ªÖn Hu·ªá": "Nguy·ªÖn Hu·ªá",
        "Gia Long": "Nguy·ªÖn √Ånh",
    }

    for k, v in FORCE_MAP.items():
        if k in event_text:
            forced.append(v)

    return forced


def merge_events_by_year(events: list[dict]) -> list[dict]:
    """
    Merge c√°c s·ª± ki·ªán TR√ôNG N·ªòI DUNG trong c√πng m·ªôt nƒÉm
    """
    if not events:
        return []

    buckets: dict[str, list[dict]] = defaultdict(list)

    # 1Ô∏è‚É£ Bucket theo ch·ªØ k√Ω n·ªôi dung
    for e in events:
        sig = event_signature(e["event"])
        buckets[sig].append(e)

    merged_events: list[dict] = []

    # 2Ô∏è‚É£ Merge t·ª´ng bucket
    for bucket in buckets.values():
        base = {
            "year": bucket[0]["year"],
            "event": choose_representative_event(
                [b["event"] for b in bucket]
            ),
            "persons": sorted(
                set(p for b in bucket for p in b.get("persons", []))
            ),
            "persons_all": sorted(
                set(p for b in bucket for p in b.get("persons_all", []))
            ),
            "places": sorted(
                set(p for b in bucket for p in b.get("places", []))
            ),
            "nature": sorted(
                set(n for b in bucket for n in b.get("nature", []))
            ),
            "tone": sorted(
                set(t for b in bucket for t in b.get("tone", []))
            ),
            "keywords": sorted(
                set(k for b in bucket for k in b.get("keywords", []))
            ),
        }

        merged_events.append(base)

    return merged_events

def build_year_summary(events):
    tones = set(t for e in events for t in e["tone"])
    natures = set(n for e in events for n in e["nature"])

    if "heroic" in tones and "tragic" in tones:
        return "M·ªôt nƒÉm mang t√≠nh b∆∞·ªõc ngo·∫∑t, v·ª´a ghi d·∫•u th·∫Øng l·ª£i l·ªõn v·ª´a ƒë·ªÉ l·∫°i h·ªá qu·∫£ l·ªãch s·ª≠ s√¢u s·∫Øc."

    if "heroic" in tones:
        if "military" in natures:
            return "M·ªôt nƒÉm ghi d·∫•u th·∫Øng l·ª£i qu√¢n s·ª± quan tr·ªçng c·ªßa d√¢n t·ªôc."
        return "M·ªôt nƒÉm ƒë√°nh d·∫•u b∆∞·ªõc ti·∫øn l·ªõn trong ti·∫øn tr√¨nh l·ªãch s·ª≠ d√¢n t·ªôc."


    if "tragic" in tones:
        return "M·ªôt nƒÉm ƒë·∫ßy bi·∫øn c·ªë, ƒë·ªÉ l·∫°i nh·ªØng t·ªïn th·∫•t v√† chia c·∫Øt l·ªãch s·ª≠."

    return "M·ªôt nƒÉm c√≥ nh·ªØng chuy·ªÉn bi·∫øn quan tr·ªçng trong ti·∫øn tr√¨nh l·ªãch s·ª≠."

def extract_year(text: str):
    # ∆Øu ti√™n ƒë·ªãnh d·∫°ng ng√†y/th√°ng/nƒÉm
    if m := DATE_WITH_YEAR.search(text):
        return m.group(2)
    
    # T√¨m t·∫•t c·∫£ c√°c s·ªë c√≥ 3-4 ch·ªØ s·ªë
    matches = YEAR_ANY.findall(text)
    for val in matches:
        year_int = int(val)
        # Gi·ªõi h·∫°n nƒÉm l·ªãch s·ª≠ h·ª£p l√Ω ƒë·ªÉ tr√°nh b·∫Øt nh·∫ßm s·ªë l∆∞·ª£ng qu√¢n nhu/ng∆∞·ªùi
        if 40 <= year_int <= 2025: 
            return val
            
    return None

def purge(text):
    text = re.sub(r"\s+\.", ".", text)
    text = re.sub(r"\.+", ".", text)
    return text.strip(" .,-")

def classify_tone(text: str, year: str | None = None) -> set[str]:
    t = text.lower()
    tones = set()

    # Nh√≥m H√†o h√πng (Heroic) - Th√™m c√°c t·ª´ kh√≥a t·ª´ test case
    heroic_keywords = [
        "chi·∫øn th·∫Øng", "l·ª´ng l·∫´y", "ch·∫•n ƒë·ªông",
        "ƒë√°nh b·∫°i", "ƒë√°nh tan", "ƒë·∫©y lui", "to√†n th·∫Øng", "gi·∫£i ph√≥ng",
        "th·ªëng nh·∫•t", "gi√†nh ƒë·ªôc l·∫≠p", "t·ª± ch·ªß", "ch·∫•m d·ª©t √°ch",
        "vang d·ªôi", "h√†o kh√≠", "oanh li·ªát", "th·∫Øng l·ª£i", "ƒë·∫°i ph√°"
    ]
    
    # Nh√≥m Bi th∆∞∆°ng/Tr·∫ßm l·∫Øng (Somber/Tragic)
    tragic_keywords = [
        "t√†n ph√°", "ƒëi√™u linh", "t·ªïn th·∫•t", "ƒëau ƒë·ªõn",
        "b·ªã x√¢m l∆∞·ª£c", "m·∫•t n∆∞·ªõc", "b·∫Øc thu·ªôc", "minh thu·ªôc",
        "chia c·∫Øt", "√°p ƒë·∫∑t", "l·∫ßm than", "ƒëau th∆∞∆°ng", "m·∫•t m√°t",
        "hy sinh", "kh√≥ khƒÉn", "th·∫•t b·∫°i", "chi·∫øm ƒë√≥ng"
    ]

    if any(k in t for k in heroic_keywords):
        tones.add("heroic")

    if any(k in t for k in tragic_keywords):
        tones.add("somber") # S·ª≠ d·ª•ng 'somber' th·ªëng nh·∫•t v·ªõi test case

    # T∆∞∆°ng th√≠ch ng∆∞·ª£c v·ªõi nh√£n 'tragic' n·∫øu b·∫°n v·∫´n mu·ªën d√πng
    if "somber" in tones:
        tones.add("tragic")

    # √Ånh x·∫° theo nƒÉm l·ªãch s·ª≠ ƒë·∫∑c bi·ªát (n·∫øu c√≥ ƒë·ªãnh nghƒ©a HEROIC_YEARS...)
    if year:
        try:
            y = int(year)
            if 'HEROIC_YEARS' in globals() and y in HEROIC_YEARS:
                tones.add("heroic")
            if 'TRAGIC_YEARS' in globals() and y in TRAGIC_YEARS:
                tones.add("somber")
        except ValueError:
            pass

    return tones if tones else {"neutral"}

def classify_nature(text: str) -> list[str]:
    text_low = text.lower()
    labels = []
    
    # Nh√≥m qu√¢n s·ª±
    mil_keywords = ["ƒë√°nh b·∫°i", "ƒë·∫°i ph√°", "chi·∫øn th·∫Øng", "ƒë·∫≠p tan", "chi·∫øn d·ªãch", "gi·∫£i ph√≥ng", "v√πng l√™n", "th·∫Øng l·ª£i"]
    # Nh√≥m th·ªÉ ch·∫ø / ch√≠nh tr·ªã
    inst_keywords = ["ban h√†nh", "lu·∫≠t", "h√¨nh th∆∞", "hi·∫øn ph√°p", "k√Ω k·∫øt", "hi·ªáp ƒë·ªãnh", "d·ªùi ƒë√¥", "gi√†nh ch√≠nh quy·ªÅn", "tuy√™n ng√¥n", "chi·∫øu"]
    # Nh√≥m s·ª± ki·ªán chung
    event_keywords = ["th√†nh l·∫≠p", "l√™n ng√¥i", "x∆∞ng v∆∞∆°ng", "kh·ªüi nghƒ©a", "ƒë·ªïi t√™n", "th√†nh ph·ªë", "d·ªùi ƒë√¥"]

    if any(k in text_low for k in mil_keywords):
        labels.append("military")
        labels.append("historical_event")
    
    if any(k in text_low for k in inst_keywords):
        labels.append("institutional")
        labels.append("historical_event")
        
    if any(k in text_low for k in event_keywords):
        labels.append("historical_event")

    if not labels:
        labels.append("general")
        
    return list(set(labels))

def normalize(text: str):
    """Chu·∫©n h√≥a v√† ph√¢n lo·∫°i th√¥ng tin s·ª± ki·ªán l·ªãch s·ª≠."""
    # 1. Tr√≠ch xu·∫•t nƒÉm
    year = extract_year(text)
    if not year: return None
    
    if text.strip().endswith("?"): return None

    # 2. L√†m s·∫°ch body (C·∫©n th·∫≠n: clean_text c√≥ th·ªÉ x√≥a m·∫•t nƒÉm l√†m keep = False)
    body = clean_text(text)
    if not body or len(body.split()) < 3: return None

    # 3. L·ªçc b·∫´y n·ªôi dung m∆° h·ªì
    vague_keywords = {"c√≥ m∆∞a", "vui v·∫ª", "ph·ª©c t·∫°p", "b√¨nh th∆∞·ªùng", "ƒë·∫πp", "l√† m·ªôt v√πng ƒë·∫•t"}
    if any(vk in body.lower() for vk in vague_keywords):
        return None

    # 4. Tr√≠ch xu·∫•t th·ª±c th·ªÉ
    all_extracted = extract_all_persons(body)
    persons_valid = {p for p in all_extracted if is_valid_person(p)}
    subjects = extract_persons_from_body(body)
    places = extract_all_places(body)
    
    # 5. Logic GI·ªÆ L·∫†I (S·ª≠a l·ªói "Nh√¢n d√¢n v√πng l√™n")
    keep = False
    body_low = body.lower()
    
    # A. C√≥ nh√¢n v·∫≠t h·ª£p l·ªá
    if persons_valid: 
        keep = True
    
    # B. C√≥ h√†nh ƒë·ªông l·ªãch s·ª≠ m·∫°nh (D√π kh√¥ng c√≥ t√™n ng∆∞·ªùi c·ª• th·ªÉ)
    # Th√™m "v√πng l√™n", "gi√†nh ƒë·ªôc l·∫≠p" ƒë·ªÉ pass test_normalize_keeps_collective_with_strong_action
    core_historical_actions = {
        "ti√™u di·ªát", "d·ªùi ƒë√¥", "l√™n ng√¥i", "x∆∞ng v∆∞∆°ng", "ƒë√°nh b·∫°i", "ƒë√°nh tan",
        "gi·∫£i ph√≥ng", "tuy√™n ng√¥n", "hi·ªáp ƒë·ªãnh", "chi·∫øn th·∫Øng", "th·∫Øng l·ª£i",
        "th√†nh l·∫≠p", "ban h√†nh", "kh·ªüi nghƒ©a", "ƒë·∫°i ph√°", "v√πng l√™n", "gi√†nh ƒë·ªôc l·∫≠p"
    }
    if any(act in body_low for act in core_historical_actions):
        keep = True

    # C. Ch·ª©a ƒë·ªãa danh/t·∫≠p th·ªÉ quan tr·ªçng ƒëang c√≥ nature ch√≠nh tr·ªã/qu√¢n s·ª±
    nature = classify_nature(body)
    important_anchors = {
        "thƒÉng long", "nh√† tr·∫ßn", "nh√† l√™", "nh√† l√Ω", "nh√¢n d√¢n",
        "b·∫°ch ƒë·∫±ng", "ƒëi·ªán bi√™n ph·ªß", "ng·ªçc h·ªìi", "ƒë·ªëng ƒëa"
    }
    if any(anchor in body_low for anchor in important_anchors):
        if any(n in nature for n in ["military", "institutional", "historical_event"]):
            keep = True
    
    if not keep: return None

    # 6. Ph√¢n lo·∫°i Tone
    tone = classify_tone(body, year)
    
    return (
        str(year),
        body,
        list(nature),
        list(tone),
        set(subjects),
        set(persons_valid),
        set(places)
    )

HEROIC_ENDINGS = [
    "S·ª± ki·ªán n√†y m·ªü ra m·ªôt ch∆∞∆°ng s·ª≠ h√†o h√πng c·ªßa d√¢n t·ªôc.",
    "Chi·∫øn c√¥ng ·∫•y kh·∫≥ng ƒë·ªãnh √Ω ch√≠ t·ª± ch·ªß v√† s·ª©c s·ªëng b·ªÅn b·ªâ c·ªßa ng∆∞·ªùi Vi·ªát.",
    "ƒê√¢y l√† d·∫•u m·ªëc th·ªÉ hi·ªán b·∫£n lƒ©nh v√† kh√°t v·ªçng l√†m ch·ªß v·∫≠n m·ªánh d√¢n t·ªôc.",
]

TRAGIC_ENDINGS = [
    "ƒê√≥ l√† giai ƒëo·∫°n bi th∆∞∆°ng, khi ƒë·∫•t n∆∞·ªõc r∆°i v√†o th·ª≠ th√°ch kh·∫Øc nghi·ªát.",
    "Bi·∫øn c·ªë n√†y ƒë·ªÉ l·∫°i nh·ªØng m·∫•t m√°t s√¢u s·∫Øc cho v·∫≠n m·ªánh d√¢n t·ªôc.",
    "Th·ªùi k·ª≥ ·∫•y ghi d·∫•u n·ªói ƒëau v√† nh·ªØng t·ªïn th·∫•t n·∫∑ng n·ªÅ c·ªßa ƒë·∫•t n∆∞·ªõc.",
]

def storyteller(year, kind, content, subject=None):
    content = content.rstrip(".")

    if subject:
        content = subject + " " + content[0].lower() + content[1:]

    if kind == "heroic":
        return (
            f"NƒÉm {year}, {content}. "
            f"{random.choice(HEROIC_ENDINGS)}"
        )

    if kind == "tragic":
        return (
            f"NƒÉm {year}, {content}. "
            f"{random.choice(TRAGIC_ENDINGS)}"
        )

    return f"NƒÉm {year}, {content}."


def collapse_year_events(events: list[dict]) -> list[dict]:
    groups = {}

    for e in events:
        sig = event_signature(e["event"])
        groups.setdefault(sig, []).append(e)

    collapsed = []

    for sig, group in groups.items():
        best = max(
            group,
            key=lambda x: (
                len(x["event"]),
                "r·ªùi b·∫øn" in x["event"],
                "ban" in x["event"],
                "k√Ω" in x["event"],
                "tuy√™n" in x["event"]
            )
        )
        all_nature = sorted(set(n for g in group for n in g["nature"]))
        all_tone = sorted(set(t for g in group for t in g["tone"]))

        collapsed.append({
            "year": best["year"],
            "event": best["event"],
            "nature": all_nature,
            "tone": all_tone
        })


    return collapsed


def deduplicate_phrases(text):
    parts = re.split(r"[.]", text)
    seen = set()
    result = []

    for p in parts:
        key = re.sub(r"\W+", "", p.lower())
        if key and key not in seen:
            seen.add(key)
            result.append(p.strip())

    return ". ".join(result).strip()

def extract_core_tokens(text: str) -> set:
    cores = set()
    for a in CORE_ACTIONS:
        if a in text.lower():
            cores.add(a)

    names = re.findall(
        r"[a-z√†-·ªπ]+(?:\s+[a-z√†-·ªπ]+){1,3}", text.lower()
    )
    cores.update(names[:2])
    return cores

def collapse_fragments(text):
    return re.sub(r"\.\s+([a-z√†-·ªπ])", r", \1", text, flags=re.I)

def remove_repeated_subject(text):
    parts = re.split(r"\.\s+", text)
    if len(parts) < 2:
        return text

    first = parts[0]
    names = re.findall(
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+)+", first
    )

    for i in range(1, len(parts)):
        for name in names:
            parts[i] = re.sub(rf"^{name}\s*", "", parts[i])

    return ". ".join(parts).strip()

def remove_redundant_actions(text):
    clauses = [c.strip() for c in re.split(r",", text)]
    kept = []
    used_groups = set()

    for clause in clauses:
        lowered = clause.lower()
        matched_group = None

        for i, group in enumerate(ACTION_GROUPS):
            if any(k in lowered for k in group):
                matched_group = i
                break

        if matched_group is not None:
            if matched_group in used_groups:
                continue
            used_groups.add(matched_group)

        kept.append(clause)

    return ", ".join(kept)

def normalize_event_text(text: str) -> set:
    text = text.lower()

    text = re.sub(r"\b(1[0-9]{3})\b", "", text)
    text = re.sub(
        r"(di·ªÖn ra|x·∫£y ra|nƒÉm|sau khi|ƒë∆∞·ª£c|v√†o|ƒë√£|c√°c)",
        "",
        text
    )
    text = re.sub(r"[^\w\s]", " ", text)

    words = [
        w for w in text.split()
        if w not in STOPWORDS and len(w) > 3
    ]

    return set(words)

def normalize_temporal_clause(text: str) -> str:
    return re.sub(
        r",?\s*Sau khi [^,]+?(?=,|$)",
        "",
        text,
        flags=re.I
    )




def is_same_event(e1: str, e2: str, threshold=0.3) -> bool:
    # 1. So core (ch·ªß th·ªÉ + h√†nh ƒë·ªông)
    c1 = extract_core_tokens(e1)
    c2 = extract_core_tokens(e2)

    if c1 and c2 and len(c1 & c2) >= 2:
        return True


    # 2. Fallback bag-of-words
    s1 = normalize_event_text(e1)
    s2 = normalize_event_text(e2)

    if not s1 or not s2:
        return False

    overlap = len(s1 & s2)
    score = overlap / min(len(s1), len(s2))

    return score >= threshold


def lowercase_after_comma(text):
    return re.sub(
        r",\s+(?![A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+[A-Zƒê√Ç√ä√î∆Ø])([A-Zƒê√Ç√ä√î∆Ø])",
        lambda m: ", " + m.group(1).lower(),
        text
    )


def remove_repeated_subject_inline(text):
    names = re.findall(
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+)+",
        text
    )
    if len(names) < 2:
        return text

    main = names[0]

    text = re.sub(
        rf",\s*{re.escape(main)}\b",
        ",",
        text
    )

    text = re.sub(
        rf",\s*{re.escape(main)}\s+",
        ", ",
        text
    )

    return re.sub(r"\s+,", ",", text)

def normalize_titles(text):
    return re.sub(
        r",\s*(VƒÉn ki·ªán|T√°c ph·∫©m|S·ª± ki·ªán)\s+",
        ", ",
        text,
        flags=re.I
    )

def event_signature(text: str) -> str:
    text = text.lower()
    text = re.sub(r"\b1[0-9]{3}\b", "", text)
    text = re.sub(
        r"(di·ªÖn ra|x·∫£y ra|nƒÉm|sau khi|ƒë∆∞·ª£c|vua|tri·ªÅu)",
        "",
        text
    )
    text = re.sub(r"[^\w\s]", " ", text)

    tokens = [
        w for w in text.split()
        if w not in STOPWORDS and len(w) > 3
    ]

    return " ".join(tokens[:8])   # ‚¨Ö tƒÉng t·ª´ 6 ‚Üí 8

def extract_keywords(text: str) -> list[str]:
    keywords = set()
    if not text:
        return []

    # 1. H√†nh ƒë·ªông v√† S·ª± ki·ªán l·ªãch s·ª≠ c·ªët l√µi
    actions = re.findall(
        r"(ƒë√°nh b·∫°i|ƒë√°nh tan|l√™n ng√¥i|x∆∞ng v∆∞∆°ng|d·ªùi ƒë√¥|th√†nh l·∫≠p|gi·∫£i ph√≥ng|th·ªëng nh·∫•t|"
        r"kh·ªüi nghƒ©a|kh√°ng chi·∫øn|chi·∫øn d·ªãch|phong tr√†o|hi·ªáp ƒë·ªãnh|tuy√™n ng√¥n|ban h√†nh|k√Ω k·∫øt|"
        r"ƒë·∫°i ph√°|ti√™u di·ªát|ph·∫£n c√¥ng|t·∫•n c√¥ng|ƒë√¨nh chi·∫øn|qu·ªëc hi·ªáu|hi·∫øn ph√°p|lu·∫≠t|h√¨nh th∆∞|chi·∫øu)",
        text.lower()
    )
    keywords.update(actions)

    # 2. T√°c ph·∫©m/VƒÉn ki·ªán n·ªïi ti·∫øng (n·∫øu c√≥ trong text)
    works = [
        "b√¨nh ng√¥ ƒë·∫°i c√°o", "h·ªãch t∆∞·ªõng sƒ©", "tuy√™n ng√¥n ƒë·ªôc l·∫≠p",
        "nh·∫≠t k√Ω trong t√π", "b·ªô h√¨nh th∆∞", "lu·∫≠t h·ªìng ƒë·ª©c"
    ]
    for w in works:
        if w in text.lower():
            keywords.add(w)

    return sorted(list(keywords))

def merge_events(events: list[str]) -> str:
    clauses = []

    for e in events:
        e = re.sub(r"(di·ªÖn ra|x·∫£y ra)", "", e)
        clauses.append(e.strip(" ,."))

    base = max(clauses, key=len)

    for c in clauses:
        if "m·ªü ra" in c or "ch·∫•m d·ª©t" in c or "kh·∫≥ng ƒë·ªãnh" in c:
            if c not in base:
                base += ", " + c

    return purge(base)

def prune_event_sentence(text: str) -> str:
    clauses = [c.strip() for c in text.split(",") if c.strip()]
    if len(clauses) <= 1:
        return text

    kept = []

    # 1Ô∏è‚É£ clause c√≥ PERSON
    for c in clauses:
        set(cached_extract_all_persons(c))


    # 2Ô∏è‚É£ n·∫øu ch∆∞a c√≥ ‚Üí clause c√≥ action
    if not kept:
        for c in clauses:
            if any(v in c.lower() for v in INFORMATIVE_VERBS + STATE_VERBS):
                kept.append(c)

    # 3Ô∏è‚É£ gi·ªØ h·ªá qu·∫£ l·ªãch s·ª≠
    for c in clauses:
        if any(k in c.lower() for k in ["m·ªü ra", "ch·∫•m d·ª©t", "kh·∫≥ng ƒë·ªãnh"]):
            if c not in kept:
                kept.append(c)

    return ", ".join(kept or [clauses[0]])


def ask_by_event(timeline, query: str):
    query = query.lower()
    matches = []

    for year, block in timeline.items():
        for e in block["events"]:
            if query in e["event"].lower():
                matches.append((year, e))

    if not matches:
        return None

    year, event = max(matches, key=lambda x: len(x[1]["event"]))
    tone = pick_tone(event.get("tone", []))

    return storyteller(int(year), tone, event["event"])

def scan_by_entity(timeline, entity: str):
    entity = entity.lower()
    results = []

    for year, block in timeline.items():
        for e in block["events"]:
            persons = [p.lower() for p in e.get("persons", [])]
            if any(entity in p for p in persons):
                results.append({
                    "year": int(year),
                    "event": e["event"],
                    "tone": e["tone"],
                    "nature": e["nature"]
                })

    return sorted(results, key=lambda x: x["year"])


def ask_by_year(timeline, year: int):
    block = timeline.get(str(year))
    if not block:
        return f"Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán n√†o trong nƒÉm {year}."

    results = []
    for e in block["events"]:
        subject = infer_subject(
            e["event"],
            set(e.get("persons", [])),
            e["nature"]
        )
        results.append(
            storyteller(
                year,
                pick_tone(e["tone"]),
                e["event"],
                subject
            )
        )

    return results

def classify_question_nature(question: str) -> str | None:
    q = question.lower()

    if any(k in q for k in [
        "chi·∫øn th·∫Øng", "tr·∫≠n", "chi·∫øn d·ªãch", "ƒë√°nh", "kh√°ng chi·∫øn"
    ]):
        return "military"

    if any(k in q for k in [
        "l√™n ng√¥i", "vua", "tri·ªÅu", "nh√†", "ch√≠nh quy·ªÅn"
    ]):
        return "political"

    if any(k in q for k in [
        "chi·∫øu", "hi·ªáp ƒë·ªãnh", "tuy√™n ng√¥n", "s·∫Øc l·ªánh"
    ]):
        return "institutional"

    if any(k in q for k in [
        "l√† g√¨", "s·ª± ki·ªán", "√Ω nghƒ©a"
    ]):
        return "event"

    return None

def ask_by_nature(timeline: dict, nature: str):
    results = []

    for year, block in timeline.items():
        for e in block.get("events", []):
            if nature in e.get("nature", []):
                results.append(
                    storyteller(
                        int(year),
                        pick_tone(e.get("tone", [])),
                        e["event"],
                        infer_subject(
                            e["event"],
                            set(e.get("persons", [])),
                            e.get("nature", [])
                        )
                    )
                )

    return results or None

def _finalize_ask_results(results):
    if not results:
        return None
    if len(results) == 1:
        return results[0]
    return "\n".join(results)

def normalize_question(q: str) -> str | None:
    """
    Chu·∫©n h√≥a c√¢u h·ªèi:
    - b·ªè d·∫•u ?
    - h·∫° th·∫•p ch·ªØ
    - lo·∫°i r√°c m·ªü ƒë·∫ßu
    """
    if not q:
        return None

    q = q.strip()
    if not q:
        return None

    q = q.replace("?", "").strip()

    # lo·∫°i c√°c ti·ªÅn t·ªë h·ªèi
    q = re.sub(
        r"^(cho bi·∫øt|h√£y cho bi·∫øt|xin cho bi·∫øt|t√¨m hi·ªÉu|gi·∫£i th√≠ch)\s+",
        "",
        q,
        flags=re.I
    )

    return q if len(q) >= 3 else None

def extract_event_keywords(q: str) -> list[str]:
    """
    Tr√≠ch keyword s·ª± ki·ªán t·ª´ c√¢u h·ªèi.
    ∆Øu ti√™n:
    1. C·ª•m danh t·ª´ vi·∫øt hoa (Chi·∫øu d·ªùi ƒë√¥)
    2. Keyword l·ªãch s·ª≠ ph·ªï bi·∫øn
    """
    if not q:
        return []

    keywords = set()

    # 1Ô∏è‚É£ c·ª•m vi·∫øt hoa (event name)
    caps = re.findall(
        r"[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+){0,4}",
        q
    )
    for c in caps:
        keywords.add(c)

    # 2Ô∏è‚É£ keyword l·ªãch s·ª≠ ph·ªï bi·∫øn
    for k in [
        "chi·∫øu",
        "hi·ªáp ƒë·ªãnh",
        "tuy√™n ng√¥n",
        "s·∫Øc l·ªánh",
        "chi·∫øn th·∫Øng",
        "tr·∫≠n",
        "chi·∫øn d·ªãch",
        "kh·ªüi nghƒ©a",
    ]:
        if k in q.lower():
            keywords.add(k)

    return sorted(keywords, key=len, reverse=True)


def ask(timeline: dict, question: str):
    if not timeline or not question:
        return None

    q = normalize_question(question)
    if not q:
        return None

    persons = extract_all_persons(q)

    results = []

    # ======================================================
    # 1. ∆ØU TI√äN H·ªéI THEO PERSON
    # ======================================================
    for person in persons:
        person_answer = ask_by_person(timeline, person)
        if person_answer:
            if isinstance(person_answer, list):
                results.extend(person_answer)
            else:
                results.append(person_answer)

    if results:
        return _finalize_ask_results(results)

    # ======================================================
    # 2. FALLBACK: H·ªéI THEO EVENT / KEYWORD
    # ======================================================
    keywords = extract_event_keywords(q)

    for year, block in timeline.items():
        for e in block.get("events", []):
            event_text = e.get("event", "")
            if any(k.lower() in event_text.lower() for k in keywords):
                story = storyteller(
                    year=int(year),
                    kind=pick_tone(e.get("tone", [])),
                    content=event_text
                )
                results.append(story)

    if results:
        return _finalize_ask_results(results)

    return None

def load_timeline(path=OUT_PATH):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def narrate_year(timeline, year):
    block = timeline.get(str(year))
    if not block:
        return None

    events = block["events"]

    merged = merge_events([e["event"] for e in events])
    tones = sorted(set(t for e in events for t in e["tone"]))

    return {
        "year": int(year),
        "event": merged,
        "tone": tones
    }

def remove_duplicate_subjects_global(text):
    clauses = [c.strip() for c in text.split(",")]
    if len(clauses) < 2:
        return text

    m = re.match(
        r"^([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+)+)\b",
        clauses[0]
    )
    if not m:
        return text

    subject = m.group(1)

    cleaned = [clauses[0]]
    for c in clauses[1:]:
        c = re.sub(rf"^{subject}\s*", "", c)
        cleaned.append(c)

    return ", ".join(cleaned)

def infer_subject(body: str, persons: set, nature: list) -> str:
    # 1. ∆Øu ti√™n nh√¢n v·∫≠t c·ª• th·ªÉ n·∫øu c√≥
    if persons:
        return sorted(list(persons))[0]
    
    body_low = body.lower()
    
    # 2. Ki·ªÉm tra t·ª´ kh√≥a t·∫≠p th·ªÉ xu·∫•t hi·ªán tr·ª±c ti·∫øp trong vƒÉn b·∫£n
    if "qu√¢n d√¢n" in body_low:
        return "Qu√¢n d√¢n Vi·ªát Nam"
    if "nh√¢n d√¢n" in body_low:
        return "Nh√¢n d√¢n Vi·ªát Nam"
    
    # 3. √Ånh x·∫° d·ª±a tr√™n nh√£n (Nature)
    # Th√™m "diplomacy" v√†o nh√≥m Ch√≠nh quy·ªÅn ƒë∆∞∆°ng th·ªùi
    if "military" in nature:
        return "Qu√¢n d√¢n Vi·ªát Nam"
        
    if "political" in nature or "diplomacy" in nature:
        return "Ch√≠nh quy·ªÅn ƒë∆∞∆°ng th·ªùi"
        
    if "institutional" in nature:
        return "VƒÉn ki·ªán l·ªãch s·ª≠"
        
    return "S·ª± ki·ªán l·ªãch s·ª≠"

def render_event_with_subject(year, body, subject=None):
    if subject:
        return f"NƒÉm {year}, {subject} {body[0].lower() + body[1:]}."
    return f"NƒÉm {year}, {body}."


def extract_person_query(q: str) -> str | None:
    q = q.lower().strip()

    matches = re.findall(
        r"[a-z√†-·ªπ]+(?:\s+[a-z√†-·ªπ]+){1,3}",
        q
    )

    for m in sorted(matches, key=len, reverse=True):
        if m in NON_PERSON_PHRASES:
            continue

        p = canonical_person(m)
        if is_valid_person(p):
            return p

    return None

def fix_common_noun_phrases(text):
    return re.sub(
        r",\s*(Qu√¢n|Nghƒ©a qu√¢n|Tri·ªÅu|Ch√≠nh quy·ªÅn)\b",
        lambda m: ", " + m.group(1).lower(),
        text
    )

def smooth_actions(text):
    text = re.sub(
        r"(ƒë√°nh (?:tan|b·∫°i|lui)[^,]+),\s*(l√£nh ƒë·∫°o[^,]+)",
        r"\2 \1",
        text,
        flags=re.I
    )

    text = re.sub(
        r"(d·ª±ng[^,]+),\s*(n·∫Øm quy·ªÅn[^,]+)",
        r"\1, sau ƒë√≥ \2",
        text,
        flags=re.I
    )

    text = re.sub(
        r"(d√πng[^,]+),\s*(ƒë√°nh (?:b·∫°i|tan|lui)[^,]+)",
        r"\1 v√† \2",
        text,
        flags=re.I
    )

    return text

def is_collective_event(nature: list[str], body: str) -> bool:
    # ‚õî n·∫øu text c√≥ PERSON ‚Üí KH√îNG BAO GI·ªú collective
    if extract_all_persons(body):
        return False

    t = body.lower()

    if "military" in nature and any(k in t for k in [
        "c√°ch m·∫°ng",
        "t·ªïng ti·∫øn c√¥ng",
        "kh√°ng chi·∫øn",
        "chi·∫øn d·ªãch",
        "to√†n th·∫Øng",
        "qu√¢n d√¢n"
    ]):
        return True

    return False

def extract_implicit_ruler(text: str) -> set[str]:
    persons = set()

    patterns = [
        r"(?:Vua\s+)?([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+(?:Th√°i|Th√°nh|Nh√¢n)\s+(?:T·ªï|T√¥ng))",
        r"(?:th·ªùi|d∆∞·ªõi th·ªùi|tri·ªÅu)\s+([A-Zƒê√Ç√ä√î∆Ø][a-z√†-·ªπ]+\s+(?:Th√°i|Th√°nh|Nh√¢n)\s+(?:T·ªï|T√¥ng))",
    ]

    for p in patterns:
        for m in re.findall(p, text):
            persons.add(canonical_person(m))

    return persons


@lru_cache(maxsize=100_000)
def cached_extract_all_persons(text: str) -> tuple[str, ...]:
    """
    Cache k·∫øt qu·∫£ extract person theo text
    """
    return tuple(extract_all_persons(text))

YEAR_PATTERN = re.compile(r"(?:nƒÉm|NƒÉm|\s|/|^)([1-9][0-9]{2,3})(?![0-9])")

def iter_raw(ds):
    for row in ds:
        for m in row.get("messages", []):
            if m.get("role") == "assistant":
                yield m.get("content", "")


def main():
    from datasets import load_dataset 
    print("[INFO] Loading dataset...")
    ds = load_dataset("arrow", data_files=ARROW_FILES, split="train")

    timeline: dict[str, list[dict]] = {}

    total_raw = 0
    total_kept = 0

    for line in iter_raw(ds):
        total_raw += 1

        res = normalize(line)
        if not res:
            continue

        year, body, nature, tone, persons_subject, persons_all, places = res

        timeline.setdefault(year, []).append({
            "year": int(year),
            "event": body,
            "persons": sorted(persons_subject),
            "persons_all": sorted(persons_all),
            "places": sorted(places),
            "nature": set(nature),
            "tone": tone,
            "keywords": set(extract_keywords(body))
        })

        total_kept += 1

    final_timeline = {}

    for year, events in timeline.items():
        final_timeline[year] = {
            "summary": build_year_summary(events),
            "events": merge_events_by_year(events)
        }

    timeline = final_timeline

    Path(OUT_PATH).parent.mkdir(parents=True, exist_ok=True)
    with open(OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(timeline, f, ensure_ascii=False, indent=2)

    print(
        f"[DONE] Raw: {total_raw} | "
        f"Gi·ªØ l·∫°i: {total_kept} | "
        f"NƒÉm c√≥ s·ª± ki·ªán: {len(timeline)}"
    )

if __name__ == "__main__":
    main()

